{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaralAminpour/ML-BME-Course-UofA-Fall-2023/blob/main/Week-2-Introduction-to-machine-learning/2.6-Training-models-in-scikit-learn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPhL87eSdb1Z"
      },
      "source": [
        "# Tutorial: Training models in Scikit-Learn\n",
        "\n",
        "In this notebook, we will show how we train models in Scikit-Learn using an example of polynomial regression. We will learn how to:\n",
        "\n",
        "* Split the data into training set and test set\n",
        "* Fit a polynomial model to the training data\n",
        "* Set up a pipeline to automate processing and fitting\n",
        "* Use cross-validation\n",
        "* Use grid search to optimize the polynomial degree hyperparameter\n",
        "* Evaluate performance on the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3qmX45Pdb1b"
      },
      "outputs": [],
      "source": [
        "# This code will download the required data files from GitHub\n",
        "\n",
        "import requests, os\n",
        "\n",
        "# Download data from GitHub\n",
        "def download_data(source, dest):\n",
        "    base_url = 'https://raw.githubusercontent.com/'\n",
        "    owner = 'SirTurtle'\n",
        "    repo = 'ML-BME-UofA-data'\n",
        "    branch = 'main'\n",
        "    url = '{}/{}/{}/{}/{}'.format(base_url, owner, repo, branch, source)\n",
        "    r = requests.get(url)\n",
        "    f = open(dest, 'wb')\n",
        "    f.write(r.content)\n",
        "    f.close()\n",
        "\n",
        "import os\n",
        "if not os.path.exists('temp'):\n",
        "   os.makedirs('temp')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAY7dQ3-db1b"
      },
      "source": [
        "## Generate the data\n",
        "\n",
        "We will generate the training data from a quadratic model $y=2x^2+4x+5$ by generating random features from uniform distribution for the interval $[-3,3]$ using `rng = np.random.default_rng()` and `rng.random` and adding Gaussian noise with $\\sigma=3$ to the target values using `rng.standard_normal`.\n",
        "\n",
        "Run the code below to generate your dataset. Note that\n",
        "* Feature matrix is given in the variable `X`\n",
        "* Target vector is given in variable `y`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HzT6q1NUdb1c"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Define the true model\n",
        "def TrueModel(x):\n",
        "    return 2*x**2 + 4*x + 5\n",
        "\n",
        "# Generate random samples\n",
        "# To keep the random samples the same at every run we will initialize the random seed to a fixed number\n",
        "rng = np.random.default_rng(seed=40)\n",
        "\n",
        "# Generate 100 random samples\n",
        "n = 100\n",
        "X = 6*rng.random(n) - 3\n",
        "\n",
        "# Generate noise for each sample\n",
        "noise = 3*rng.standard_normal(n)\n",
        "\n",
        "# Generate noisy samples\n",
        "y = TrueModel(X) + noise\n",
        "\n",
        "# Convert features X into a 2D array\n",
        "X = X.reshape(-1, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9TWuaHddb1c"
      },
      "source": [
        "__Activity 1:__ Write code to print out the dimensions of the feature matrix X and target vector y. How many features and samples we have?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-TQSaoycdb1c"
      },
      "outputs": [],
      "source": [
        "# Print dimensions of feature matrix and target vector\n",
        "print('Feature Matrix:', None) # Edit this line\n",
        "print('Target Vector:', None) # Edit this line\n",
        "\n",
        "# Print number of features and samples\n",
        "print('Samples:', None) # Edit this line\n",
        "print('Features:', None) # Edit this line"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQ5OoWQqdb1c"
      },
      "source": [
        "__Activity 2:__ Plot the generated samples. *Hint:* use `plt.plot`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KW6qCdYdb1d"
      },
      "outputs": [],
      "source": [
        "# Add your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJrBzjhFdb1d"
      },
      "source": [
        "## Split the data to training and test set\n",
        "\n",
        "Scikit-learn offers a function `sklearn.model_selection.train_test_split` to perform the splitting of the dataset. A common pattern is to keep 80% of the data for training and use 20% for testing. This can be set using the option `test_size`. Splitting is random, so if we wish to keep the same split at every run we can use parameter `random_state`.\n",
        "\n",
        "Finally, to create a representative test set, we can use the `stratify` parameter. In this example, we will split the data into 7 bins by rounding the feature values, and stratify the test set to have the same proportion of the data from each bin as the training set.\n",
        "\n",
        "__Activity 3:__ Play with parameter `test_size` to see different splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rx_OX8Dtdb1d"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Create bins for the target values\n",
        "bins = np.round(X)\n",
        "\n",
        "# Perform stratified train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=bins, random_state=42)\n",
        "\n",
        "# Plot the training and test data\n",
        "plt.plot(X_train, y_train, 'o', label='Training set')\n",
        "plt.plot(X_test, y_test, 'o', label='Test set')\n",
        "plt.legend()\n",
        "plt.title('Train test split')\n",
        "plt.xlabel('Feature')\n",
        "plt.ylabel('Target value')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTaGR7Nodb1d"
      },
      "source": [
        "## Fitting a polynomial model\n",
        "\n",
        "Scikit-learn does not implement a polynomial regression model. Instead, the polynomial regression is performed in two steps:\n",
        "1. Polynomial feature transformation - a __transformer__ object `PolynomialFeatures` transforms each feature $x$ into a feature vector $(1,x,x^2,...,x^M)$, where polynomial degree $M$ is defined by setting parameter `degree`\n",
        "2. Performing multivariate linear regression - a __regressor__ object `LinearRegression` fits the model\n",
        "$y=x_0+x_1w_1+...+x_Mw_M$ to the data.\n",
        "\n",
        "Note that if we combine these two steps, we will have $x_j=x^j$ and therefore create a polynomial model $y=x_0+w_1x+...+w_Mx^M$\n",
        "\n",
        "The code below demonstrates how we can fit the 2nd order polynomial model to the training set and evaluate it on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DryYH9i0db1d"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Create polynomial features object of second degree\n",
        "M = 2\n",
        "poly_features = PolynomialFeatures(degree=M)\n",
        "\n",
        "# Transform training features (note conversion to 2D array)\n",
        "X_train_poly = poly_features.fit_transform(X_train)\n",
        "\n",
        "# Create and fit multivariate linear regression model\n",
        "# We do not need intercept, because the first feature is 1\n",
        "model = LinearRegression(fit_intercept=False)\n",
        "model.fit(X_train_poly, y_train)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "X_test_poly = poly_features.fit_transform(X_test)\n",
        "r2_test = model.score(X_test_poly, y_test)\n",
        "print('R2 score on test set is', round(r2_test, 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkJt1Vvwdb1d"
      },
      "source": [
        "### Plot the fitted polynomial model\n",
        "\n",
        "__Activity 4:__ Fill in the code below to plot the second order polynomial model that we just fitted. Then change the polynomial degree in the cell above, and rerun both cells to see how the model changes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_fmzJiDdb1d"
      },
      "outputs": [],
      "source": [
        "# Plot the training and test data\n",
        "# Add your code here\n",
        "\n",
        "# Define the feature space\n",
        "X_curve=np.linspace(-3, 3, 100).reshape(-1, 1)\n",
        "\n",
        "# Perform the polynomial feature transformation of X_curve\n",
        "X_curve_poly = poly_features.transform(X_curve)\n",
        "\n",
        "# Predict target values using X_curve_poly\n",
        "y_curve = model.predict(X_curve_poly)\n",
        "\n",
        "# Plot the curve\n",
        "# Add your code here\n",
        "\n",
        "# Annotate the plot\n",
        "plt.title('Polynomial fit $M={}$'.format(M))\n",
        "plt.xlabel('Feature')\n",
        "plt.ylabel('Target value')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lObfl8b5db1e"
      },
      "source": [
        "__Activity 4:__ Print the coefficients of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXTndOkYdb1e"
      },
      "outputs": [],
      "source": [
        "# Show the coefficients of the model\n",
        "print(model.coef_)\n",
        "print('{2:.1f}x^2 + {1:.1f}x + {0:.1f} '.format(model.coef_[0], model.coef_[1], model.coef_[2]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trLHfV7Odb1e"
      },
      "source": [
        "We see that we have recovered a model $y=2.1x^2 + 4.0x + 5.6$ which is fairly close to our true model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4RDIttRdb1e"
      },
      "source": [
        "Okay, we have fit a polynomial model to our data. So far so good!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02DB34Dfdb1e"
      },
      "source": [
        "## Setting up a model pipeline\n",
        "\n",
        "It is not always convenient to perform several steps for model training, prediction and evaluation. In particular, if the model becomes an input to another `sklearn` object, such as hyperparameter search using cross-validation that we will introduce below, the steps need to be unified in a single object.\n",
        "\n",
        "Scikit-learn implements a class `sklearn.pipeline.Pipeline` to join multiple __steps__ into one model. `Pipeline` implements methods `fit`, `predict`, `score` and others to offer a unified API with other sklearn objects.\n",
        "\n",
        "In the code below we will demonstrate how to join the polynomial feature transformation and multivariate linear regression into a single model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysTjcXZ2db1f"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Polynomial regression model is created as a 'pipeline'\n",
        "# combining creation of features (1,x,x^2,...) followed\n",
        "# by multivariate linear regression\n",
        "pipeline = Pipeline((\n",
        "(\"poly_features\", PolynomialFeatures(degree=2)),\n",
        "(\"lin_reg\", LinearRegression(fit_intercept=False)) ))\n",
        "\n",
        "# Fit the model using features and labels\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Calculate performance\n",
        "r2_test = pipeline.score(X_test, y_test)\n",
        "print('R2 score on test set is', round(r2_test, 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AHsgWSkdb1f"
      },
      "source": [
        "Let's look in detail at the syntax for creating the pipelines. It is in the form `Pipeline(steps)`, where the input argument `steps` is a list of transforms and models to be chained, in the order with which they need to be called. Each step is described by a tuple `(name, model)`, where `name` is a string chosen by the user and `model` is a `sklearn` object.\n",
        "\n",
        "To access the original objects joint in a `Pipeline` we use attribute `Pipeline.named_steps`. For example, the linear regression object defined in the cell above can be accessed as:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oABbSc3sdb1f"
      },
      "outputs": [],
      "source": [
        "# Access LinearRegression object\n",
        "pipeline.named_steps[\"lin_reg\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tal2B9fodb1f"
      },
      "source": [
        "## Cross-validation\n",
        "\n",
        "Even if we stratify our test set, it is not guaranteed that it is representative and the performance may vary every time we create a different split. A more robust technique is to perform cross-validation, where we split the data into $k$ groups (folds), and each fold will be used to measure performance exactly once, while remaining data are used to fit the model. Average performance over the $k$ folds will be much more robust. If there are no hyperparameters to tune, cross-validation can be used directly to measure the performance of the model.\n",
        "\n",
        "We will use the [cross_val_score](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) function.\n",
        "\n",
        "In scikit-learn cross validation is called using `cross_val_score(estimator, X, y)` with arguments\n",
        "* `estimator:` the model to be fit\n",
        "* `scoring:` the score to use; if this parameter is not specified, use the model's default score\n",
        "* `X:` the feature matrix\n",
        "* `y:` optional target values or labels for supervised models\n",
        "- `cv:` optional argument that defines the number of folds.\n",
        "\n",
        "The model returns an array with scores from each fold. Note that the function cross_val_score is only for measuring the scores that the model would produce on the $k$ folds. It does not return the trained models. For that task, use GridSearchCV, as described below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bnvjphi1db1f"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Define the model\n",
        "pipeline = Pipeline((\n",
        "(\"poly_features\", PolynomialFeatures(degree=2)),\n",
        "(\"lin_reg\", LinearRegression(fit_intercept=False)),))\n",
        "\n",
        "# Perform 5-fold cross-validation\n",
        "scores = cross_val_score(pipeline, X, y, cv=5)\n",
        "\n",
        "# Print scores\n",
        "print('Scores:', np.around(scores, 2))\n",
        "print('Cross-validated R2 score: mean = {}, standard deviation = {}'.format(round(scores.mean(),3),round(scores.std(),3)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8heiwWCdb1f"
      },
      "source": [
        "## Hyperparameter search using grid search\n",
        "\n",
        "Finally we will show how we can automatically find the optimal hyperparameters in scikit-learn. One of the most common ways is to train a model for each hyper-parameter value and measure its performance using cross-validation. This approach is implemented in `sklearn.model_selection.GridSearchCV`.\n",
        "\n",
        "First we need to create the grid search object using `GridSearchCV(estimator, param_grid)` with arguments:\n",
        "* `estimator:` the model to be fitted\n",
        "* `param_grid:` a dictionary, with parameter names as keys, and parameter values as lists\n",
        "* `cv:` optional parameter to determine the number of folds\n",
        "\n",
        "We will now find the optimal polynomial degree automatically, using the `GridSearchCV`. First, we need to define our parameter grid dictionary. Because we have a `Pipeline` object, the parameter name is composed of the step name `\"poly_features\"` and parameter name `\"degree\"` which are joined by two underscores `__`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQqGjJwodb1f"
      },
      "outputs": [],
      "source": [
        "# Propose a range of possible values for the polynomial degree\n",
        "degrees = range(1,20)\n",
        "\n",
        "# Define parameter dictionary\n",
        "# Because the model is a pipeline, the name of the parameter is composed\n",
        "# of the name of the step and parameters names joint by a double underscore\n",
        "parameters = {\"poly_features__degree\": degrees}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tladse-wdb1g"
      },
      "source": [
        "Next we create the `GridSearchCV` object and run the hyperparameter optimization using method `fit`, which is performed on the training set. The chosen polynomial model can be accessed using the variable `best_estimator_`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFngIXZcdb1g"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Create grid search object\n",
        "grid_search = GridSearchCV(pipeline, parameters, cv=5)\n",
        "\n",
        "# Perform the search over all hyperparameter values\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# This is the chosen model\n",
        "best_model = grid_search.best_estimator_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xggmB9fjdb1g"
      },
      "source": [
        "We can also access the chosen polynomial, and the best R2 score identified through cross-validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mk4au3KTdb1g"
      },
      "outputs": [],
      "source": [
        "# Degree of the best model\n",
        "best_degree = best_model.named_steps['poly_features'].degree\n",
        "print('The best polynomial degree is', best_degree)\n",
        "\n",
        "# Best score\n",
        "best_score = best_model.score(X_train, y_train)\n",
        "grid_search.best_score_\n",
        "print('The best cross-validated R2 score on training set is', round(best_score, 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHwFUzsMdb1g"
      },
      "source": [
        "## Evaluate performance on test set\n",
        "\n",
        "We can evaluate the performance on the test set directly using a trained `GridSearchCV` by calling its method `score`. It will return the score of the model with the best hyperparameter setting identified during the search and fitted to the whole training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CO604pKdb1g"
      },
      "outputs": [],
      "source": [
        "r2 = grid_search.score(X_test, y_test)\n",
        "print('R2 score on test set is:', round(r2, 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vX5q2bl3db1h"
      },
      "source": [
        "## Exercise\n",
        "\n",
        "Now we will load the data 'noisy_data.csv' using `pandas`. The file contains two columns:\n",
        "* column zero contains features `X`\n",
        "* column one contains target values `y`\n",
        "\n",
        "Run the code below to create `X` and `y` and plot the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uD6lg8-vdb1h"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Download data\n",
        "download_data('Week-2-Introduction-to-machine-learning/data/noisy_data.csv', 'temp/noisy_data.csv')\n",
        "\n",
        "# Load data\n",
        "import pandas as pd\n",
        "df = pd.read_csv('temp/noisy_data.csv')\n",
        "\n",
        "print(df)\n",
        "\n",
        "X = df.values[:,0].reshape(-1,1)\n",
        "y = df.values[:,1]\n",
        "\n",
        "plt.plot(X, y, 'o')\n",
        "plt.title('Noisy data')\n",
        "plt.xlabel('Feature')\n",
        "plt.ylabel('Target value')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rri-xpf3db1i"
      },
      "source": [
        "Your task is to fit a polynomial model to the data. Perform the following steps:\n",
        "* Perform stratified train-test split\n",
        "* Create a polynomial model using `Pipeline`\n",
        "* Use `GridSearchCV` to find the optimal polynomial degree and print out the best cross-validated score\n",
        "* Calculate performance on the test set\n",
        "* Plot the fitted model\n",
        "* Print out the coefficients of the model\n",
        "\n",
        "Can you work out what was the underlying polynomial model? (*Hint: the coefficients were whole numbers*)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kK-FJ1ymdb1i"
      },
      "outputs": [],
      "source": [
        "# STRATIFIED TRAIN TEST SPLIT\n",
        "bins = np.round(X)\n",
        "#X_train, X_test, y_train, y_test = None # Edit this line"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-o8MqBadb1i"
      },
      "outputs": [],
      "source": [
        "# CREATE POLYNOMIAL MODEL\n",
        "pipeline = None # Edit this line"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-YH9Qqldb1i"
      },
      "outputs": [],
      "source": [
        "# CREATE PARAMETER GRID\n",
        "#parameters = {\"poly_features__degree\": range(1,20)}\n",
        "parameters = None # Edit this line\n",
        "# Tip: Using 20 for max degree sometimes overfits the data!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZ_muFQkdb1i"
      },
      "outputs": [],
      "source": [
        "# OPTIMISE HYPERPARAMETERS\n",
        "# Create grid search object\n",
        "grid_search = None # Edit this line\n",
        "\n",
        "# Fit GridSearchCV object\n",
        "# Add your code here\n",
        "\n",
        "# Extract the selected model\n",
        "best_model = None # Edit this line\n",
        "\n",
        "# Print out the best score and chosen polynomial degree\n",
        "#print('The best cross-validated R2 score on training set is',round(grid_search.best_score_,2))\n",
        "#print('The best polynomial degree is',best_model.named_steps[\"poly_features\"].degree)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FpZGigvOdb1j"
      },
      "outputs": [],
      "source": [
        "# CALCULATE PERFORMANCE ON TEST SET\n",
        "r2 = None # Edit this line\n",
        "#print('R2 score on test set is', round(r2,2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWmTETWhdb1j"
      },
      "outputs": [],
      "source": [
        "# PLOT THE FITTED MODEL\n",
        "x_curve = None # Edit this line\n",
        "y_curve = None # Edit this line\n",
        "\n",
        "# Plot the training data\n",
        "# Add your code here\n",
        "\n",
        "# Plot the test data\n",
        "# Add your code here\n",
        "\n",
        "# Plot the model curve\n",
        "\n",
        "plt.title('Polynomial fit')\n",
        "plt.xlabel('Feature')\n",
        "plt.ylabel('Target value')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSsJYB0Ndb1j"
      },
      "outputs": [],
      "source": [
        "# COEFFICIENTS OF THE MODEL\n",
        "coef = None # Edit this line\n",
        "#print('Coefficients: ', np.around(coef))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5sy1mNHdb1k"
      },
      "source": [
        "The original model is $y=2-3x+x^3$\n",
        "\n",
        "*Note: Splitting of training and test set is random, so different solutions are possible. From the shape of the dataset, we can judge that the underlying polynomial was cubic. If you obtain a polynomial of a higher degree, try to re-run your code until you get a cubic polynomial. That will help you to work out true model coefficients.*"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}