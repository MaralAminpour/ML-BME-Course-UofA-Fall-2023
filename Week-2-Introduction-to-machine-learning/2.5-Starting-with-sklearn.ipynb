{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Basics\n",
    "## Starting with Scikit-learn\n",
    "\n",
    "In this notebook, we will introduce the basics of the Scikit-learn application programming interface (API).\n",
    "\n",
    "Scikit-learn supports many different machine learning models and also has many useful supporting functions. One nice feature of scikit-learn is that different models have similar interfaces, meaning there will be minimal code changes required to switch between one type of model to another.\n",
    "\n",
    "We will cover very simple examples of (1) regression, (2) classification, (3) clustering and (4) dimensionality reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1. Regression\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-2-Introduction-to-machine-learning/imgs/brain-volume.png\" width = \"150\" style=\"float: right;\">\n",
    "\n",
    "The first example demonstrates the __regressor__ object API.\n",
    "\n",
    "The file 'neonatal_brain_volumes.csv' contains gestational ages (GA) and brain volumes of premature babies. We will fit a `LinearRegression` model to predict the brain volumes from GA. This is an example of a regression problem.\n",
    "\n",
    "The image on the right is an example of a scan that could be used to estimate brain volumes; however the data we'll be using has the brain volumes already computed.\n",
    "\n",
    "We will cover regression in more detail in Week 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code will download the required data files from GitHub\n",
    "# This is important so you can easily run the notebook on Colab\n",
    "# Or if you want to run an example but have downloaded only notebook\n",
    "# We'll use code like this in most of our notebooks\n",
    "# Normally we'll put files we download in a directory called \"temp\"\n",
    "#    This will help keep everything organized\n",
    "\n",
    "import requests, os\n",
    "\n",
    "# Download data from GitHub\n",
    "def download_data(source, dest):\n",
    "    base_url = 'https://raw.githubusercontent.com/'\n",
    "    owner = 'SirTurtle'\n",
    "    repo = 'ML-BME-UofA-data'\n",
    "    branch = 'main'\n",
    "    url = '{}/{}/{}/{}/{}'.format(base_url, owner, repo, branch, source)\n",
    "    r = requests.get(url)\n",
    "    f = open(dest, 'wb')\n",
    "    f.write(r.content)\n",
    "    f.close()\n",
    "\n",
    "# Create the temp directory, if it doesn't already exist\n",
    "if not os.path.exists('temp'):\n",
    "   os.makedirs('temp')\n",
    "\n",
    "download_data('Week-2-Introduction-to-machine-learning/data/neonatal_brain_volumes.csv', 'temp/neonatal_brain_volumes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data\n",
    "First we will import the data file using the `pandas` package and check its content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read file into a dataframe object\n",
    "df = pd.read_csv('temp/neonatal_brain_volumes.csv')\n",
    "\n",
    "# Print the first few lines\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could also open the data file with Excel or any program that can read a CSV formatted file to examine its contents.\n",
    "\n",
    "#### Data dictionary\n",
    "\n",
    "__Tip:__ It is a very good practice to create a data dictionary that describes the structure of your data. The data dictionary will have text about each column. It is useful to include info such as units and acceptable values when possible.\n",
    "\n",
    "*GA*: gestational age (in weeks)\n",
    "\n",
    "*brain volume*: measured brain volume (in mL)\n",
    "\n",
    "Next, we will convert the data into numpy arrays. We will create a feature matrix containing the 'GA' column and the target vector containing the 'brain volume' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert dataframe object into a numpy array\n",
    "brain_volume_data = df.to_numpy()\n",
    "\n",
    "# Create the feature matrix and convert it to a 2D numpy array\n",
    "# In this case we have only one feature: \"GA\" (gestational age)\n",
    "# X is a conventional name for a feature matrix\n",
    "X = brain_volume_data[:,0].reshape(-1, 1)\n",
    "print('Number of samples: ', X.shape[0])\n",
    "print('Number of features: ', X.shape[1])\n",
    "print('Feature matrix X dimensions: ', X.shape)\n",
    "\n",
    "# Create the target vector\n",
    "# In this case the target is \"brain volume\"\n",
    "# y is a conventional name for target vector\n",
    "y = brain_volume_data[:,1]\n",
    "print('Target vector y dimensions: ', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the example our feature matrix for each data sample is only dimension N x 1 since we have only a single feature. But generally, there will be multiple features for each sample.\n",
    "\n",
    "We've selected brain volume as the target that we'll build our model to predict, but we could have done the reverse and built a model to predict GA from brain volume.\n",
    "\n",
    "Here would be a good time to do some exploratory data analysis (EDA), but for this example, we'll just go straight to modeling. In a real machine learning protocol, we would need to do additional steps here, such as splitting our data into train and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model\n",
    "Now we select and create the [linear regression model](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html). A linear regression model with one feature (univariate) has this functional form:\n",
    "\n",
    "$y=w_0+w_1x$\n",
    "\n",
    "It is just a line! $w_0$ and $w_1$ are the parameters (weights) for this model.\n",
    "\n",
    "The object we instantiate will be an untrained linear regression model that we can train on any type of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that to import from Scikit-learn use the name 'sklearn'\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create the model\n",
    "lr_model = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model\n",
    "The next step is to fit the model to the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "# Note the lr_model object is modified in place\n",
    "lr_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view the coefficients of the linear model \n",
    "\n",
    "$y=w_0+w_1x$\n",
    "\n",
    "which we fitted to the data, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = lr_model.intercept_\n",
    "print('w0:', round(w0))\n",
    "\n",
    "w1 = lr_model.coef_[0]\n",
    "print('w1:', round(w1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model\n",
    "The model accuracy can be evaluated by calling the function `score`. For regression models in scikit-learn the score function returns the $R^2$ score. This is the most commonly used metric to evaluate the performance of regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = lr_model.score(X, y)\n",
    "print('R2 score:', round(r2, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How good is this score? 1 is perfect (the best score). So 0.84 is a decent score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions on new data\n",
    "\n",
    "We are interested in visualising the model, and to do that we will create a grid that samples the feature space. The code below will create 10 artificial samples that span the values between the minimum and maximum of the GA. Note we need a 2D array for prediction (because although in this case we have only one feature per sample, the functions support multiple features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_model = np.linspace(np.min(X), np.max(X), 10).reshape(-1, 1)\n",
    "print('Feature space:\\n', np.around(X_model).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to predict the target values for these new samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_model = lr_model.predict(X_model)\n",
    "print('Predicted targets for the feature space:\\n', np.around(y_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the result\n",
    "We will plot the result using `matplotlib` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "# Plot the data\n",
    "plt.plot(X, y, 'bo', alpha = 0.5, label = 'samples')\n",
    "\n",
    "# Plot the model\n",
    "plt.plot(X_model, y_model, 'k', label = 'model')\n",
    "\n",
    "# Annotate the plot\n",
    "plt.title('Regression')\n",
    "plt.xlabel('Feature: Gestational age at scan')\n",
    "plt.ylabel('Target value: brain volume in mL')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the points mostly lie near the line defined by the linear model. This is a visual representation of the $R^2$ score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "It is now your turn to write the solution to the following problem: You would like to predict the GA of a preterm baby from the measurement of the brain volume. Note that in this case the GA and volumes switched roles - volume is a feature and GA is the target value. The feature matrix `X1` and target vector `y1` were created for you.\n",
    "\n",
    "Write code to\n",
    "* Create the `LinearRegression` model\n",
    "* Fit the model \n",
    "* Calculate the $R^2$ score\n",
    "\n",
    "Commands for printing out the score and the equation of the fitted model were created for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the feature matrix using brain volumes\n",
    "X1 = brain_volume_data[:,1].reshape(-1,1)\n",
    "\n",
    "# Create the target vector using GA\n",
    "y1 = brain_volume_data[:,0]\n",
    "\n",
    "# Create the model\n",
    "lr_model1 = None # Edit this line\n",
    "\n",
    "# Fit the model\n",
    "# Add your code here\n",
    "\n",
    "# Calculate the R2 score\n",
    "r2_1 = None # Edit this line\n",
    "\n",
    "# Print the score\n",
    "#print('R2 score:', round(r2_1,2))\n",
    "\n",
    "# Print the equation of the fitted model\n",
    "#print('Fitted model: y = {} + {}x'.format(round(lr_model1.intercept_), round(lr_model1.coef_[0],2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2. Classification\n",
    "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-2-Introduction-to-machine-learning/imgs/HeartSegmentation.gif\" width = \"150\" style=\"float: right;\">\n",
    "\n",
    "This example demonstrates the __classifier__ object API.\n",
    "\n",
    "The file 'heart_failure_data.csv' contains features Ejection Fraction (EF), Global Longitudinal Strain (GLS) and a label indicating whether the patient has heart failure (HF). We will fit a linear `Perceptron` model to predict the heart failure from EF and GLS.\n",
    "\n",
    "A [linear perceptron](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html) is a simple model that will find a line (or in higher dimensions a plane or hyper-plane) that divides the data into classes. This is the *decision boundary*. Perceptrons can be used for binary classification (as here), and also for multiclass problems.\n",
    "\n",
    "We will cover classification in more detail in Week 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data\n",
    "download_data('Week-2-Introduction-to-machine-learning/data/heart_failure_data.csv', 'temp/heart_failure_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data\n",
    "First we will import the file using the `pandas` package and check its content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data file into a dataframe object\n",
    "df = pd.read_csv('temp/heart_failure_data.csv')\n",
    "\n",
    "# Print the first few lines\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data dictionary\n",
    "\n",
    "*EF*: Ejection Fraction. A measurement of how much blood the left ventricle pumps out with each contraction. Expressed as a percent in the range 0 to 100.\n",
    "\n",
    "*GLS*: Global Longitudinal Strain. A measurement of myocardial deformation along the longitudinal cardiac axis. Expressed as a negative percent in the range 0 to -100.\n",
    "\n",
    "*HF*: Heart Failure class.\n",
    "- 0 = Healthy\n",
    "- 1 = Heart failure\n",
    "\n",
    "The code below creates the feature matrix `X` and label vector `y`. Note that now the feature matrix has dimension N x 2.\n",
    "\n",
    "Also, we will do some preprocessing on the data: we will scale the features to have zero mean and unit variance across the dataset.\n",
    "\n",
    "*Tip*: This preprocessing is called *standardization* or *normalization*. This is important for many types of models and will make it easier for the model to fit your data. It is particularly important if your features differ by orders of magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and create an object to scale the features\n",
    "# to have zero mean and unit variance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Convert to numpy\n",
    "heart_failure_data = df.to_numpy()\n",
    "\n",
    "# Create feature matrix containing EF and GLS\n",
    "X = scaler.fit_transform(heart_failure_data[:,:2])\n",
    "print('Feature matrix X dimensions: ', X.shape)\n",
    "\n",
    "# Create target vector containing HF\n",
    "y = heart_failure_data[:,2]\n",
    "print('Target vector y dimensions: ', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model\n",
    "This code creates the `Perceptron` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "# Create the model\n",
    "p_model = Perceptron()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model\n",
    "This code fits the `Perceptron` model to the training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "p_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficients of the fitted decision function\n",
    "$h(\\mathbf{x})=w_0+w_1x_1+w_2x_2$ can be accessed as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = p_model.intercept_[0]\n",
    "print('w0:', round(w0))\n",
    "\n",
    "w1 = p_model.coef_[0][0]\n",
    "print('w1:', round(w1))\n",
    "\n",
    "w2 = p_model.coef_[0][1]\n",
    "print('w2:', round(w2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model\n",
    "For classification models, the function `score` returns accuracy, which is the proportion of the correctly classified samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = p_model.score(X, y)\n",
    "\n",
    "# Print the score\n",
    "print('Accuracy score:', round(accuracy,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy will be in the range [0,1], with 1 being perfect accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the model\n",
    "The result of the classification is plotted below.\n",
    "\n",
    "Note that for this example is easy to visualize the decision boundary since we have 2 features. In higher dimensions, this type of visualization would not work!\n",
    "\n",
    "Remember also that we standardized our features before fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Plot data\n",
    "plt.plot(X[y==0,0], X[y==0,1], 'bo', alpha=0.75, label = 'Healthy')\n",
    "plt.plot(X[y==1,0], X[y==1,1], 'r*', alpha=1, label = 'Heart Failure')\n",
    "\n",
    "# Plot decision boundary\n",
    "# Define y-coordinates\n",
    "x2 = np.array([X[:,1].min(), X[:,1].max()])\n",
    "\n",
    "# Define x-coordinates\n",
    "x1 = -(w0 + w2*x2)/w1\n",
    "\n",
    "# Plot \n",
    "plt.plot(x1, x2, \"k-\") \n",
    "\n",
    "plt.legend()\n",
    "plt.title('Classification')\n",
    "plt.xlabel('Feature 1: Ejection Fraction')\n",
    "plt.ylabel('Feature 2: Global Longitudinal Strain')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line is our model's decision boundary between the two classes (healthy and heart failure)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Write a solution to the following problem: You would like to find out whether using only Ejection Fraction (EF) would be sufficient to predict heart failure (HF). \n",
    "\n",
    "Write code to\n",
    "* Create the new feature matrix and the target vector\n",
    "* Fit the model and calculate the accuracy score\n",
    "* Print the equation of the decision boundary\n",
    "\n",
    "What is the drop in accuracy compared to using both features (EF and GLS)?\n",
    "\n",
    "*Hint*: If your accuracy is not good, try changing the learning rate by changing the eta0 learning rate hyperparameter from the default:\n",
    "`Perceptron(eta0=0.2)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature matrix containing EF only\n",
    "X2 = None # Edit this line\n",
    "\n",
    "# Create label vector containing HF\n",
    "y2 = None # Edit this line\n",
    "\n",
    "# Create the model\n",
    "from sklearn.linear_model import Perceptron\n",
    "p_model2 = Perceptron(eta0=0.2)\n",
    "\n",
    "# Fit the model\n",
    "# Add your code here\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy2 = None # Edit this line\n",
    "\n",
    "# Print the score\n",
    "#print('Accuracy score is:', round(accuracy2,2))\n",
    "\n",
    "# Print the decision boundary\n",
    "#print('Decision boundary: {} + {}x = 0'.format(round(p_model2.intercept_[0],2),round(p_model2.coef_[0][0],2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3. Clustering\n",
    "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-2-Introduction-to-machine-learning/imgs/T1.png\" width = \"150\" style=\"float: right;\">\n",
    "\n",
    "This example demonstrates the __cluster__ object API.\n",
    "\n",
    "The file 'T1.png' contains a slice of T1-weighted magnetic resonance image (MRI) of an adult brain. The non-brain tissues have been removed in preprocessing. We perform `KMeans`) clustering to segment white matter (WM), grey matter (GM) and cerebrospinal fluid (CSF) in this image.\n",
    "\n",
    "This is an example of unsupervised learning. [KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) is one of many clustering algorithms available in Scikit-Learn.\n",
    "\n",
    "Note that in the previous examples, the data consisted of features in tabular format. In this case, the data is a 2D image. Each pixel in the image could be black (representing non-brain tissues) or a shade of gray representing different types of brain tissue.\n",
    "\n",
    "We will cover clustering in more detail in Week 5.\n",
    "\n",
    "### Prepare the data\n",
    "First we will load the image using the `matplotlib` function `imread` and display it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data\n",
    "download_data('Week-2-Introduction-to-machine-learning/data/T1.png', 'temp/T1.png')\n",
    "\n",
    "# Load image\n",
    "T1 = plt.imread('temp/T1.png')\n",
    "\n",
    "# Display image\n",
    "plt.imshow(T1, cmap = 'gray')\n",
    "\n",
    "# Print shape\n",
    "print('Image dimensions: ', T1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually it is easy to identify the 3 kinds of brain tissue based on the shades of gray. We will try to use clustering to identify these 3 kinds of brain tissue.\n",
    "\n",
    "Next we need to convert the image into the feature matrix suitable for processing using `sklearn` functions. First, we need to remove the background pixels that have values zero. Then we need to create the feature matrix as a 2D array object but with only one feature in each row.\n",
    "\n",
    "Note that we are \"flattening\" the 2D image, turning it into a single vector along x-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all the non-zero elements\n",
    "ind = T1 > 0\n",
    "\n",
    "# Create the feature matrix with the correct dimensions\n",
    "X = T1[ind].reshape(-1,1)\n",
    "print('Shape of the feature matrix X is', X.shape)\n",
    "\n",
    "print('The first 10 entries of the feature matrix:\\n', X[1:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data values are what we visualized with different shades of gray. Notice that the numbers are not exactly the same for each brain region. We will use clustering to find boundaries between the grayscale values for each of the 3 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and fit the model\n",
    "\n",
    "Now we are ready to perform k-means clustering into 3 classes, which will correspond to three brain tissues: WM, GM and CSF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Create the model\n",
    "# random_state will initialize the random seed to fixed value\n",
    "# n_init is the number of times the algorithm is run with different cluster centroid seeds\n",
    "km_model = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "\n",
    "# Fit the model\n",
    "km_model.fit(X)\n",
    "\n",
    "# Fitted parameters\n",
    "c = km_model.cluster_centers_\n",
    "print(c.round(1).flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the labels\n",
    "\n",
    "The next step is the predict the labels. Note that this time we did not calculate any score - this is because we do not have the training labels, so cannot evaluate the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the labels\n",
    "y_predict = km_model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the result\n",
    "To plot the result, we need to reshape the predicted labels to the original 2D array and then we can display it as an image. We are using the \"viridis\" color map so each of the 3 clusters will appear in a different color. You could try a different color map such as \"magma\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty segmentation image\n",
    "segmentation = np.zeros(T1.shape)\n",
    "\n",
    "# Paste the labels into correct locations\n",
    "# We can do this since we stored the original index of each non-zero\n",
    "#    pixel in the ind array\n",
    "segmentation[ind] = y_predict + 1\n",
    "\n",
    "# Plot the segmentation\n",
    "plt.imshow(segmentation, cmap = 'viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "Now perform the k-means clustering for the T2-weighted image 'T2.png'. Is the result similar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image\n",
    "download_data('Week-2-Introduction-to-machine-learning/data/T2.png', 'temp/T2.png')\n",
    "T2 = plt.imread('temp/T2.png')\n",
    "\n",
    "# Select non-zero pixels\n",
    "ind2 = T2 > 0\n",
    "\n",
    "# Create feature matrix\n",
    "X2 = T2[ind2].reshape(-1,1)\n",
    "\n",
    "# Create the model\n",
    "km_model2 = None # Edit this line\n",
    "\n",
    "# Fit the model\n",
    "# Add your code here\n",
    "\n",
    "# Predict the labels\n",
    "y2 = None # Edit this line\n",
    "\n",
    "# Create segmentation image\n",
    "segmentation2 = None # Edit this line\n",
    "\n",
    "# Paste the labels into correct locations\n",
    "#segmentation2[ind2] = y2+1\n",
    "\n",
    "# Plot the segmentation\n",
    "#plt.imshow(segmentation2, cmap = 'viridis')\n",
    "#plt.show()\n",
    "\n",
    "# Fitted parameters\n",
    "#c = km_model2.cluster_centers_\n",
    "#print(c.round(1).flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction\n",
    "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-2-Introduction-to-machine-learning/imgs/malignant.gif\" width = \"150\" style=\"float: right;\">\n",
    "\n",
    "This example demonstrates dimensionality with [Principal Component Analysis](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html).\n",
    "\n",
    "We will use a breast cancer dataset that is included with Scikit-Learn. It contains 30 features - properties of cells extracted using biopsy and photographed under a microscope - and labels whether the tumour was malignant or benign.\n",
    "\n",
    "We will reduce the dimensionality of the feature vectors to 2 to visualise the patterns in this high-dimensional dataset.\n",
    "\n",
    "We will cover dimensionality reduction in more detail in Week 5.\n",
    "\n",
    "### Prepare the data\n",
    "\n",
    "First we will load the dataset and check its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "bc = datasets.load_breast_cancer()\n",
    "\n",
    "print(bc.keys())\n",
    "print('\\n Features: \\n', bc.feature_names)\n",
    "print('\\n Labels: ', bc.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will extract the feature matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = bc.data\n",
    "print('We have {} features and {} samples.'.format(X.shape[1], X.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model\n",
    "\n",
    "We will use principal component analysis (PCA) with 2 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca_model = PCA(n_components = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model \n",
    "\n",
    "The model is fitted using function `fit`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_model.fit(X)\n",
    "pc1 = pca_model.components_[0]\n",
    "pc2 = pca_model.components_[1]\n",
    "print('Component 1: \\n', np.around(pc1, 3))\n",
    "print('Component 2: \\n', np.around(pc2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the features\n",
    "\n",
    "Rather than predicting some outputs, the PCA model transforms the features using the function `transform`. We can check that transformed feature vectors are now 2-dimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced = pca_model.transform(X)\n",
    "print('We have {} features.'.format(X_reduced.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the data\n",
    "\n",
    "The code below visualises the projection of the breast cancer data on the first two principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Labels = bc.target\n",
    "\n",
    "plt.plot(X_reduced[:, 0][Labels==0], X_reduced[:, 1][Labels==0], \"r*\", alpha = 0.5, label = 'malignant')\n",
    "plt.plot(X_reduced[:, 0][Labels==1], X_reduced[:, 1][Labels==1], \"bo\", alpha = 0.5, label = 'benign')\n",
    "\n",
    "plt.title('Dimensionality reduction')\n",
    "plt.xlabel('Principal component 1')\n",
    "plt.ylabel('Principal component 2')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "\n",
    "The goal of this exercise is to compare the performance of a `Perceptron` classifier to detect breast cancer using the original and the reduced features.\n",
    "\n",
    "First we will load the dataset and extract the feature matrix and label vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "# Load the data\n",
    "bc = datasets.load_breast_cancer()\n",
    "\n",
    "# Original dataset - feature matrix and label vector\n",
    "X = bc.data\n",
    "y = bc.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to apply PCA to the feature matrix and check that the reduced matrix has only two features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create PCA model with 2 components\n",
    "pca_model2 = None # Edit this line\n",
    "\n",
    "# Fit the model\n",
    "# Add your code here\n",
    "\n",
    "# Transform the feature matrix to 2-dimensional space\n",
    "X_reduced = None # Edit this line\n",
    "\n",
    "# Print number of features\n",
    "#print('We have {} features.'.format(X_reduced.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now compare the accuracy of classification using `Perceptron` when fitting to the original feature matrix `X` or reduced feature matrix `X_reduced`. Note that the labels vector `y` is the same in both cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create Perceptron model\n",
    "clf = None # Edit this line\n",
    "\n",
    "# Fit model using the original dataset\n",
    "# Add your code here\n",
    "\n",
    "# Calculate accuracy using the original dataset\n",
    "#acc_orig = clf.score(scaler.fit_transform(X), y)\n",
    "#print('Original dataset accuracy: ',round(acc_orig, 2))\n",
    "\n",
    "# Fit model using the reduced dataset\n",
    "# Add your code here\n",
    "\n",
    "# Calculate accuracy using the reduced dataset\n",
    "#acc_reduced = clf.score(scaler.fit_transform(X_reduced), y)\n",
    "#print('Reduced dataset accuracy: ',round(acc_reduced, 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
