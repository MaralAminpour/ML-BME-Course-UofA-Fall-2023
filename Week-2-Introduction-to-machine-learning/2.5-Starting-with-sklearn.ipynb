{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaralAminpour/ML-BME-Course-UofA-Fall-2023/blob/main/Week-2-Introduction-to-machine-learning/2.5-Starting-with-sklearn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-L1t6b71A98"
      },
      "source": [
        "# Machine Learning Basics\n",
        "## Starting with Scikit-learn\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-2-Introduction-to-machine-learning/imgs/Scikit_learn_logo_.png\" width = \"150\" style=\"float: right;\">\n",
        "\n",
        "In this notebook we will introduce the basics of the Scikit-learn application programming interface (API). Scikit-learn supports many different machine learning models, and also has many useful supporting functions. One nice feature of scikit-learn is that different models have **similar interfaces**, meaning there will be **minimal code changes required** to switch between one type of model to another.\n",
        "\n",
        "We will cover very simple examples of (1) regression, (2) classification, (3) clustering and (4) dimensionality reduction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huRgUuCi1A9-"
      },
      "source": [
        "## **Example 1. Regression**\n",
        "\n",
        "We'll explore the topic of regression in depth in Week 4. This is just a simple overview for now. One classic example that's super useful for understanding regression is house pricing. Take note of this house pricing example—it's an excellent hands-on exercise you can try at home, so feel free to give it a try before next week!\n",
        "\n",
        "###**Linear regression**\n",
        "\n",
        "Regression problems we are** predicting a numerical quantity** (unlike with classification where we are **predicting a nominal label**).\n",
        "The simplest form of regression is **linear regression** (also known as the least squares method). This is a problem from calculus that **tries to find a line that minimizes the squared distance from a collection of points.**\n",
        "\n",
        "***From Wikipedia:** In linear regression, the **observations** (red) are assumed to be the result of **random deviations** (green) from an **underlying relationship** (blue) between a dependent variable (y) and an independent variable (x).*\n",
        "\n",
        "![](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Linear_regression.svg/200px-Linear_regression.svg.png)\n",
        "\n",
        "![](https://upload.wikimedia.org/wikipedia/commons/thumb/5/53/Linear_least_squares_example2.png/190px-Linear_least_squares_example2.png)\n",
        "\n",
        "\n",
        "**Linear regression is like drawing a straight line through a bunch of points on a graph to find the best \"average\" line that represents the relationship between two things**. For example, you might want to know how the size of a house affects its price. With linear regression, you use existing data to find a formula that can then predict future prices based on the size of the house. It's a way to make educated guesses based on patterns you see in the data you already have.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "\n",
        "### **Predicting house prices: A case study in regression**\n",
        "\n",
        "One of the classic examples used to teach regression is house pricing. In this context, the **\"intelligence\" we're seeking to derive is the potential value of a house that's not currently on the market.** Our goal is to estimate this unknown value using data.\n",
        "\n",
        "So, where does this data originate? For this case, we'll be studying other houses, using their **sale prices as a basis to infer the potential value of our target house**. But the sale price isn't the only factor we're considering. We'll also look at various characteristics of these houses, such as the number of bedrooms, bathrooms, and the total square footage, among other things.\n",
        "\n",
        "**Our machine learning strategy will be to establish a relationship between these house features and their sale prices.** By successfully modeling this relationship with our data from past house sales, we can then use it to predict the sale price of a new house. In simpler terms, we'll be taking the new house's attributes to forecast its potential sale price. This practical technique is known as regression.\n",
        "\n",
        "**Suggested videos and tutorials about predicting house prices**\n",
        "\n",
        "1. Simple Linear Regression by Emily Fox (16 minutes)\n",
        "\n",
        "check this video about gouse porice prediction and linear regression [here ](https://youtu.be/xsTmUtwUg9Q)\n",
        "\n",
        "2. check the related tutorial by Smith White [here](https://colab.research.google.com/github/ualberta-rcg/python-machine-learning/blob/main/notebooks/02-regression.ipynb).\n",
        "\n",
        "\n",
        "After this quick intro, let's roll up our sleeves and get started on our example:\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "xB_FAr6q9o9c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Predict the brain volumes from GA**\n",
        "\n",
        "The first example demonstrates the __regressor__ object API.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-2-Introduction-to-machine-learning/imgs/brain-volume.png\" width = \"150\" style=\"float: right;\">\n",
        "\n",
        "The file 'neonatal_brain_volumes.csv' contains gestational ages (GA) and brain volumes of premature babies. We will fit a `LinearRegression` model to **predict the brain volumes from GA.** This is an example of a regression problem.\n",
        "\n",
        "The image on the right is an example of a **scan** that could be used to estimate brain volumes; however **the data we'll be using has the brain volumes already computed.**\n",
        "\n",
        "**gestational ages (GA)**: The gestational age of a baby refers to the length of time a baby has been developing in the uterus, starting from the first day of the mother's last menstrual period to the current date. It is measured in weeks and days."
      ],
      "metadata": {
        "id": "g6bi60S39vp3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ew2cYjKY1A9-"
      },
      "outputs": [],
      "source": [
        "# This code will download the required data files from GitHub\n",
        "# This is important so you can easily run the notebook on Colab\n",
        "# Or if you want to run an example but have downloaded only notebook\n",
        "# We'll use code like this in most of our notebooks\n",
        "# Normally we'll put files we download in a directory called \"temp\"\n",
        "#    This will help keep everything organized\n",
        "\n",
        "import requests, os\n",
        "\n",
        "# Download data from GitHub\n",
        "def download_data(source, dest):\n",
        "    base_url = 'https://raw.githubusercontent.com/'\n",
        "    owner = 'SirTurtle'\n",
        "    repo = 'ML-BME-UofA-data'\n",
        "    branch = 'main'\n",
        "    url = '{}/{}/{}/{}/{}'.format(base_url, owner, repo, branch, source)\n",
        "    r = requests.get(url)\n",
        "    f = open(dest, 'wb')\n",
        "    f.write(r.content)\n",
        "    f.close()\n",
        "\n",
        "# Create the temp directory, if it doesn't already exist\n",
        "if not os.path.exists('temp'):\n",
        "   os.makedirs('temp')\n",
        "\n",
        "download_data('Week-2-Introduction-to-machine-learning/data/neonatal_brain_volumes.csv', 'temp/neonatal_brain_volumes.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgLweDY-1A9_"
      },
      "source": [
        "### Prepare the data\n",
        "First we will **import the data file using the `pandas` package** and check its content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8AdCZdjh1A9_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read file into a dataframe object\n",
        "df = pd.read_csv('temp/neonatal_brain_volumes.csv')\n",
        "\n",
        "# Print the first few lines\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3g-_Yqe1A-A"
      },
      "source": [
        "You could also open the data file with Excel or any program that can read a CSV formatted file to examine its contents.\n",
        "\n",
        "#### Data dictionary\n",
        "\n",
        "__Tip:__ It is a very good practice to create a data dictionary that describes **the structure of your data**. The data dictionary will have text about each column. It is useful to include info such as **units and acceptable values** when possible.\n",
        "\n",
        "*GA*: gestational age (in weeks)\n",
        "\n",
        "*brain volume*: measured brain volume (in mL)\n",
        "\n",
        "Next, we will **convert the data into numpy arrays**. We will create a **feature matrix containing the 'GA' column** and the target vector **containing the 'brain volume' column**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phk-3Oi-1A-A"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Convert dataframe object into a numpy array\n",
        "brain_volume_data = df.to_numpy()\n",
        "\n",
        "# Create the feature matrix and convert it to a 2D numpy array\n",
        "# In this case we have only one feature: \"GA\" (gestational age)\n",
        "# X is a conventional name for a feature matrix\n",
        "X = brain_volume_data[:,0].reshape(-1, 1)\n",
        "print('Number of samples: ', X.shape[0])\n",
        "print('Number of features: ', X.shape[1])\n",
        "print('Feature matrix X dimensions: ', X.shape)\n",
        "\n",
        "# Create the target vector\n",
        "# In this case the target is \"brain volume\"\n",
        "# y is a conventional name for target vector\n",
        "y = brain_volume_data[:,1]\n",
        "print('Target vector y dimensions: ', y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZ2ibvC51A-A"
      },
      "source": [
        "Note that in the example our feature matrix for each data sample is only dimension N x 1 since we have only a single feature. But generally, there will be multiple features for each sample.\n",
        "\n",
        "We've selected brain volume as the target that we'll build our model to predict, but we could have done the reverse and built a model to predict GA from brain volume.\n",
        "\n",
        "Here would be a good time to do some exploratory data analysis (EDA), but for this example, we'll just go straight to modeling. In a real machine learning protocol, we would need to do additional steps here, such as splitting our data into train and test sets."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Note about reshape (-1,1)**: In the context of programming and data\n",
        "science, reshape(-1, 1) is often used to change the shape of an array or a data series into a column vector.\n",
        "\n",
        "Let's break down what reshape(-1, 1) means:\n",
        "\n",
        "**reshape:** It is a function/method used to give a new shape to an array without changing its data.\n",
        "\n",
        "**-1:** This is used as a placeholder for an unknown dimension that should be inferred from the length of the array. The -1 tells the reshape method to automatically calculate the number of rows that is necessary to maintain the number of elements in the array.\n",
        "\n",
        "**1: **This indicates that we want each element in the array to be contained within its own array - essentially turning the array into a column vector with one element per row.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "wW57Q1NW_MnQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQL3yMlo1A-A"
      },
      "source": [
        "### Create the model\n",
        "Now we select and create the [linear regression model](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html). A linear regression model with one feature (univariate) has this functional form:\n",
        "\n",
        "$y=w_0+w_1x$\n",
        "\n",
        "It is just a line! $w_0$ and $w_1$ are the parameters (weights) for this model.\n",
        "\n",
        "**The object we instantiate will be an untrained linear regression model that we can train on any type of data.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WpjFwHBZ1A-A"
      },
      "outputs": [],
      "source": [
        "# Note that to import from Scikit-learn use the name 'sklearn'\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Create the model\n",
        "lr_model = LinearRegression()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRNrrih11A-A"
      },
      "source": [
        "### Fit the model\n",
        "The next step is to fit the model to the training data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtXXyjHm1A-B"
      },
      "outputs": [],
      "source": [
        "# Fit the model\n",
        "# Note the lr_model object is modified in place\n",
        "lr_model.fit(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**lr_model.fit(X, y)** is a command to train a linear regression model (denoted here as lr_model) using a **set of input features (X)** and corresponding **output labels (y)**.\n",
        "\n",
        "**X: **This is generally a two-dimensional array or a **pandas dataframe** where** each row represents a distinct data point** and **each column represents a different feature**. Essentially, it contains the independent variables that are used to predict the target variable (y).\n",
        "\n",
        "**y:** This is generally a** one-dimensional array or a pandas series** that **contains the target variable or output label**s that we are trying to predict. The y array has as many elements as **there are rows in the X array, with each element of y corresponding to a row in X.**\n",
        "\n",
        "When you run **lr_model.fit(X, y)**, the algorithm will learn the relationship between **X and y** during the training process, by finding the best-fit parameters that minimize the error between the actual and predicted values of **y**."
      ],
      "metadata": {
        "id": "0fTd6DRk_wzX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "js3MoMtS1A-B"
      },
      "source": [
        "We can view the coefficients of the linear model\n",
        "\n",
        "$y=w_0+w_1x$\n",
        "\n",
        "which we fitted to the data, as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLw4YNO11A-B"
      },
      "outputs": [],
      "source": [
        "w0 = lr_model.intercept_\n",
        "print('w0:', round(w0))\n",
        "\n",
        "w1 = lr_model.coef_[0]\n",
        "print('w1:', round(w1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sc_VNq_L1A-B"
      },
      "source": [
        "### Evaluate the model\n",
        "**The model accuracy** can be evaluated by calling the function **`score`**. For regression models in scikit-learn the score function returns the **$R^2$ score**. This is the most commonly used **metric** to evaluate the performance of regression models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHzchmJw1A-B"
      },
      "outputs": [],
      "source": [
        "r2 = lr_model.score(X, y)\n",
        "print('R2 score:', round(r2, 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3QZlI-j1A-B"
      },
      "source": [
        "How good is this score? 1 is perfect (the best score). So 0.84 is a decent score.\n",
        "\n",
        "**Note**: The R-squared score in regression is like a **report card for your model**. **It measures how well the model's predictions match the actual data.** The score ranges from 0 to 1. A score close to 1 means your model does an **excellent job at explaining the variation in the data**, while a low score suggests **there's room for improvement**.\n",
        "\n",
        "**Keep in mind** that a high R-squared doesn't guarantee your model is perfect; it's just one indicator of how well it's doing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MKLt6Yc1A-C"
      },
      "source": [
        "### Predictions on new data\n",
        "\n",
        "When we're working with machine learning models, especially ones for predicting continuous outcomes, **it's super helpful to visualize how well our model fits the data.** To do that, we need to sample the \"feature space,\" which is just a fancy term for the **range of all possible feature** values—in this case, gestational age (GA).\n",
        "\n",
        "So, we create a **grid of 10 evenly-spaced points** that cover the smallest to the largest GA values in our dataset. This grid allows us to make predictions at various points, giving us a nice, smooth curve when we plot the data and the model together.\n",
        "\n",
        "**And yep, even though we're dealing with only one feature (GA), we still need a 2D array because most machine learning functions are designed to handle multiple features. So, that's why we reshape the array to make it a column vector.**\n",
        "\n",
        "This will make it super easy to see how well our model fits when we plot it against our actual data.\n",
        "\n",
        "**extra**  it looks like you're creating a new feature space X_model using NumPy's linspace function. This new feature space consists of 10 evenly-spaced points between the minimum and maximum values of your original feature set X.\n",
        "\n",
        "The .reshape(-1, 1) part is making sure that the shape of X_model is a **column vecto**r, which is often required by machine learning models.\n",
        "\n",
        "Finally, print('Feature space:\\n', np.around(X_model).T) **is printing out the transposed version of this new feature space, after rounding each value to the nearest integer.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mu8gKmRv1A-C"
      },
      "outputs": [],
      "source": [
        "X_model = np.linspace(np.min(X), np.max(X), 10).reshape(-1, 1)\n",
        "print('Feature space:\\n', np.around(X_model).T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSuqp8j_1A-C"
      },
      "source": [
        "We are now ready to predict the target values for these new samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lM6Jf2X_1A-C"
      },
      "outputs": [],
      "source": [
        "y_model = lr_model.predict(X_model)\n",
        "print('Predicted targets for the feature space:\\n', np.around(y_model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZVhTiEr1A-C"
      },
      "source": [
        "### Plot the result\n",
        "We will plot the result using `matplotlib` library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVEH4K3i1A-C"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Plot the data\n",
        "plt.plot(X, y, 'bo', alpha = 0.5, label = 'samples')\n",
        "\n",
        "# Plot the model\n",
        "plt.plot(X_model, y_model, 'k', label = 'model')\n",
        "\n",
        "# Annotate the plot\n",
        "plt.title('Regression')\n",
        "plt.xlabel('Feature: Gestational age at scan')\n",
        "plt.ylabel('Target value: brain volume in mL')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dpv9n1Qi1A-C"
      },
      "source": [
        "Note that the points mostly lie near the line defined by the linear model. This is a visual representation of the $R^2$ score.\n",
        "\n",
        "**Note:** When the data points are closely packed around the line produced by the linear model, **it's a visual indication that the model is a good fit for the data. In technical terms, a high $R^2$ score would mean that a large percentage of the variance in the target variable (in this case, brain volume in mL) can be explained by the feature (Gestational age at scan).**\n",
        "\n",
        "So, when you look at the plot and see the points hugging the line, you can be pretty confident that your **$R^2$ score is going to be high**, indicating a good model fit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKpjR-l_1A-D"
      },
      "source": [
        "### Exercise 1\n",
        "\n",
        "It is now your turn to write the solution to the following problem: You would like to predict the GA of a preterm baby from the measurement of the brain volume. Note that in this case the GA and volumes switched roles - volume is a feature and GA is the target value. The feature matrix `X1` and target vector `y1` were created for you.\n",
        "\n",
        "Write code to\n",
        "* Create the `LinearRegression` model\n",
        "* Fit the model\n",
        "* Calculate the $R^2$ score\n",
        "\n",
        "Commands for printing out the score and the equation of the fitted model were created for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iO1OTI2n1A-D"
      },
      "outputs": [],
      "source": [
        "# Create the feature matrix using brain volumes\n",
        "X1 = brain_volume_data[:,1].reshape(-1,1)\n",
        "\n",
        "# Create the target vector using GA\n",
        "y1 = brain_volume_data[:,0]\n",
        "\n",
        "# Create the model\n",
        "lr_model1 = None # Edit this line\n",
        "\n",
        "# Fit the model\n",
        "# Add your code here\n",
        "\n",
        "# Calculate the R2 score\n",
        "r2_1 = None # Edit this line\n",
        "\n",
        "# Print the score\n",
        "#print('R2 score:', round(r2_1,2))\n",
        "\n",
        "# Print the equation of the fitted model\n",
        "#print('Fitted model: y = {} + {}x'.format(round(lr_model1.intercept_), round(lr_model1.coef_[0],2)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 2. Classification\n",
        "\n",
        "We'll explore the topic of classification in depth in Week 3. This is just a simple overview for now.\n",
        "\n",
        "In machine learning, classification refers to the task of assigning a label to an input based on certain features or attributes. Essentially, it's like sorting things into different buckets or groups.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-2-Introduction-to-machine-learning/imgs/apple_oranes.png\" width = \"400\" style=\"float: right;\">\n",
        "\n",
        "For example, imagine you have a bunch of fruits, and you want to classify them as either \"apple,\" \"banana,\" or \"cherry.\" You could use features like color, shape, and size to make that decision. In machine learning, you would use a classification algorithm to learn the characteristics that define each fruit type based on training data. Once the algorithm is trained, you could give it a new, unknown fruit, and it would be able to classify it into one of the existing categories.\n",
        "\n",
        "Common algorithms used for classification tasks include Logistic Regression, Decision Trees, and Support Vector Machines, among others. Each algorithm has its own way of dividing the feature space into regions, each corresponding to a particular label or class.\n",
        "\n",
        "So, whether you're sorting fruits, identifying spam emails, or recognizing handwritten digits, classification algorithms are the go-to tools for these types of tasks.\n",
        "\n",
        "Now, let's focus on our main example:"
      ],
      "metadata": {
        "id": "GtJP79S9D9Ja"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTTY0I261A-D"
      },
      "source": [
        "###**Predict the heart failure from EF and GLS**\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-2-Introduction-to-machine-learning/imgs/HeartSegmentation.gif\" width = \"150\" style=\"float: right;\">\n",
        "\n",
        "This example demonstrates the __classifier__ object API.\n",
        "\n",
        "The file 'heart_failure_data.csv' contains **features Ejection Fraction** (EF), **Global Longitudinal Strain** (GLS) and a **label indicating whether patient has heart failure (HF)**. We will fit a linear `Perceptron` model to predict the heart failure from EF and GLS.\n",
        "\n",
        "A [linear perceptron](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html) is simple model that **will find a line** (or in higher dimensions a plane or hyper-plane) that **divides the data into classes. This is the *decision boundary*.** Perceptrons can be used for binary classification (as here), and also for multiclass problems.\n",
        "\n",
        "We will cover classification in more detail in **Week 3**.\n",
        "\n",
        "**Note** **Global Longitudinal Strain (GLS)** is a modern echocardiographic parameter that provides a more detailed analysis of **cardiac function** compared to traditional measurements like Ejection Fraction (EF). While EF gives an **overall idea of how well the heart is pumping**, GLS specifically looks at the **ability of the heart muscle to contract and stretch along its length**.\n",
        "\n",
        "read more about features Ejection Fraction (EF), Global Longitudinal Strain (GLS) [read more](https://docs.google.com/document/d/1O8onYcy3Q_J6qBC4zzyOwqAktgPGM0Izqmudi-Qce8Q/edit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNzSLOyk1A-D"
      },
      "outputs": [],
      "source": [
        "# Download the data\n",
        "download_data('Week-2-Introduction-to-machine-learning/data/heart_failure_data.csv', 'temp/heart_failure_data.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzoIhTjE1A-D"
      },
      "source": [
        "### Prepare the data\n",
        "First we will import the file using the `pandas` package and check its content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRkEqd811A-E"
      },
      "outputs": [],
      "source": [
        "# Read data file into a dataframe object\n",
        "df = pd.read_csv('temp/heart_failure_data.csv')\n",
        "\n",
        "# Print the first few lines\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0ZUvxSs1A-E"
      },
      "source": [
        "#### Data dictionary\n",
        "\n",
        "*EF*: Ejection Fraction. A measurement of how much blood the left ventricle pumps out with each contraction. Expressed as a percent in the range 0 to 100.\n",
        "\n",
        "*GLS*: Global Longitudinal Strain. A measurement of myocardial deformation along the longitudinal cardiac axis. Expressed as a negative percent in the range 0 to -100.\n",
        "\n",
        "*HF*: Heart Failure class.\n",
        "- 0 = Healthy\n",
        "- 1 = Heart failure\n",
        "\n",
        "The code below creates the feature matrix `X` and label vector `y`. Note that now the feature matrix has dimension N x 2.\n",
        "\n",
        "Also, we will do some preprocessing on the data: we will scale the features to have zero mean and unit variance across the dataset.\n",
        "\n",
        "*Tip*: This preprocessing is called *standardization* or *normalization*. This is important for many types of models and will make it easier for the model to fit your data. It is particularly important if your features differ by orders of magnitude."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_1yCD2K1A-E"
      },
      "outputs": [],
      "source": [
        "# Import and create an object to scale the features\n",
        "# to have zero mean and unit variance\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Convert to numpy\n",
        "heart_failure_data = df.to_numpy()\n",
        "\n",
        "# Create feature matrix containing EF and GLS\n",
        "X = scaler.fit_transform(heart_failure_data[:,:2])\n",
        "print('Feature matrix X dimensions: ', X.shape)\n",
        "\n",
        "# Create target vector containing HF\n",
        "y = heart_failure_data[:,2]\n",
        "print('Target vector y dimensions: ', y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlNeFNPT1A-E"
      },
      "source": [
        "### Create the model\n",
        "This code creates the `Perceptron` model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "flR5whTz1A-F"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Perceptron\n",
        "\n",
        "# Create the model\n",
        "p_model = Perceptron()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7miOEWGC1A-F"
      },
      "source": [
        "### Fit the model\n",
        "This code fits the `Perceptron` model to the training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0za5lIy31A-F"
      },
      "outputs": [],
      "source": [
        "# Fit the model\n",
        "p_model.fit(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQanvPL-1A-F"
      },
      "source": [
        "The coefficients of the fitted decision function\n",
        "$h(\\mathbf{x})=w_0+w_1x_1+w_2x_2$ can be accessed as follows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5CxFtxL-1A-G"
      },
      "outputs": [],
      "source": [
        "w0 = p_model.intercept_[0]\n",
        "print('w0:', round(w0))\n",
        "\n",
        "w1 = p_model.coef_[0][0]\n",
        "print('w1:', round(w1))\n",
        "\n",
        "w2 = p_model.coef_[0][1]\n",
        "print('w2:', round(w2,2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITLvy4op1A-G"
      },
      "source": [
        "### Evaluate the model\n",
        "For classification models, the function `score` returns accuracy, which is the proportion of the correctly classified samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Acl8w0H51A-G"
      },
      "outputs": [],
      "source": [
        "# Calculate accuracy\n",
        "accuracy = p_model.score(X, y)\n",
        "\n",
        "# Print the score\n",
        "print('Accuracy score:', round(accuracy,2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9XiJh-h1A-G"
      },
      "source": [
        "Accuracy will be in the range [0,1], with 1 being perfect accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_LS_eSo1A-H"
      },
      "source": [
        "### Plot the model\n",
        "The result of the classification is plotted below.\n",
        "\n",
        "Note that for this example is easy to visualize the decision boundary since we have 2 features. In higher dimensions, this type of visualization would not work!\n",
        "\n",
        "Remember also that we standardized our features before fitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iaN3bTjQ1A-H"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Plot data\n",
        "plt.plot(X[y==0,0], X[y==0,1], 'bo', alpha=0.75, label = 'Healthy')\n",
        "plt.plot(X[y==1,0], X[y==1,1], 'r*', alpha=1, label = 'Heart Failure')\n",
        "\n",
        "# Plot decision boundary\n",
        "# Define y-coordinates\n",
        "x2 = np.array([X[:,1].min(), X[:,1].max()])\n",
        "\n",
        "# Define x-coordinates\n",
        "x1 = -(w0 + w2*x2)/w1\n",
        "\n",
        "# Plot\n",
        "plt.plot(x1, x2, \"k-\")\n",
        "\n",
        "plt.legend()\n",
        "plt.title('Classification')\n",
        "plt.xlabel('Feature 1: Ejection Fraction')\n",
        "plt.ylabel('Feature 2: Global Longitudinal Strain')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_UHa0Df1A-H"
      },
      "source": [
        "The line is our model's decision boundary between the two classes (healthy and heart failure)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eodEhzTi1A-H"
      },
      "source": [
        "### Exercise 2\n",
        "\n",
        "Write a solution to the following problem: You would like to find out whether using only Ejection Fraction (EF) would be sufficient to predict heart failure (HF).\n",
        "\n",
        "Write code to\n",
        "* Create the new feature matrix and the target vector\n",
        "* Fit the model and calculate the accuracy score\n",
        "* Print the equation of the decision boundary\n",
        "\n",
        "What is the drop in accuracy compared to using both features (EF and GLS)?\n",
        "\n",
        "*Hint*: If your accuracy is not good, try changing the learning rate by changing the eta0 learning rate hyperparameter from the default:\n",
        "`Perceptron(eta0=0.2)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zoa65Rw1A-H"
      },
      "outputs": [],
      "source": [
        "# Create feature matrix containing EF only\n",
        "X2 = None # Edit this line\n",
        "\n",
        "# Create label vector containing HF\n",
        "y2 = None # Edit this line\n",
        "\n",
        "# Create the model\n",
        "from sklearn.linear_model import Perceptron\n",
        "p_model2 = Perceptron(eta0=0.2)\n",
        "\n",
        "# Fit the model\n",
        "# Add your code here\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy2 = None # Edit this line\n",
        "\n",
        "# Print the score\n",
        "#print('Accuracy score is:', round(accuracy2,2))\n",
        "\n",
        "# Print the decision boundary\n",
        "#print('Decision boundary: {} + {}x = 0'.format(round(p_model2.intercept_[0],2),round(p_model2.coef_[0][0],2)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zlc1R9-p1A-I"
      },
      "source": [
        "## Example 3. Clustering\n",
        "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-2-Introduction-to-machine-learning/imgs/T1.png\" width = \"150\" style=\"float: right;\">\n",
        "\n",
        "This example demonstrates the __cluster__ object API.\n",
        "\n",
        "The file 'T1.png' contains a slice of T1-weighted magnetic resonance image (MRI) of an adult brain. The non-brain tissues have been removed in preprocessing. We perform `KMeans`) clustering to segment white matter (WM), grey matter (GM) and cerebrospinal fluid (CSF) in this image.\n",
        "\n",
        "This is an example of unsupervised learning. [KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) is one of many clustering algorithms available in Scikit-Learn.\n",
        "\n",
        "Note that in the previous examples, the data consisted of features in tabular format. In this case, the data is a 2D image. Each pixel in the image could be black (representing non-brain tissues) or a shade of gray representing different types of brain tissue.\n",
        "\n",
        "We will cover clustering in more detail in Week 5.\n",
        "\n",
        "### Prepare the data\n",
        "First we will load the image using the `matplotlib` function `imread` and display it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jzC0J-4K1A-I"
      },
      "outputs": [],
      "source": [
        "# Download data\n",
        "download_data('Week-2-Introduction-to-machine-learning/data/T1.png', 'temp/T1.png')\n",
        "\n",
        "# Load image\n",
        "T1 = plt.imread('temp/T1.png')\n",
        "\n",
        "# Display image\n",
        "plt.imshow(T1, cmap = 'gray')\n",
        "\n",
        "# Print shape\n",
        "print('Image dimensions: ', T1.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7D1DpiJa1A-I"
      },
      "source": [
        "Visually it is easy to identify the 3 kinds of brain tissue based on the shades of gray. We will try to use clustering to identify these 3 kinds of brain tissue.\n",
        "\n",
        "Next we need to convert the image into the feature matrix suitable for processing using `sklearn` functions. First, we need to remove the background pixels that have values zero. Then we need to create the feature matrix as a 2D array object but with only one feature in each row.\n",
        "\n",
        "Note that we are \"flattening\" the 2D image, turning it into a single vector along x-axis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XI-hAdYQ1A-I"
      },
      "outputs": [],
      "source": [
        "# Find all the non-zero elements\n",
        "ind = T1 > 0\n",
        "\n",
        "# Create the feature matrix with the correct dimensions\n",
        "X = T1[ind].reshape(-1,1)\n",
        "print('Shape of the feature matrix X is', X.shape)\n",
        "\n",
        "print('The first 10 entries of the feature matrix:\\n', X[1:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcCiSCwy1A-I"
      },
      "source": [
        "The data values are what we visualized with different shades of gray. Notice that the numbers are not exactly the same for each brain region. We will use clustering to find boundaries between the grayscale values for each of the 3 classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nt9vjrA11A-I"
      },
      "source": [
        "### Create and fit the model\n",
        "\n",
        "Now we are ready to perform k-means clustering into 3 classes, which will correspond to three brain tissues: WM, GM and CSF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZc9bV_l1A-I"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Create the model\n",
        "# random_state will initialize the random seed to fixed value\n",
        "# n_init is the number of times the algorithm is run with different cluster centroid seeds\n",
        "km_model = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
        "\n",
        "# Fit the model\n",
        "km_model.fit(X)\n",
        "\n",
        "# Fitted parameters\n",
        "c = km_model.cluster_centers_\n",
        "print(c.round(1).flatten())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWb5y_KC1A-I"
      },
      "source": [
        "### Predict the labels\n",
        "\n",
        "The next step is the predict the labels. Note that this time we did not calculate any score - this is because we do not have the training labels, so cannot evaluate the performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DpyJeqGi1A-J"
      },
      "outputs": [],
      "source": [
        "# Predict the labels\n",
        "y_predict = km_model.predict(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKhYzkpV1A-J"
      },
      "source": [
        "### Plot the result\n",
        "To plot the result, we need to reshape the predicted labels to the original 2D array and then we can display it as an image. We are using the \"viridis\" color map so each of the 3 clusters will appear in a different color. You could try a different color map such as \"magma\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQWiT5Md1A-J"
      },
      "outputs": [],
      "source": [
        "# Create empty segmentation image\n",
        "segmentation = np.zeros(T1.shape)\n",
        "\n",
        "# Paste the labels into correct locations\n",
        "# We can do this since we stored the original index of each non-zero\n",
        "#    pixel in the ind array\n",
        "segmentation[ind] = y_predict + 1\n",
        "\n",
        "# Plot the segmentation\n",
        "plt.imshow(segmentation, cmap = 'viridis')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9P_UMlT1A-J"
      },
      "source": [
        "### Exercise 3\n",
        "\n",
        "Now perform the k-means clustering for the T2-weighted image 'T2.png'. Is the result similar?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rTO4w3E1A-J"
      },
      "outputs": [],
      "source": [
        "# Load image\n",
        "download_data('Week-2-Introduction-to-machine-learning/data/T2.png', 'temp/T2.png')\n",
        "T2 = plt.imread('temp/T2.png')\n",
        "\n",
        "# Select non-zero pixels\n",
        "ind2 = T2 > 0\n",
        "\n",
        "# Create feature matrix\n",
        "X2 = T2[ind2].reshape(-1,1)\n",
        "\n",
        "# Create the model\n",
        "km_model2 = None # Edit this line\n",
        "\n",
        "# Fit the model\n",
        "# Add your code here\n",
        "\n",
        "# Predict the labels\n",
        "y2 = None # Edit this line\n",
        "\n",
        "# Create segmentation image\n",
        "segmentation2 = None # Edit this line\n",
        "\n",
        "# Paste the labels into correct locations\n",
        "#segmentation2[ind2] = y2+1\n",
        "\n",
        "# Plot the segmentation\n",
        "#plt.imshow(segmentation2, cmap = 'viridis')\n",
        "#plt.show()\n",
        "\n",
        "# Fitted parameters\n",
        "#c = km_model2.cluster_centers_\n",
        "#print(c.round(1).flatten())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7h--wdo1A-J"
      },
      "source": [
        "## Dimensionality reduction\n",
        "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-2-Introduction-to-machine-learning/imgs/malignant.gif\" width = \"150\" style=\"float: right;\">\n",
        "\n",
        "This example demonstrates dimensionality with [Principal Component Analysis](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html).\n",
        "\n",
        "We will use a breast cancer dataset that is included with Scikit-Learn. It contains 30 features - properties of cells extracted using biopsy and photographed under a microscope - and labels whether the tumour was malignant or benign.\n",
        "\n",
        "We will reduce the dimensionality of the feature vectors to 2 to visualise the patterns in this high-dimensional dataset.\n",
        "\n",
        "We will cover dimensionality reduction in more detail in Week 5.\n",
        "\n",
        "### Prepare the data\n",
        "\n",
        "First we will load the dataset and check its structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6wZSTHF1A-J"
      },
      "outputs": [],
      "source": [
        "from sklearn import datasets\n",
        "\n",
        "bc = datasets.load_breast_cancer()\n",
        "\n",
        "print(bc.keys())\n",
        "print('\\n Features: \\n', bc.feature_names)\n",
        "print('\\n Labels: ', bc.target_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5g-A6NZ1A-J"
      },
      "source": [
        "Next we will extract the feature matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "op_yZZTo1A-J"
      },
      "outputs": [],
      "source": [
        "X = bc.data\n",
        "print('We have {} features and {} samples.'.format(X.shape[1], X.shape[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5O85V6E1A-J"
      },
      "source": [
        "### Create the model\n",
        "\n",
        "We will use principal component analysis (PCA) with 2 dimensions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KyLSj-rt1A-K"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca_model = PCA(n_components = 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGfogCmG1A-K"
      },
      "source": [
        "### Fit the model\n",
        "\n",
        "The model is fitted using function `fit`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppkbZowy1A-K"
      },
      "outputs": [],
      "source": [
        "pca_model.fit(X)\n",
        "pc1 = pca_model.components_[0]\n",
        "pc2 = pca_model.components_[1]\n",
        "print('Component 1: \\n', np.around(pc1, 3))\n",
        "print('Component 2: \\n', np.around(pc2, 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xU8vSulb1A-K"
      },
      "source": [
        "### Transform the features\n",
        "\n",
        "Rather than predicting some outputs, the PCA model transforms the features using the function `transform`. We can check that transformed feature vectors are now 2-dimensional."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ri6v9BOf1A-K"
      },
      "outputs": [],
      "source": [
        "X_reduced = pca_model.transform(X)\n",
        "print('We have {} features.'.format(X_reduced.shape[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNpl4XU71A-K"
      },
      "source": [
        "### Plot the data\n",
        "\n",
        "The code below visualises the projection of the breast cancer data on the first two principal components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29CvZr5H1A-K"
      },
      "outputs": [],
      "source": [
        "Labels = bc.target\n",
        "\n",
        "plt.plot(X_reduced[:, 0][Labels==0], X_reduced[:, 1][Labels==0], \"r*\", alpha = 0.5, label = 'malignant')\n",
        "plt.plot(X_reduced[:, 0][Labels==1], X_reduced[:, 1][Labels==1], \"bo\", alpha = 0.5, label = 'benign')\n",
        "\n",
        "plt.title('Dimensionality reduction')\n",
        "plt.xlabel('Principal component 1')\n",
        "plt.ylabel('Principal component 2')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1cy2kLu1A-K"
      },
      "source": [
        "### Exercise 4\n",
        "\n",
        "The goal of this exercise is to compare the performance of a `Perceptron` classifier to detect breast cancer using the original and the reduced features.\n",
        "\n",
        "First we will load the dataset and extract the feature matrix and label vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28f86Pm91A-K"
      },
      "outputs": [],
      "source": [
        "from sklearn import datasets\n",
        "\n",
        "# Load the data\n",
        "bc = datasets.load_breast_cancer()\n",
        "\n",
        "# Original dataset - feature matrix and label vector\n",
        "X = bc.data\n",
        "y = bc.target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9LEye5N1A-K"
      },
      "source": [
        "The next step is to apply PCA to the feature matrix and check that the reduced matrix has only two features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHCf7I7_1A-L"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Create PCA model with 2 components\n",
        "pca_model2 = None # Edit this line\n",
        "\n",
        "# Fit the model\n",
        "# Add your code here\n",
        "\n",
        "# Transform the feature matrix to 2-dimensional space\n",
        "X_reduced = None # Edit this line\n",
        "\n",
        "# Print number of features\n",
        "#print('We have {} features.'.format(X_reduced.shape[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOIhcccI1A-L"
      },
      "source": [
        "Let's now compare the accuracy of classification using `Perceptron` when fitting to the original feature matrix `X` or reduced feature matrix `X_reduced`. Note that the labels vector `y` is the same in both cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "As4YrCxL1A-L"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Create Perceptron model\n",
        "clf = None # Edit this line\n",
        "\n",
        "# Fit model using the original dataset\n",
        "# Add your code here\n",
        "\n",
        "# Calculate accuracy using the original dataset\n",
        "#acc_orig = clf.score(scaler.fit_transform(X), y)\n",
        "#print('Original dataset accuracy: ',round(acc_orig, 2))\n",
        "\n",
        "# Fit model using the reduced dataset\n",
        "# Add your code here\n",
        "\n",
        "# Calculate accuracy using the reduced dataset\n",
        "#acc_reduced = clf.score(scaler.fit_transform(X_reduced), y)\n",
        "#print('Reduced dataset accuracy: ',round(acc_reduced, 2))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}