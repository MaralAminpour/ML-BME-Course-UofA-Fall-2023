{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaralAminpour/ML-BME-Course-UofA-Fall-2023/blob/main/Week-7-Ensemble-learning/Ensemble_learning_introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Learning Objectives:\n",
        "\n",
        "This week, our focus is on Ensemble Learning:\n",
        "\n",
        "- **Understanding**: We want you to get a clear idea of what ensemble learning is and why it's useful.\n",
        "\n",
        "- **Types of Ensembles**: We'll explore two main types:\n",
        "  - **Homogeneous Ensembles**: Here, we use the same type of model multiple times. Examples include **bagging** **random forests** and **boosting**.\n",
        "\n",
        "  - **Heterogeneous Ensembles**: This involves using different types of models together. We'll touch on this briefly.\n",
        "\n",
        "  - **Sequential vs. Parallel Methods**: We'll differentiate between these two. Sequential methods build on the previous model's output, whereas parallel methods operate concurrently, aiming to improve accuracy and reduce overfitting.\n",
        "\n",
        "- **Practical Work**: We'll build a Bagging model using decision trees. If you missed our last session, don't worry; there's a ready-to-use code module for you.\n",
        "\n",
        "- **Using Scikit-Learn**: We'll practice implementing our learned methods using this popular tool.\n",
        "\n",
        "By the end of the week, you should:\n",
        "\n",
        "- Know what ensemble learning is and its benefits.\n",
        "\n",
        "- Understand the difference between bagging, boosting, and other methods.\n",
        "\n",
        "- Be able to create a basic Bagging model.\n",
        "\n",
        "- Know how to use these methods in Scikit-Learn.\n",
        "\n",
        "- Have a basic idea about stacking (another ensemble method)."
      ],
      "metadata": {
        "id": "hOshBkzFDub1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ensemble Learning\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-7-Ensemble-learning/imgs/orchestra.png' width=500px>\n",
        "\n",
        "Ensemble Learning is a technique in machine learning where multiple models (often referred to as \"base models\") are trained and their predictions are combined to produce a final result.\n",
        "\n",
        "Ensemble learning is a concept that has been around for a while. At its core, it's based on the principle that gathering opinions from multiple sources often leads to better outcomes than relying on just one source. This idea is commonly referred to as the \"wisdom of the crowd.\"\n",
        "\n",
        "This week, we'll apply this principle to machine learning. By combining simple models, like the decision trees we discussed last week, we aim to create more accurate and robust predictive models that perform better on new, unseen data.\n",
        "\n",
        "\n",
        "**Ensemble Learning Analogy** is like an orchestra performing a symphony. Each individual musician (model) has their own instrument (algorithm) and plays their part (makes a prediction). While each musician is talented on their own, it's when they all come together under the guidance of the conductor (ensemble method) that they produce a harmonious and powerful performance (more accurate prediction). Just as a single out-of-tune instrument can be drowned out by the harmony of the entire orchestra, the errors from a single model in ensemble learning can be offset by the correct predictions of other models. The collective output is often more beautiful and impactful than any solo performance."
      ],
      "metadata": {
        "id": "k7n5i7UxFkRu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Toy example (combining by voting)\n",
        "\n",
        "Let's break down a simple example to see how combining multiple systems can improve accuracy:\n",
        "\n",
        "Imagine you have 10 samples, and they're all marked as positive or \"1\".\n",
        "\n",
        "Now, we have three systems or classifiers named A, B, and C. Individually, each can correctly identify the samples about 70% of the time. Here's what their results look like:\n",
        "\n",
        "- A's results: {1,1,0,1,1,1,1,1,1,0}\n",
        "- B's results: {0,1,1,1,1,1,1,0,1,1}\n",
        "- C's results: {1,1,1,0,1,1,1,0,1,1}\n",
        "\n",
        "Even though they're right 70% of the time, they make different mistakes. They're not wrong about the same samples.\n",
        "\n",
        "So, how do we get better results? We use \"majority voting\". This means if at least two systems say a sample is \"1\", then we go with \"1\" as the final answer.\n",
        "\n",
        "When we do this for our example, the combined result is {1,1,1,1,1,1,1,1,1,1}. Now, they're right 90% of the time together!\n",
        "\n",
        "This magic happens because the systems' mistakes don't often overlap. When two systems get it right, they can correct the third system's mistake.\n",
        "\n",
        "However, this only works when the systems make different mistakes. If all three systems were often wrong about the same samples, then majority voting wouldn't improve accuracy."
      ],
      "metadata": {
        "id": "X5XR0KxhIqu8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly. Here's the revised content:\n",
        "\n",
        "When using ensembles that are correlated, the impact on improving accuracy can be minimal. Consider three correlated classifiers, each with a prediction accuracy of 70%. The predictions from these classifiers are:\n",
        "\n",
        "- $ \\hat{y}_A $ = {1,1,1,0,1,1,1,1,0,0} 70% correct\n",
        "- $ \\hat{y}_B $ = {1,1,1,0,1,1,1,1,0,0}  70% correct\n",
        "- $ \\hat{y}_C $ = {1,1,1,0,1,1,1,1,0,0}\n",
        " 70% correct\n",
        "- $ \\hat{y}_mv$ = {1,1,1,0,1,1,1,1,0,0}  70% correct\n",
        "\n",
        "Given that these predictions are identical and correlated, majority voting (represented as \\( \\hat{y}_{mv} \\)) will not enhance their combined performance. This cannot be improved through majority voting as each classifier individually achieves 70% accuracy.\n",
        "\n",
        "On the other hand, if our three models A, B, and C are highly correlated with completely overlapping predictions, then we see little or no improvement through majority voting."
      ],
      "metadata": {
        "id": "x3yc8a_DPt4C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basics of Probability Theory and Application to Classifiers\n",
        "\n",
        " To understand how combining multiple classifiers can enhance accuracy, we can use probability theory.\n",
        "\n",
        "1. **Basics of Probability Theory**: Before diving deep, it's important to grasp some fundamental probability concepts.\n",
        "\n",
        "    - **Independent Events**: These are events that don't influence one another. The probability of all of them occurring is the product of their individual probabilities. Mathematically:\n",
        "$ P(\\hat{y}_A \\cap \\hat{y}_B \\cap \\hat{y}_C) = P(\\hat{y}_A) \\times P(\\hat{y}_B) \\times P(\\hat{y}_C) $\n",
        "\n",
        "    - **Mutually Exclusive Events**: These are events that can't occur at the same time. The combined probability for such events is the sum of their individual probabilities. For instance:\n",
        "    \n",
        "      $$ P(\\hat{y}_A \\cup \\hat{y}_B) = P(\\hat{y}_A) + P(\\hat{y}_B) $$\n",
        "\n",
        "2. **Application to Classifiers**: When dealing with multiple classifiers, like in our example, these principles come into play. Each classifier might have an accuracy of 70%, but they don't necessarily make mistakes on the same data points. This variance can be leveraged.\n",
        "\n",
        "    - If one classifier mispredicts an outcome, the others might get it right. So, by taking a majority vote from the classifiers, we can 'correct' these individual errors.\n",
        "\n",
        "    - Picture this as having three friends, each good at answering 70% of the questions in a quiz. If they were to work together, taking a collective decision on each question, their combined expertise could lead to an even higher score.\n",
        "\n",
        "3. **Potential Outcomes**:\n",
        "    - **Best Case**: All classifiers predict the right outcome, leading to 100% accuracy for that particular instance.\n",
        "\n",
        "    - **Average Case**: Their combined expertise, through methods like majority voting, can push the accuracy up to 78% across a larger set of data.\n",
        "\n",
        "    - **Worst Case**: If all classifiers get it wrong, then combining them won't help, and the accuracy remains at 70%.\n",
        "\n",
        "In essence, the principle behind combining classifiers is to capitalize on their individual strengths and offset their weaknesses. This method often leads to a more robust and accurate predictive system, as confirmed by our probability theory."
      ],
      "metadata": {
        "id": "vWf_ZjmnNBia"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Toy example (combining by voting)\n",
        "\n",
        "In this example, we're working with three separate prediction models. Each of these models gets things right about 70% of the time ($ \\hat{y}_i $ are correct 70% of the time.).\n",
        "\n",
        "From this setup, four distinct situations can arise for any given example:\n",
        "\n",
        "\n",
        "1.** All three independent models are correct: **\n",
        "All three models hit the mark. Since each model's prediction doesn't affect the others, the combined chance of all being right is 0.7 (the chance one is right) times itself twice. Doing the math: $0.7 \\times 0.7 \\times 0.7 = 0.343$.\n",
        "\n",
        "$ P(\\hat{y}_1 = 1, \\hat{y}_2 = 1, \\hat{y}_3 = 1) = 0.7 * 0.7 * 0.7 = 0.343 $\n",
        "\n",
        "2. **Two models are correct (with 3 different possible mutually exclusive combinations)**:\n",
        "\n",
        "Two models get it right, and one doesn't. There are three ways this can play out: either the first, second, or third model could be the one that's off. These scenarios can't all occur at once, so they're separate from one another. For each of these cases, the probability is $0.7 \\times 0.7 \\times 0.3$. Since there are three such cases, the total chance for this situation is $3 \\times 0.7 \\times 0.7 \\times 0.3 = 0.441$.T\n",
        "\n",
        "- $ P(\\hat{y}_1 = 1, \\hat{y}_2 = 1, \\hat{y}_3 = 0) $ OR\n",
        "\n",
        "- $ P(\\hat{y}_1 = 1, \\hat{y}_2 = 0, \\hat{y}_3 = 1) $ OR\n",
        "\n",
        "- $ P(\\hat{y}_1 = 0, \\hat{y}_2 = 1, \\hat{y}_3 = 1) = 0.7 * 0.7 * 0.3 + 0.7 * 0.3 * 0.7 + 0.3 * 0.7 * 0.7 = 0.147 + 0.147 + 0.147 = 0.441 $\n",
        "\n",
        "3. **Two models are wrong:**\n",
        "\n",
        "It's also possible for two models to miss the mark and only one to get it right. Calculating in the same way, the combined chance for this scenario is 0.18.\n",
        "\n",
        "- $ P(\\hat{y}_1 = 1, \\hat{y}_2 = 0, \\hat{y}_3 = 0) $ OR\n",
        "\n",
        "- $ P(\\hat{y}_1 = 0, \\hat{y}_2 = 1, \\hat{y}_3 = 0) $ OR\n",
        "\n",
        "- $ P(\\hat{y}_1 = 0, \\hat{y}_2 = 0, \\hat{y}_3 = 1) = 0.7 * 0.3 * 0.3 + 0.3 * 0.7 * 0.3 + 0.3 * 0.3 * 0.7 = 0.063 + 0.063 + 0.063 = 0.189 $\n",
        "\n",
        "4. **All three are wrong:**\n",
        "\n",
        "Lastly, there's the slim chance that all three models mess up. This has a likelihood of $0.3 \\times 0.3 \\times 0.3 = 0.027$.\n",
        "\n",
        "$ P(\\hat{y}_1 = 0, \\hat{y}_2 = 0, \\hat{y}_3 = 0) = 0.3 * 0.3 * 0.3 = 0.027 $\n",
        "\n",
        "\n",
        "**Probability of all possible events** = 0.343 + 0.441 + 0.189 + 0.027 = 1\n",
        "\n",
        "When you put all these situations together, the probabilities add up to 1, which makes sense because these four scenarios cover every possible outcome.\n"
      ],
      "metadata": {
        "id": "8yOPDthtXIpz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## continued: Toy example (combining by voting)\n",
        "\n",
        "In this scenario (In this case, $ \\hat{y_i} $ are correct 70% of the time.), we want to know the chance that at least 2 predictions out of 3 are accurate. So, we need to look at situations where either 2 answers are spot on or all 3 nail it.\n",
        "\n",
        "From our calculations, the likelihood of getting 2 right combinations is 44%, which we represent as 0.441.\n",
        "\n",
        "For a majority vote with 3 members, we can expect 4 classes of mutually exclusive outcomes.\n",
        "In other words:\n",
        "Probability of all possible events = $0.343 + 0.441 + 0.189 + 0.027 = 1$\n",
        "\n",
        "If we toss in the times when all 3 predictions hit the mark, our overall accuracy jumps to 78%, denoted as 0.784.\n",
        "\n",
        "When taking a majority vote of 3 predictions, the result will be correct when at least 2 of the predictions are correct.\n",
        "- The total probability of any combination of results (where 2 are right) is $0.441$.\n",
        "- Thus majority voting will correct the result ~ 44% of the time.\n",
        "- By adding cases where all 3 are correct, it means the ensemble will be correct an average of $0.441 + 0.343 = 0.784$ % of the time.\n",
        "- This increases to 83% of the time if we instead combine 5 ensembles.\n",
        "\n",
        "A cool thing to highlight is, the more models we use, the better our results. Like, if we blend the insights from 5 models, our accuracy rate climbs to an impressive 83%.\n"
      ],
      "metadata": {
        "id": "XD9djtX6Y8Lz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding the Bias-Variance Tradeoff\n",
        "\n",
        "When we discuss prediction models, prediction errors can be decomposed into two main subcomponents we care about: error due to \"bias\" and error due to \"variance\". There is a tradeoff between a model's ability to minimize bias and variance. Understanding these two types of error can help us diagnose model results and avoid the mistake of over- or under-fitting.\n",
        "\n",
        "Think of understanding bias and variance like playing darts on a target board. The bullseye in the middle is the perfect prediction. The farther away the darts land from the bullseye, the more off our predictions are.\n",
        "\n",
        "Every time we make a model using different training data, it's like throwing a dart. Sometimes, the data is great and our dart (prediction) is close to the bullseye. But sometimes, if our data has odd values or outliers, our dart might land far off.\n",
        "\n",
        "So, with all these darts (models), we can see a pattern on the board. This pattern helps us understand different scenarios of bias and variance, both high and low.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-7-Ensemble-learning/imgs/target.webp' width=500px>\n",
        "\n",
        "[Source](http://scott.fortmann-roe.com/docs/BiasVariance.html)\n",
        "\n",
        "### **Understanding Bias and Variance**\n",
        "\n",
        "**Bias Error:** This is the difference between what our model predicts and the actual true values. If there's a high bias error, it means our model isn't doing a great job because it's missing key patterns.\n",
        "\n",
        "**Variance Error:** This is about the consistency of our model's predictions. If there's high variance, it means the model's predictions change a lot depending on the sample data. If it's too high, the model might fit our training data too closely, but do a poor job with new data.\n",
        "\n",
        "**Noise (or Irreducible Error):** No matter how good we get, our measurements won't always be perfect. This error comes from things out of our control, like if there's an error in how we measure something.\n",
        "\n",
        "### **Understanding Over- and Under-Fitting**\n",
        "\n",
        "At its root, dealing with bias and variance is really about dealing with over- and under-fitting. Bias is reduced and variance is increased in relation to model complexity. As more and more parameters are added to a model, the complexity of the model rises and variance becomes our primary concern while bias steadily falls. For example, as more polynomial terms are added to a linear regression, the greater the resulting model's complexity will be 3. In other words, bias has a negative first-order derivative in response to model complexity 4 while variance has a positive slope.\n",
        "\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-7-Ensemble-learning/imgs/bias_varaince.png' width=300px>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Y1jvA1dMcaio"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ensemble Learning: overview\n",
        "\n",
        "The aim is to mix many weak learners that are not related to make one powerful guesser by lowering shakiness and/or slant.\n",
        "\n",
        "There are a few ways to do this.\n",
        "\n",
        "First, we look at similar learners. These use the same kind of machine learning to make guesses. Examples are bagging, random forests, and boosting. We'll mostly talk about these in the videos.\n",
        "\n",
        "Second, there are mixed learners, also called stacking models. These use different types of machine learning, like logistic regression, support vector machines, and random forests, to make guesses. You can even mix similar learners into these.\n",
        "\n",
        "We'll only briefly talk about mixed learners this week, but what you learn about similar learners will help you understand how to make a stacking model.\n",
        "\n",
        "To sum up:\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-7-Ensemble-learning/imgs/review.jpg' width=400px>\n",
        "\n",
        "For similar learners, they use the same type of machine learning:\n",
        "- Bagging\n",
        "- Random Forests\n",
        "- Boosting\n",
        "\n",
        "For mixed learners, they use different types:\n",
        "- Stacking\n"
      ],
      "metadata": {
        "id": "Vq_KDQXlnRRg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Types of Ensemble Learning\n",
        "\n",
        "The goal is to mix different simple guessers to make one strong guesser. There are two main ways to do this, and each has a different effect on mistakes.\n",
        "\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-7-Ensemble-learning/imgs/sequential.webp' width=500px>\n",
        "\n",
        "\n",
        "First way, called \"Parallel,\" uses many simple guessers at the same time. They work separately but get mixed together in the end. This lowers the shakiness of the final guess by averaging out single mistakes. Examples are Voting, Bagging, and Random Forests.\n",
        "\n",
        "Second way, called \"Sequential,\" adds one simple guesser after another. If a guesser makes a mistake, the next one pays more attention to that mistake. This helps the group of guessers work better together, lowering the wrongness and making the final guess more accurate. This is mostly seen in Boosting methods, like Adaboost, Gentleboost, and Gradient Boosted Trees.\n",
        "\n",
        "So, in short:\n",
        "\n",
        "Parallel:\n",
        "- Many guessers work at the same time.\n",
        "- Lowers shakiness by averaging out single mistakes.\n",
        "- Examples: Voting, Bagging, Random Forests.\n",
        "\n",
        "Sequential:\n",
        "- Add guessers one by one.\n",
        "- Each new guesser focuses more on past mistakes.\n",
        "- Makes the final guess more accurate and lowers wrongness.\n",
        "- Also helps avoid overfitting, which means it's good at not memorizing the data.\n",
        "- Examples: Boosting, Adaboost, Gentleboost, Gradient Boosted Trees.\n",
        "\n",
        "Knowing the difference between these two ways is important because they deal with mistakes in different ways."
      ],
      "metadata": {
        "id": "fFyd-0nmp0RA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "Ensemble learning joins guesses from lots of different guessing models. These models can either be of the same kind, called \"homogenous,\" or of different kinds, called \"heterogenous.\"\n",
        "\n",
        "For the first way, called \"Parallel,\" many models work at the same time and their results are put together at the end. This helps make the final guess less shaky. Examples of this are Voting, Bagging, and Forests.\n",
        "\n",
        "The second way, called \"Serial,\" adds models one by one. Each new model pays more attention to the mistakes made before. This helps make the final guess more accurate and less wrong. This method is known as Boosting.\n",
        "\n",
        "## So, to sum up:\n",
        "\n",
        "- Ensemble learning methods aggregate predictions made from many different learning models\n",
        "\n",
        "- They can either combine methods of the same class (homogenous) or different classes (heterogenous)\n",
        "\n",
        "- This can be trained in parallel and combined at the end to reduce model variance (voting/Bagging/Forests)\n",
        "\n",
        "- Or trained in serial, one after another, upweighting misclassified examples each time to reduce bias (and decrease variance) - Boosting\n"
      ],
      "metadata": {
        "id": "kKYyjZAiqnkK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bagging (Also called Bootstrap Aggregating)\n",
        "\n",
        "Bagging, also known as Bootstrap Aggregating, is a technique that tries to make the final guess more stable and less shaky (reduce model variance). **How?** By using many decision trees. Each of these trees is trained on different parts of the main data set.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-7-Ensemble-learning/imgs/bootstrapping.webp' width=500px>\n",
        "\n",
        "It fits the base learners (classifiers) on each random subset taken from the original dataset (bootstrapping). Due to the parallel ensemble, all of the classifiers in a training set are independent of each other so that each model will inherit slightly different features.\n",
        "Next, bagging combines the results of all the learners and adds (aggregates) their prediction by averaging (mean) their outputs to get to final results.\n",
        "The Random Forest (RF) algorithm can solve the problem of overfitting in decision trees. Random orest is the ensemble of the decision trees. It builds a forest of many random decision trees.\n",
        "The process of RF and Bagging is almost the same. RF selects only the best features from the subset to split the node.\n",
        "The diverse outcomes reduce the variance to give smooth predictions.\n",
        "\n",
        "\n",
        "The smart idea behind bagging is based on a math rule. This rule tells us that when you take the average of many different guesses, the shakiness or uncertainty of that average becomes much smaller. To understand this rule, let's consider T, which represents the total number of models (or trees) you're using. According to the rule, the shakiness of your average guess is only 1/T of the combined shakiness from all individual guesses. In other words, for separate random guesses, the average's variance (or \"shakiness\") is 1/T of the variance across all the models. In this rule, T stands for the total number of models we have.\n",
        "\n",
        "Last time, we got our hands dirty with creating a decision tree. Today, we're taking it a step further. We'll see how this single decision tree can be a small but vital part of the bigger bagging method. By averaging out the guesses from many trees, bagging can significantly reduce the shakiness of the final prediction.\n",
        "\n",
        "But here comes a common question: If we start with just one main set of data, **where do all these varied data pieces, used to train different trees, come from?** The simple answer is that these varied data sets are created by taking different samples from the main data set. This sampling method is what allows bagging to have so many unique decision trees, even when working from one main dataset."
      ],
      "metadata": {
        "id": "u4i2fseIq7wf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bootstrapping\n",
        "\n",
        "The bootstrap method is a statistical technique that involves generating multiple small subsets from a larger dataset. To do this, random data points are selected from the larger dataset, and after each selection, that data point is put back (or \"replaced\") so it can potentially be chosen again. This act of selecting and then putting the data point back is termed as \"resampling\".\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-7-Ensemble-learning/imgs/bootstrapping.webp' width=500px>\n",
        "\n",
        "Imagine a bag full of marbles, each marble representing a data point. Now, if you were to draw a marble, note its color, and then put it back into the bag before drawing again, you're essentially practicing the bootstrap method.\n",
        "\n",
        "Because each data point (or marble) has the same chance of being picked every time, over multiple draws, some will be selected more often than others, while some might not be picked at all. This randomness ensures that each subset we create has a unique combination of data points.\n",
        "\n",
        "The introduction of such randomness alters the mean (average) and standard deviation (a measure of the spread of data points) of these subsets compared to the original dataset. These changes introduced by bootstrapping can make machine learning models more robust. Essentially, by training the model on slightly different versions of the data, it becomes better equipped to handle a variety of scenarios, thereby enhancing its reliability and performance.\n",
        "\n",
        "In the context of ensemble methods, which involve using multiple models (or \"base learners\" and \"classifiers\") to make predictions, bootstrapping plays a pivotal role. Each of these models is trained on a different bootstrap subset, allowing them to learn from different \"perspectives\" of the data. When their individual predictions are combined, the final result is often more accurate and reliable than any single model could produce on its own.\n",
        "\n",
        "Of course!\n",
        "\n",
        "\n",
        "**Bootstrap Sampling**\n",
        "\n",
        "1. Take the original dataset $X$ which has $N$ training examples.\n",
        "\n",
        "2. Create $T$ copies, denoted as $\\{ \\hat{X}_m \\}^{M}_{m=1}$, by sampling with replacement.\n",
        "\n",
        "   - Each $\\hat{X}_m$ contains $N$ examples (or rows).\n",
        "   \n",
        "   - Each $\\hat{X}_m$ is unique because some examples might appear more than once, while others might not be included at all.\n",
        "\n",
        "For large sample sizes, Bootstrap sampling will closely match the sampling distribution derived from the entire population.\n"
      ],
      "metadata": {
        "id": "7NMhxAAD2P7N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why Bootstrapping Works?\n",
        "\n",
        "Run the next code cell from `Notebook 7.Ensemble_Learning.ipynb` to:\n",
        "\n",
        "Using this function (function bootstrap_sample ), we can illustrate how bootstrapping helps in approximating sampling distributions that would typically require estimation from entire populations.\n",
        "\n",
        "Suppose we had a vast random real-world population. For this illustration, we'll create an extensive array containing 100,000 integers ranging from 0 to 20. Imagine this array as representing a real-world population, say, if we wanted to gather brain scans from every human. It would be impractical, given our resources, to sample everyone.\n",
        "\n",
        "In such scenarios, we'd take as large a sample as possible, which we'll refer to as DATA_sample.\n",
        "\n",
        "Our objective is then to evaluate the efficacy of estimating a sample mean by continually taking bootstrapped samples from our DATA_sample. We want to compare this to the results we'd obtain if we had the means to consistently draw substantial samples from the entire population.\n",
        "1. Create a very large random population (in real-world examples this would be unavailable)\n",
        "```python\n",
        "DATA_population = np.random.randint(0,20,100000)\n",
        "```\n",
        "\n",
        "2. Simulate the process of generating random samples from this population for different sample sizes\n",
        "```python\n",
        "DATA_sample = np.random.choice(DATA_population, sample_size)\n",
        "```\n",
        "\n",
        "3. Take bootstrapped samples:\n",
        "```python\n",
        "sample = bootstrap_sample(DATA_sample)\n",
        "```\n",
        "\n",
        "- $n\\_samples$ times with replacement\n",
        "\n",
        "- Generate $n\\_samples$ different bootstrapped samples of the mean.\n",
        "\n",
        "4. Compare against $n\\_samples$ from true population (again not practical to obtain)\n",
        "```python\n",
        "population_sample = np.random.choice(DATA_population, sample_size)\n",
        "```\n"
      ],
      "metadata": {
        "id": "vSqP8iLW4g05"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plotting resulting distributions as histograms\n",
        "\n",
        "When we visualize the results for various sample sizes, it's evident that, with a sufficiently large number of samples, our aim is to get distributions centered around the true mean, which is 9.51.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-7-Ensemble-learning/imgs/result1.png' width=700px>\n",
        "\n",
        "In our initial illustration where the sample size is 1, it's unsurprising that our approximation is off-mark. Given that DATA_sample contains only a single row, no matter how many times we bootstrap from it, the result remains constant - in this instance, it's 3, which deviates significantly from the mean.\n",
        "\n",
        "Yet, as we increase the sample size, especially beyond 1000 samples, bootstrapping begins to provide an accurate representation of the genuine sampling distribution. This accuracy enhances further, nearing a near-perfect overlap when the sample size is a tenth of the full population's size."
      ],
      "metadata": {
        "id": "-owhBnQN6mMo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decision tree\n",
        "\n",
        "In machine learning, decision trees have a huge impact on decision-based analysis problems. They cover both classification and regression. As the name implies, they use a tree-like model containing nodes and leaves.\n",
        "\n",
        "In the image below, the model's root is at the top—it's an upside-down tree! Here, **conditions are internal nodes** and **outcomes are leaf nodes**.\n",
        "Notice that the decision tree contains several **if-else statements** in a strict order, making the model **inflexible**. It gives rise to **overfitting**, which occurs when a function fits the data too well. That means the model will be accurate only on the data it's been trained on. The model fails to work if there is **a slight change** in the training data or new data.\n",
        "\n",
        "Using one decision tree is can be problematic and might not be stable enough; however, using **multiple decision trees and combining their results will do great.** Combining multiple classifiers in a prediction model is called ensembling. The simple rule of ensemble methods is to reduce the error by reducing the variance.\n",
        "\n"
      ],
      "metadata": {
        "id": "f3SkkkzK7qBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ensemble methods: blind men and the elephant\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-7-Ensemble-learning/imgs/elephant.jpg' width=500px>\n",
        "\n",
        "\n",
        "Ensemble methods* are techniques that combine the decisions from several base machine learning (ML) models to find a predictive model to achieve optimum results. Consider the fable of the blind men and the elephant depicted in the image below. The blind men are each describing an elephant from their own point of view. Their descriptions are all correct but incomplete. Their understanding of the elephant would be more accurate and realistic if they came together to discuss and combined their descriptions."
      ],
      "metadata": {
        "id": "0Rfw4VOcALI1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bagging (Bootstrap Aggregating)\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-7-Ensemble-learning/imgs/Ensemble_Bagging.svg.png' width=500px>\n",
        "\n",
        "\n",
        "Bagging, which stands for \"Bootstrap Aggregating\", is a machine learning ensemble method designed to improve the accuracy and robustness of a model. Here's a breakdown of how it works and its primary benefits:\n",
        "\n",
        "1. **Bootstrapping:** Bagging begins by creating multiple subsets of the original dataset. These subsets are formed by randomly sampling the dataset with replacement, which means some data points might be repeated in a single subset while others might be left out.\n",
        "\n",
        "2. **Training Multiple Models:** For each of these bootstrapped subsets, a separate model (often a decision tree, but it can be any type) is trained. As a result, you end up with multiple independently trained models.\n",
        "\n",
        "3. **Aggregation of Predictions:** When making predictions, all the models in the ensemble cast their \"votes\". For regression problems, the final output is usually the average of all the model outputs. For classification problems, the final prediction is typically the class that gets the majority vote from all the models.\n",
        "\n",
        "\n",
        "In summary:\n",
        "\n",
        "1. **Create bootstrapped samples** from data sets\n",
        "2. **Train a separate weak learner** (e.g. Trees) on each Bootstrap sample\n",
        "3. **Test data on each weak learner**\n",
        "4. **Aggregate results**; how?\n",
        "\n",
        "    * For classification use majority voting\n",
        "     \n",
        " $ f(X) = \\text{mode}\\{f_1(X), f_2(X)... f_T(X)\\} $\n",
        "\n",
        "    * For regression: averaging\n",
        "        $$ f(X) = \\sum_{t=1}^{T} f_t(X) $$\n",
        "\n",
        "**Note:** This we will code from scratch during in our tutorial.\n"
      ],
      "metadata": {
        "id": "v5EMpke29jHJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key Benefits of Bagging:\n",
        "\n",
        "- **Reduction in Variance:** By averaging the predictions from multiple models, bagging tends to reduce the variance, making the final prediction more stable and less susceptible to the fluctuations in the training data.\n",
        "\n",
        "- **Handles Overfitting:** Individual models, especially complex ones like deep decision trees, can sometimes overfit to their training data. By averaging predictions over multiple models, bagging can mitigate the overfitting problem.\n",
        "\n",
        "- **Parallel Computation:** Since each model is trained independently, bagging is inherently parallelizable, making it efficient to compute, especially with modern multi-core processors or distributed computing environments.\n",
        "\n",
        "- **Out-of-Bag Evaluation:** One unique advantage of bagging is the ability to use out-of-bag (OOB) samples (samples not used in a particular bootstrapped set) to validate the performance of the model, eliminating the need for a separate validation set.\n",
        "\n",
        "In essence, bagging leverages the power of multiple models to achieve better generalization and accuracy than would be possible with any single model."
      ],
      "metadata": {
        "id": "PUhUoLl__HJ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Comparison between Bagging and Decision Trees Performance:**\n",
        "\n",
        "For a decision tree, training on this dataset yields an accuracy of 0.875. Notably, the blue examples in the data are incorrectly labeled; they should be in the red class.\n",
        "\n",
        "In contrast, the bagged model correctly classifies nearly all the data points. It makes a single error, misclassifying an isolated blue point."
      ],
      "metadata": {
        "id": "5kyRTFbk-YKj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Understanding Out-of-Bag (OOB) Error in Bagging**\n",
        "\n",
        "The Out-of-Bag (OOB) error provides a way to gauge the performance of a bagging ensemble without requiring a separate validation set. Here's how it works and why it's important:\n",
        "\n",
        "**Evaluating Error in a Bootstrapped Ensemble Approach:**\n",
        "When we use techniques such as bagging, which involves bootstrapped training sets, a certain portion of the training data doesn't get included in each bootstrap sample. This non-included data serves as an automatic \"left-out\" or \"out-of-bag\" set.\n",
        "\n",
        "**Key Features of OOB Error Calculation:**\n",
        "1. **Automatic Left-out Set:** With every bootstrapped training set, some data points don't get selected. For sizable training datasets, roughly 37% of the data points aren't included in a particular bootstrap sample.\n",
        "  \n",
        "2. **Validation through OOB:** The OOB samples can be used to validate or test the model. For instance, for a given data point, xi, which is left out of a particular bootstrap sample, we can test the model trained on that sample using xi.\n",
        "\n",
        "3. **Prediction Aggregation:** For a given left-out data point, xi, predictions from all the bootstrap samples that didn't contain xi are aggregated. This means if we had 100 bootstrap samples and xi was left out in 37 of them, we would get 37 predictions for xi which we can then average (for regression problems) or take a majority vote (for classification problems).\n",
        "\n",
        "4. **Accuracy/Prediction Score:** The predictions from the models, based on the OOB samples, are aggregated to compute a single accuracy or prediction score for the entire ensemble.\n",
        "\n",
        "5. **Estimation for Classification:** For classification tasks, the OOB error can be estimated as the inverse of the accuracy obtained from the OOB samples. For example, if the accuracy is 90%, the OOB error would be 10%.\n",
        "\n",
        "**Visual Representation:** The accompanying graph titled \"OOB error for Bagged Classifier\" plots the OOB error against the number of trees in the ensemble. From the graph, we can observe how the OOB error fluctuates as the number of trees increases. This is a valuable visualization as it can guide the decision on the optimal number of trees to use in the ensemble. Typically, as more trees are added, the OOB error tends to stabilize or decrease, indicating the ensemble's improved performance."
      ],
      "metadata": {
        "id": "29QKhVei-xjC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Boosting\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-7-Ensemble-learning/imgs/boosting.svg.png' width=500px>\n",
        "\n",
        "rewrite: The output of one base learner will be input to another. If a base classifier is misclassified (red box), its weight will get increased (over-weighting) and the next base learner will classify more correctly.\n",
        "\n",
        "The boosting technique follows a sequential order. So Boosting operates in a step-by-step manner. Each model's output serves as input for the next one. If a model (base classifier) gets it wrong (highlighted by a red box), its importance is amplified (or over-weighted) so that the subsequent model (next base learner) can correct the mistake.\n",
        "\n",
        "After training these models, their results are merged to make final predictions.\n",
        "There are several advanced variations of boosting, including Gradient Descent Boosting, AdaBoost, and XGBoost. Gradient boosting not only aims to reduce errors but also integrates gradient optimization in each step. On the other hand, AdaBoost adjusts the data weights with each new model added to the ensemble."
      ],
      "metadata": {
        "id": "zPTGlu52CcQ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## When not to use Bagging\n",
        "\n",
        "- Bagging seeks to creates uncorrelated predictions\n",
        "\n",
        "- Through training on multiple datasets generated by bootstrap sampling (from the original sampling)\n",
        "\n",
        "- Useful for datasets that have high variance and are noisy\n",
        "\n",
        "- If predictions across trees vary considerably bagging can really help\n",
        "\n",
        "BUT\n",
        "\n",
        "- If predictions are stable across trees, then bagging may even lead to a degradation of the result\n",
        "\n",
        "**In details:**\n",
        "\n",
        "### Understanding Bagging\n",
        "- **Uncorrelated Predictions**:\n",
        "\n",
        "Bagging's primary objective is to produce uncorrelated predictions. This means that it aims to ensure that individual models (like decision trees) aren't making the same errors. By having a diverse set of predictions, the aggregated result is often more robust and accurate.\n",
        "\n",
        "- **Bootstrap Sampling**:\n",
        "\n",
        "Bagging achieves this diversity by training multiple models on various datasets. These datasets are created using bootstrap sampling, where random subsets of the original data are selected with replacement.\n",
        "\n",
        "- **Dealing with Noisy Data**:\n",
        "\n",
        " Bagging shines when dealing with data that's 'noisy' or has high variance. This is because individual models might overfit to different parts of the data, but when their predictions are aggregated, these overfittings tend to cancel out, leading to a more generalized model.\n",
        "\n",
        "- **Varying Predictions**:\n",
        "\n",
        "If the individual models (trees, in many cases) produce very different predictions for the same data point, bagging can be extremely beneficial. By combining these diverse predictions, bagging often arrives at a more accurate and stable result.\n",
        "\n",
        "### The Caveat\n",
        "However, like all techniques, bagging isn't universally beneficial.\n",
        "\n",
        "- **Stable Predictions Across Models**:\n",
        "\n",
        "If the base models are producing very similar or stable predictions, bagging might not offer much advantage. The principle behind bagging is to bring together diverse models to counteract their individual errors. If there's little diversity in the predictions, then bagging doesn't have much to \"work with.\"\n",
        "\n",
        "- **Potential Degradation**:\n",
        "\n",
        "In scenarios where predictions are already stable across trees (or other base models), introducing bagging might not only be redundant but might also degrade the performance. This is because the process of bootstrapping and aggregating can introduce its own sources of error or noise, which would not be offset by the benefits of diversity (since the predictions are already stable).\n",
        "\n",
        "In conclusion, while bagging is a powerful tool for many machine learning tasks, it's essential to understand the nature of your data and the behavior of your models. If your base models are already performing consistently and with low variance, the added complexity of bagging might be unnecessary and could even be counterproductive.\n"
      ],
      "metadata": {
        "id": "7G3gXu-VEQPS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extension of Bagged Ensembles of Decision Trees: Decision Forests\n",
        "\n",
        "In the world of machine learning, ensembles have become a powerful way to improve the performance of individual models. One such ensemble technique is the bagging of decision trees, which has led to the evolution of Decision Forests. But what differentiates a Decision Forest from a simple bagged ensemble of decision trees? Let's dive deep into this extension.\n",
        "\n",
        "### The Core Idea\n",
        "Decision Forests, often known as Random Forests, are an evolution of the bagging technique applied specifically to decision trees. The primary goal remains consistent: reduce overfitting and enhance the model's accuracy by combining multiple trees' predictions. However, Decision Forests introduce additional layers of randomness to achieve these goals more effectively.\n",
        "\n",
        "### Key Features of Decision Forests:\n",
        "\n",
        "1. **Random Feature Selection**:\n",
        "   - In traditional decision trees, at each node, the best feature is chosen to split the data based on some criterion (like Gini impurity or information gain).\n",
        "   - Decision Forests add a twist to this. Instead of allowing every feature to be considered for a split at each node, they randomly select a subset of features. This subset is then evaluated, and the best feature from this subset is used for the split.\n",
        "   - This approach increases randomness, ensuring that individual trees in the forest are not just different due to bootstrapped data samples but also because of the features they consider at each decision point.\n",
        "\n",
        "2. **De-correlation of Models**:\n",
        "   - By introducing this randomness in feature selection, Decision Forests further de-correlate the individual trees. This means that the errors or biases of one tree are less likely to be replicated across the forest, leading to a more robust overall model.\n",
        "\n",
        "3. **Reduction in Model Variance**:\n",
        "   - Variance refers to how much a model's predictions might change if trained on a different set of data. High variance can lead to overfitting.\n",
        "   - The increased randomization in Decision Forests means that individual trees might have higher variance, but when their predictions are aggregated, the variances tend to cancel out, leading to a model with reduced overall variance.\n",
        "\n",
        "4. **Increased Stability Against Feature Noise**:\n",
        "   - In real-world data, not all features are equally informative. Some might contain 'noise' or irrelevant information.\n",
        "   - By considering only a subset of features at each decision point, Decision Forests increase the chances that noisy features are left out in many of the splits, thus providing a natural resistance against overfitting due to such noisy features.\n",
        "\n",
        "### Conclusion:\n",
        "Decision Forests represent an advanced stage in the evolution of ensemble methods, particularly for decision trees. By introducing controlled randomness in the decision-making process, they effectively combat common pitfalls like overfitting and high variance. This ensures that the ensemble not only benefits from the wisdom of multiple models but also that each of those models brings a unique perspective, making the overall ensemble more powerful and reliable."
      ],
      "metadata": {
        "id": "8A1yiblQMcOa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-3-Classification-models/imgs/random_forest.jpeg' width=500px >\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-3-Classification-models/imgs/Random_forest_explain.png' width=500px >\n",
        "\n",
        "Think about it this way - if one tree is handy, wouldn't a whole forest be even better?\n",
        "\n",
        "Imagine we could create a bunch of decision trees, each one asking different questions. Then, we could combine their answers to make a final prediction. This is exactly what a Random Forest does.\n",
        "\n",
        "A Random Forest is what we call an 'ensemble model'. It's like a supergroup band made up of lots of individual musicians, all working together to create a harmonious sound. Here, each model contributes to a final, hopefully better, prediction."
      ],
      "metadata": {
        "id": "C2MtfprRMmkF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Difference between bagging and boosting\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-7-Ensemble-learning/imgs/difference.jpg.png' width=600px >\n",
        "\n"
      ],
      "metadata": {
        "id": "KnRwQfoygIYw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forests: Training\n",
        "\n",
        "For each tree $ t $ of $ T $ trees in the forest:\n",
        "\n",
        "1. Bootstrap examples from the training data matrix $ X_{TRAIN} $ (with replacement).\n",
        "\n",
        "2. While the tree is of depth less than $ D_{max} $ OR the number of examples $ n_j $ at a node is less than $ n_{leaf\\_min} $:\n",
        "    - At each node $ j $:\n",
        "        a. Select a sub-set (slice) of the feature-space.\n",
        "        b. Train an (axis-aligned) decision stump.\n",
        "3. Assign label $ y_t $ to each leaf node for each tree.\n",
        "\n",
        "**Key Points**:\n",
        "- Key difference relative to Bagging!\n",
        "- Each node sees a slightly different subset of examples.\n",
        "\n",
        "---\n",
        "\n",
        "### Elaboration:\n",
        "\n",
        "**Random Forests and Their Training Process**\n",
        "\n",
        "Random Forests are an ensemble learning method, which means they use multiple models (in this case, decision trees) to make predictions. The idea is to harness the strengths of each individual tree to create a more robust and accurate overall model.\n",
        "\n",
        "**Training Process**:\n",
        "\n",
        "1. **Initialization**: For each tree in the forest, a subset of training data is chosen. This subset is created using bootstrapping, which means randomly sampling from the original training data with replacement. This ensures each tree gets a slightly different view of the data.\n",
        "\n",
        "2. **Tree Growth**:\n",
        "   - The tree grows iteratively. At each node of the tree, a subset of the features is randomly selected. This random feature selection is a hallmark of Random Forests and distinguishes them from traditional decision trees and bagged trees.\n",
        "   - With this subset of features, an axis-aligned decision stump is trained. A decision stump is essentially a one-level decision tree. It decides the best split based on some criterion, such as Gini impurity or information gain.\n",
        "   \n",
        "3. **Leaf Node Assignment**:\n",
        "   - As the tree grows, it will eventually reach a stopping condition. This could be a maximum depth $ D_{max} $ or when the number of examples at a node is below a threshold $ n_{leaf\\_min} $.\n",
        "   - Once a node meets these conditions, it becomes a leaf node. The final decision or classification label $ y_t $ is then assigned to this node based on the majority class of the samples at that node.\n",
        "\n",
        "**Distinguishing Features**:\n",
        "\n",
        "- **Difference from Bagging**: While both Random Forests and Bagging involve creating multiple trees from bootstrapped samples, Random Forests introduce another layer of randomness by selecting a random subset of features at each node during training. This increased randomness helps in reducing correlation between trees and thus creates a more robust ensemble.\n",
        "\n",
        "- **Variability in Nodes**: One of the advantages of this method is that each node of a tree sees a slightly different subset of examples and features. This ensures that the individual trees in the forest capture different patterns and nuances from the data, making the overall ensemble more comprehensive and less prone to overfitting.\n",
        "\n",
        "In conclusion, Random Forests offer a sophisticated approach to harness the power of multiple decision trees. By introducing randomness in both data samples and feature selection, they effectively increase the diversity of the individual trees, leading to a more powerful and generalized ensemble model."
      ],
      "metadata": {
        "id": "4L8DPMtpeSWS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forests: Testing\n",
        "\n",
        "- For each tree $ t $ of $ T $ trees in the forest:\n",
        "  - Pass each test example $ i $ from $ X_{TEST} $.\n",
        "  - At each node $ j $:\n",
        "    - While $ j $ supports further splitting:\n",
        "      - Apply trained decision stump.\n",
        "    - At terminal node, store $ y_t^i $, the predicted label for that test example.\n",
        "\n",
        "- Aggregate (using exactly the same process as for bagging).\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "1. **For each tree $ t $ of $ T $ trees in the forest:**  \n",
        "   Random forests consist of multiple decision trees (often a large number, represented by $ T $). During the testing phase, each individual tree is used to predict the outcome for the test examples. This means we are going to use each of these trees to make predictions on our test data.\n",
        "\n",
        "2. **Pass each test example $ i $ from $ X_{TEST} $:**  \n",
        "   For each tree, every test example (or instance) in our test dataset, denoted by $ X_{TEST} $, is passed through the tree. This means we're trying to get a prediction for each test instance from every tree in the forest.\n",
        "\n",
        "3. **At each node $ j $:**  \n",
        "   As we pass a test example down the tree, we reach different decision nodes, represented by $ j $, where a decision is made based on a feature value of the test instance. This decision determines which child node the instance goes to next.\n",
        "\n",
        "4. **While $ j $ supports further splitting:**  \n",
        "   If the current node $ j $ is not a terminal node (or leaf node), it means it can further split or segregate the data. This continues until we reach a terminal node or if the test instance satisfies all decision criteria in the path it has taken.\n",
        "\n",
        "5. **Apply trained decision stump:**  \n",
        "   A decision stump is a simple decision tree with one level. In random forests, each internal node of the tree represents a decision stump that makes a decision based on a particular feature's value. Here, the \"trained decision stump\" refers to using the criteria decided during the training phase to route the test instance further down the tree.\n",
        "\n",
        "6. **At terminal node, store $ y_t^i $, the predicted label for that test example:**  \n",
        "   Once a test instance reaches a leaf node (terminal node) of the tree, a prediction is made. This prediction, represented by $ y_t^i $, is the label assigned by the tree $ t $ for the test example $ i $.\n",
        "\n",
        "7. **Aggregate (using the same process as for bagging):**  \n",
        "   Once all trees have made their individual predictions for all test instances, these predictions need to be combined to get a final consensus prediction. In the case of regression, this could be the average of all predictions, and for classification, it's typically the majority vote. The process of combining these predictions is similar to that in bagging (bootstrap aggregating).\n",
        "\n",
        "In essence, this process allows the random forest model to leverage the \"wisdom of the crowd\" by taking multiple predictions from different trees and then aggregating them for a more robust and accurate final prediction.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DsAwbD0Ahwap"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random forests: (important) Tuning Parameters\n",
        "\n",
        "When fine-tuning random forests, there are several key parameters to pay attention to:\n",
        "\n",
        "1. **Number of features (selected per node):**\n",
        "\n",
        "   - Trade-off between bias and variance.\n",
        "\n",
        "Firstly, consider the number of features at each node. This strikes a balance between bias and variance.\n",
        "\n",
        "   - A small number of features ($ m $) will reduce variance.\n",
        "\n",
        "   - However, if many features are noisy, this can reduce the likelihood of selecting a beneficial feature, thereby increasing bias.\n",
        "\n",
        "Restricting the number of features too much can compromise model accuracy, especially if only a few features contribute significantly to predictions.\n",
        "\n",
        "   - Common default is the square root of $ m $. Other choices include log base 2 of $ m $, a fraction of $ m $, or selecting all features (though this approach aligns with bagging).\n",
        "\n",
        "A common default is to use the square root of $ m $, where $ m $ is the total number of features. Alternatives include using log base 2 of $ m $, a specific fraction of $ m $, or using all features. However, using all features is essentially the same as bagging.\n",
        "\n",
        "2. **Number of Trees:**\n",
        "   - Increasing the number of trees up to a certain limit can reduce the model's variance.\n",
        "\n",
        "Another parameter to experiment with is the number of trees. As seen in the out-of-bag example, increasing the number of trees can enhance performance up to a certain point. Thus, there's a trade-off between performance enhancement and computational time.\n",
        "\n",
        "3. **max_depth (maximum tree depth; alternatively min_samples_leaf):**\n",
        "\n",
        "   - Trees with greater depth are more prone to overfitting.\n",
        "\n",
        "Lastly, it's crucial to set a maximum depth or a minimum number of samples per leaf to prevent overfitting. Setting\n",
        "\n",
        "   - It's recommended to either control the tree's depth or set a minimum number of samples per leaf.\n",
        "\n",
        "`min_samples_leaf` can offer an indirect way to control tree depth, making it a more flexible option and potentially a preferred choice.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EA1nOw6lFV9R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ensembles: Feature importance ranking\n",
        "\n",
        "\n",
        "\n",
        "- Ensemble methods can rank features\n",
        "  - Compare how often each feature is selected as best for splitting\n",
        "  - Over every node (decision stump) trained during optimisation\n",
        "\n",
        "- Useful for interpretation\n",
        "\n",
        "- Can also be used for feature selection (week 6)\n",
        "  - Potentially for use in combination with another ML algorithm\n",
        "  - Useful when the number of features far exceeds the number of examples\n",
        "\n",
        "---\n",
        "\n",
        "One of the main strengths of ensemble methods is how they balance both performance and clarity.\n",
        "\n",
        "All the homogenous methods we talked about today work by combining basic and easy-to-understand decision stumps in different manners.\n",
        "\n",
        "Decision stumps are straightforward because they make decisions based on setting limits on individual features.\n",
        "\n",
        "This feature is vital because it lets users identify the most crucial features for a prediction. They can do this by observing how frequently each feature is selected by the decision stumps within the ensemble.\n",
        "\n",
        "Understanding feature importance can be very beneficial. For instance, it can help in choosing features before they are used in another model or a second stage of learning, which you'll get more insights on next week.\n",
        "\n",
        "On the right side, there's a bar chart titled: \"Feature importances from the neonatal GA prediction task*\". The graph displays a series of vertical bars, with some highlighted in red, representing feature importances. The x-axis seems to denote different feature numbers, and the y-axis denotes the importance score, ranging from 0 to 0.5.\n",
        "\n",
        "Furthermore, it aids in creating models that can be explained. For instance, in the field of medical imaging, these important features can be emphasized as potential indicators of disease development. They can also help in creating detailed models that show how certain image features are linked to biological processes or the advancement of a disease."
      ],
      "metadata": {
        "id": "PVOY5iDoJZct"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "In summary, both bagging and random forests function as parallel learning ensembles, rooted in similar principles. While bagging is versatile enough to be paired with any learning model, its original intent was for trees, and random forests evolved from there.\n",
        "\n",
        "The core concept is training numerous weak learners independently and in parallel. Once trained, they're combined either through majority voting or averaging. The aim here is to maintain base learners that are as diverse as possible. This diversity is achieved by introducing randomness during training, which is accomplished in both methods using bootstrap sampling of datasets.\n",
        "\n",
        "Moreover, random forests train decision stumps using random subsets of features. Collectively, these methods help in reducing correlation between predictions and diminish model variance by mitigating the effects of isolated errors through averaging.\n",
        "\n",
        "One can gauge the potential generalization performance of either bagging or forest models by observing the out-of-bag error. This error stems from the training examples that are not included in training each base learner due to the bootstrap sampling. These omitted examples serve as a validation set, predicting the model's likely performance on new data.\n",
        "\n",
        "A standout capability of all the homogenous ensembles discussed today is their ability to pinpoint feature importances. This provides insights into which features played pivotal roles in model construction, proving valuable for feature selection and interpretation.\n",
        "\n",
        "That wraps up the section on bagging and forests. Despite being content-rich, ensure you've reviewed the bootstrap example in the notebook and completed the Keats quiz to solidify your understanding. Then, you can proceed to the subsequent video on boosting."
      ],
      "metadata": {
        "id": "Bq8iwaoXOmXt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Boosting\n",
        "\n",
        "- Rather than building independent weak learners in parallel and aggregating at end…\n",
        "\n",
        "- build weak learners (decision stumps) in serial (one at a time) BUT\n",
        "\n",
        "- adaptively reweight training data prior to training each new weak learner to give\n",
        "higher weight to previously misclassified examples\n",
        "\n",
        "\n",
        " Boosting is indeed a powerful ensemble learning technique that focuses on training weak learners sequentially, with each new learner aiming to correct the mistakes of its predecessor. Let me help illustrate the toy example you've mentioned:\n",
        "\n",
        "**Toy Example Illustration:**\n",
        "\n",
        "1. **Initial Data Visualization**:\n",
        "\n",
        "Imagine a two-dimensional space with features $x1$ and $x2$. We have positive examples (pluses) and negative examples (minuses) scattered in this space.\n",
        "\n",
        "The goal is to classify the data points, represented by pluses and minuses.\n",
        "\n",
        "2. **First Iteration**:\n",
        "\n",
        "The first decision stump is trained to make a split (threshold) based on one of the features. Let's say the decision boundary is drawn as a vertical line (based on feature $x1$), creating a \"red box\" as you mentioned. In the first iteration, the threshold chosen (visualized by the red box) correctly classifies some of the data but misclassifies three red crosses.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-7-Ensemble-learning/imgs/boosting1.png' width=400px >\n",
        "\n",
        " This line correctly classifies some of the negative examples (red crosses), but also misclassifies three of them.\n",
        "\n",
        "In boosting, after every iteration, the misclassified samples are given more importance. This is done by upweighting these samples, so they have a higher influence on the decision boundary in the next iteration. In visual representations, this is often shown by enlarging these misclassified data points.\n",
        "\n",
        "3. **Second Iteration**:\n",
        "\n",
        "In the subsequent iteration, with the emphasis now on the previously misclassified samples, the model (or the base learner) will try to correct its mistake. This might lead it to choose another threshold, visualized by the next red box. However, this time, it misclassifies three blue minus signs.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-7-Ensemble-learning/imgs/boosting2.png' width=500px >\n",
        "\n",
        "4. **Third Iteration**:\n",
        "\n",
        "In the third round, the \"minus\" class examples get more weight. At the same time, the wrongly marked red examples from before keep their added importance. Because of this, the third base learner tries to find a line that separates these examples right.\n",
        "\n",
        "So, the learner picks a line based on the second feature.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-7-Ensemble-learning/imgs/boosting3.png' width=500px >\n",
        "\n",
        "5. **Final Iteration**:\n",
        "\n",
        "The process continues, and in each iteration, the misclassified samples from the previous iteration are given more importance. Over many iterations, the final model will combine the knowledge from each base learner to make a more accurate and robust decision boundary. This is the essence of boosting, where the strength comes from focusing on the mistakes and continuously improving upon them.\n",
        "\n",
        "Boosting helps create detailed, curved classifiers. These classifiers make the group's accuracy much better than single base learners. Plus, as we'll see, they help the model do better on tricky edge cases that might otherwise be hard to figure out.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-7-Ensemble-learning/imgs/boosting4.png' width=700px >\n",
        "\n",
        "Eventually, the final model will be a combination of these individual decision stumps, which together work to classify most, if not all, of the examples correctly.\n",
        "\n",
        "The beauty of boosting lies in its ability to convert weak learners into a strong learner by focusing on areas of misclassification and adapting accordingly.\n",
        "\n",
        "It's worth noting that there are different variations of boosting algorithms, such as AdaBoost, Gradient Boosting, and XGBoost, each with its own method of training and combining weak learners."
      ],
      "metadata": {
        "id": "TdeM9AC-PUDM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Boosting: Toy example\n",
        "\n",
        "Boosting is a powerful ensemble technique that builds on the idea of converting weak learners into strong learners. By focusing on the samples that are hard to classify in each iteration, boosting increases the overall accuracy of the model.\n",
        "\n",
        "Our description indicates a 2D space with features $x1$ and $x2$. The goal is to classify the data points, represented by pluses and minuses. In the first iteration, the threshold chosen (visualized by the red box) correctly classifies some of the data but misclassifies three red crosses.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-7-Ensemble-learning/imgs/boosting1.png' width=400px >\n",
        "\n",
        "In boosting, after every iteration, the misclassified samples are given more importance. This is done by upweighting these samples, so they have a higher influence on the decision boundary in the next iteration. In visual representations, this is often shown by enlarging these misclassified data points.\n",
        "\n",
        "In the subsequent iteration, with the emphasis now on the previously misclassified samples, the model (or the base learner) will try to correct its mistake. This might lead it to choose another threshold, visualized by the next red box. However, this time, it misclassifies three blue minus signs.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-7-Ensemble-learning/imgs/boosting2.png' width=500px >\n",
        "\n",
        "+\n",
        "\n",
        "\n",
        "\n",
        "In the third round, the \"minus\" class examples get more weight. At the same time, the wrongly marked red examples from before keep their added importance. Because of this, the third base learner tries to find a line that separates these examples right.\n",
        "\n",
        "So, the learner picks a line based on the second feature.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-7-Ensemble-learning/imgs/boosting3.png' width=500px >\n",
        "\n",
        "The process continues, and in each iteration, the misclassified samples from the previous iteration are given more importance. Over many iterations, the final model will combine the knowledge from each base learner to make a more accurate and robust decision boundary. This is the essence of boosting, where the strength comes from focusing on the mistakes and continuously improving upon them.\n",
        "\n",
        "Boosting helps create detailed, curved classifiers. These classifiers make the group's accuracy much better than single base learners. Plus, as we'll see, they help the model do better on tricky edge cases that might otherwise be hard to figure out.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-7-Ensemble-learning/imgs/boosting4.png' width=700px >"
      ],
      "metadata": {
        "id": "yluFQdGsPru9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems you're providing a high-level description of a toy example for boosting algorithms. Boosting is indeed a powerful ensemble learning technique that focuses on training weak learners sequentially, with each new learner aiming to correct the mistakes of its predecessor. Let me help illustrate the toy example you've mentioned:\n",
        "\n",
        "**Toy Example Illustration:**\n",
        "\n",
        "1. **Initial Data Visualization**:\n",
        "Imagine a two-dimensional space with features $x1$ and $x2$. We have positive examples (pluses) and negative examples (minuses) scattered in this space.\n",
        "\n",
        "2. **First Iteration**:\n",
        "The first decision stump is trained to make a split (threshold) based on one of the features. Let's say the decision boundary is drawn as a vertical line (based on feature $x1$), creating a \"red box\" as you mentioned. This line correctly classifies some of the negative examples (red crosses), but also misclassifies three of them.\n",
        "\n",
        "3. **Subsequent Iterations**:\n",
        "Based on the errors in the first iteration, the misclassified examples are given more weight or importance. This means that the next decision stump will try harder to get those examples right. The decision boundary may then be drawn based on feature $x2$ or a different threshold on $x1$.\n",
        "\n",
        "This process continues for several iterations. With each iteration, the algorithm puts more emphasis on examples that were previously misclassified, which ensures that difficult-to-classify examples receive more attention.\n",
        "\n",
        "Eventually, the final model will be a combination of these individual decision stumps, which together work to classify most, if not all, of the examples correctly.\n",
        "\n",
        "The beauty of boosting lies in its ability to convert weak learners into a strong learner by focusing on areas of misclassification and adapting accordingly.\n",
        "\n",
        "It's worth noting that there are different variations of boosting algorithms, such as AdaBoost, Gradient Boosting, and XGBoost, each with its own method of training and combining weak learners."
      ],
      "metadata": {
        "id": "tC9BU1PvQOHb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Boosting works by combining multiple weak learners to produce a strong learner. Each base learner's prediction is weighted based on its accuracy, and the final prediction is typically an aggregation of these weighted predictions. By focusing on the misclassified examples from previous iterations and adjusting their weights, boosting ensures that each subsequent learner focuses on the challenging examples. This iterative process results in a complex, non-linear decision boundary that can better classify examples that might be misclassified by individual base learners.\n",
        "\n",
        "Furthermore, this approach not only helps in achieving higher accuracy but also in improving the model's performance on edge cases or marginal examples. These are the instances that often lie close to the decision boundary and are difficult to classify correctly. Since boosting specifically emphasizes misclassified examples from prior iterations, it becomes more attuned to these edge cases. Thus, boosting can effectively reduce the noise and achieve better generalization on unseen data.\n",
        "\n",
        "In practice, boosting algorithms like AdaBoost, Gradient Boosting, and XGBoost have been shown to provide impressive results on a variety of tasks and datasets, often outperforming other ensemble methods and standalone models."
      ],
      "metadata": {
        "id": "OPZ7RLqIQgaM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The AdaBoost (short for \"Adaptive Boosting\") algorithm is a significant milestone in the history of ensemble methods. As you mentioned, it was introduced in the 90s and played a pivotal role in the popularization of boosting techniques in machine learning.\n",
        "\n",
        "**AdaBoost Algorithm Overview:**\n",
        "1. **Initialization**: Each training example is assigned an equal weight.\n",
        "2. **Iterative Training**: For a set number of rounds or until a threshold is met:\n",
        "   a. Train a weak learner on the training data.\n",
        "   b. Calculate the weighted error of the weak learner.\n",
        "   c. Compute the learner's weight based on its accuracy.\n",
        "   d. Update the weights of the training examples. Misclassified examples get increased weights, making them more critical in the subsequent rounds.\n",
        "3. **Prediction**: When making a prediction, AdaBoost considers the weighted vote of each weak learner and outputs the class with the highest combined weight.\n",
        "\n",
        "**Key Points about AdaBoost:**\n",
        "- **Weak Learners**: Often decision stumps (one-level decision trees) are used, but the algorithm is versatile enough to incorporate other weak learners.\n",
        "- **Weights**: The weights of training examples ensure that the algorithm focuses more on the harder-to-classify instances in subsequent rounds.\n",
        "- **Combining Predictions**: The final prediction is a weighted sum of the predictions made by the individual weak learners. The more accurate a learner, the higher its weight in the final prediction.\n",
        "\n",
        "The adaptiveness of AdaBoost comes from its ability to focus on the instances that the previous weak learners misclassified.\n",
        "\n",
        "**Regarding Gradient Boosted Trees and XGBoost:**\n",
        "- **Gradient Boosted Trees**: This method generalizes boosting to optimize for any differentiable loss function. It builds trees sequentially, where each new tree corrects the errors of the previous one.\n",
        "- **XGBoost**: An optimized implementation of gradient boosting, XGBoost provides better performance and faster execution times than other gradient boosting methods. It offers parallel tree boosting, regularization, and is designed for efficiency and performance.\n",
        "\n",
        "**Conclusion**:\n",
        "Ensemble methods, particularly boosting techniques, have proven to be extremely effective in both academic competitions and real-world applications. AdaBoost laid the foundation for these techniques, and its descendants, like XGBoost, continue to be widely used in the machine learning community today."
      ],
      "metadata": {
        "id": "7C6jGrCLQvZ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adaboost (Adaptive Boosting)\n",
        "\n",
        "Adaboost, short for \"Adaptive Boosting\", is a machine learning algorithm that builds a strong classifier from a number of weak classifiers. The algorithm operates by iteratively selecting a weak classifier that corrects the mistakes of the previous classifiers. Here, we'll delve deeper into the use of decision stumps as weak learners and the weighting process of Adaboost.\n",
        "\n",
        "### Decision Stumps as Weak Learners:\n",
        "A decision stump is a machine learning model consisting of a one-level decision tree. In other words, it's a decision tree with just one split. Because of their simplicity, decision stumps are weak learners, as they're slightly better than random guessing.\n",
        "\n",
        "Reasons for using decision stumps in Adaboost:\n",
        "1. **Simplicity**: Decision stumps are easy to understand and interpret.\n",
        "2. **Efficiency**: Being simple, they can be computed very quickly.\n",
        "3. **Versatility**: Decision stumps can handle any type of data - numerical or categorical.\n",
        "4. **Effectiveness with Adaboost**: Their weakness is complemented by Adaboost, which focuses on their mistakes, making them a perfect fit for the boosting procedure.\n",
        "\n",
        "### Weighting Process:\n",
        "\n",
        "1. **Initialization**: Start by giving each data example an equal weight. If there are $n$ examples, each example gets a weight of $1/n$.\n",
        "\n",
        "2. **Training the Weak Learner**: Train a weak learner (like a decision stump). Let it classify the training examples.\n",
        "\n",
        "3. **Compute Error**: Calculate the weighted error rate ($\\epsilon$) of the decision stump. This is the sum of the weights of the misclassified examples.\n",
        "\n",
        "   $$\\epsilon = \\sum_{\\text{misclassified}} \\text{weight of example}$$\n",
        "\n",
        "4. **Compute Learner Weight**: The decision stump gets a weight in the final vote, which is a function of its error:\n",
        "\n",
        "   $$\\alpha = \\frac{1}{2} \\ln\\left(\\frac{1-\\epsilon}{\\epsilon}\\right)$$\n",
        "\n",
        "   Here, $\\ln$ is the natural logarithm. $\\alpha$ is larger for smaller errors.\n",
        "\n",
        "5. **Update Weights**:\n",
        "   - For each misclassified example, its weight is increased:\n",
        "     $$\\text{new weight} = \\text{weight} \\times \\exp(\\alpha)$$\n",
        "   - For each correctly classified example, its weight is decreased:\n",
        "     $$\\text{new weight} = \\text{weight} \\times \\exp(-\\alpha)$$\n",
        "\n",
        "6. **Normalization**: After updating, normalize the weights of all examples so that they sum up to 1.\n",
        "\n",
        "7. **Iteration**: Repeat the above steps until a pre-set number of weak learners have been trained or the error falls below a threshold.\n",
        "\n",
        "Once all the weak learners are trained, the final model makes predictions by taking a weighted vote of all the weak learners. The weights of the weak learners in this vote are determined by their individual $\\alpha$ values.\n",
        "\n",
        "Adaboost, by focusing more on the misclassified examples in each iteration, makes sure that the hard examples get the attention they need. This iterative correction of the previous mistakes is what makes Adaboost a powerful ensemble method.\n"
      ],
      "metadata": {
        "id": "OvAWTDmHRZZS"
      }
    }
  ]
}