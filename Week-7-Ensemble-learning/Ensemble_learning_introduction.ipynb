{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaralAminpour/ML-BME-Course-UofA-Fall-2023/blob/main/Week-7-Ensemble-learning/Ensemble_learning_introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Learning Objectives:\n",
        "\n",
        "This week, our focus is on Ensemble Learning:\n",
        "\n",
        "- **Understanding**: We want you to get a clear idea of what ensemble learning is and why it's useful.\n",
        "\n",
        "- **Types of Ensembles**: We'll explore two main types:\n",
        "  - **Homogeneous Ensembles**: Here, we use the same type of model multiple times. Examples include **bagging** **random forests** and **boosting**.\n",
        "\n",
        "  - **Heterogeneous Ensembles**: This involves using different types of models together. We'll touch on this briefly.\n",
        "\n",
        "  - **Sequential vs. Parallel Methods**: We'll differentiate between these two. Sequential methods build on the previous model's output, whereas parallel methods operate concurrently, aiming to improve accuracy and reduce overfitting.\n",
        "\n",
        "- **Practical Work**: We'll build a Bagging model using decision trees. If you missed our last session, don't worry; there's a ready-to-use code module for you.\n",
        "\n",
        "- **Using Scikit-Learn**: We'll practice implementing our learned methods using this popular tool.\n",
        "\n",
        "By the end of the week, you should:\n",
        "\n",
        "- Know what ensemble learning is and its benefits.\n",
        "\n",
        "- Understand the difference between bagging, boosting, and other methods.\n",
        "\n",
        "- Be able to create a basic Bagging model.\n",
        "\n",
        "- Know how to use these methods in Scikit-Learn.\n",
        "\n",
        "- Have a basic idea about stacking (another ensemble method)."
      ],
      "metadata": {
        "id": "hOshBkzFDub1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ensemble Learning\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-7-Ensemble-learning/imgs/orchestra.png' width=500px>\n",
        "\n",
        "Ensemble Learning is a technique in machine learning where multiple models (often referred to as \"base models\") are trained and their predictions are combined to produce a final result.\n",
        "\n",
        "Ensemble learning is a concept that has been around for a while. At its core, it's based on the principle that gathering opinions from multiple sources often leads to better outcomes than relying on just one source. This idea is commonly referred to as the \"wisdom of the crowd.\"\n",
        "\n",
        "This week, we'll apply this principle to machine learning. By combining simple models, like the decision trees we discussed last week, we aim to create more accurate and robust predictive models that perform better on new, unseen data.\n",
        "\n",
        "\n",
        "**Ensemble Learning Analogy** is like an orchestra performing a symphony. Each individual musician (model) has their own instrument (algorithm) and plays their part (makes a prediction). While each musician is talented on their own, it's when they all come together under the guidance of the conductor (ensemble method) that they produce a harmonious and powerful performance (more accurate prediction). Just as a single out-of-tune instrument can be drowned out by the harmony of the entire orchestra, the errors from a single model in ensemble learning can be offset by the correct predictions of other models. The collective output is often more beautiful and impactful than any solo performance."
      ],
      "metadata": {
        "id": "k7n5i7UxFkRu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "X5XR0KxhIqu8"
      }
    }
  ]
}