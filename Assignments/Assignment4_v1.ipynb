{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "id": "7pnRBmZYMW5p"
   },
   "source": [
    "# Assignment4\n",
    "# Ensemble Learning: Random Forests and Adaboost\n",
    "\n",
    "## Deadline: Thursday, November 9 at 11:59 PM\n",
    "## The assignment must be submitted in the form of a Jupyter notebook and uploaded to eClass.\n",
    "\n",
    "In this assignment, you will use the Gestational Age dataset that you have already worked on during in-class exercises and in Assignment 3. You will explore ensemble learning through two applications: Random Forests and boosting with Adaboost.\n",
    "\n",
    "## Marks:\n",
    "<ul type=\"none\">\n",
    "  <li><b>Exercise 1: 4 marks</b></li>\n",
    "    <ul>\n",
    "      <li>Implement the training and testing of a DecisionTreeRegressor: 1 mark;</li>\n",
    "      <li>Implement the training and testing of the RandomForestRegressor: 1 mark;</li>\n",
    "      <li>Return and plot feature importances: 0.5 marks;</li>\n",
    "      <li>Compare and discuss the performance of the two implemented regressors (max 200 words): 1.5 marks.</li>\n",
    "    </ul>\n",
    "  <li><b>Exercise 2: 3 marks</b></li>\n",
    "    <ul>\n",
    "      <li>Create a dictionary called param_dist which specifies parameters for max_depth, max_features and n_estimators: 1 mark;</li>\n",
    "      <li>Create an instance (grid) of GridSearchCV with RandomForestRegressor() as the model and param_dist as the search grid (with cross validation: cv=5); then run on the training data: 1 mark;</li>\n",
    "      <li>Apply the tuned parameters to the model and get a new test prediction: 0.5 marks;</li>\n",
    "      <li>Compare the performance with the previously implemented Random Forest regressor: 0.5 marks.</li>\n",
    "    </ul>\n",
    "  <li><b>Exercise 3: 3 marks</b></li>\n",
    "    <ul>\n",
    "      <li>Try Adaboost Regression using default parameters: 1 mark;</li>\n",
    "      <li>Try optimising parameters (such as min_samples_leaf and n_estimators) using GridSearchCV: 1 mark;</li>\n",
    "      <li>Apply the optimised parameters to train and test a new model: 1 mark.</li>\n",
    "    </ul>   \n",
    "\n",
    "**Total: 10 marks**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r19gIGINJpXQ"
   },
   "source": [
    "First load all needed modules and download the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "id": "WOtGKsQVMW5r"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PzMpqSeKqPzY"
   },
   "outputs": [],
   "source": [
    "# This code will download the required data files from GitHub\n",
    "import requests\n",
    "def download_data(source, dest):\n",
    "    base_url = 'https://raw.githubusercontent.com/'\n",
    "    owner = 'SirTurtle'\n",
    "    repo = 'ML-BME-UofA-data'\n",
    "    branch = 'main'\n",
    "    url = '{}/{}/{}/{}/{}'.format(base_url, owner, repo, branch, source)\n",
    "    r = requests.get(url)\n",
    "    f = open(dest, 'wb')\n",
    "    f.write(r.content)\n",
    "    f.close()\n",
    "\n",
    "# Create the temp directory, if it doesn't already exist\n",
    "import os\n",
    "if not os.path.exists('temp'):\n",
    "   os.makedirs('temp')\n",
    "\n",
    "download_data('Week-6-Feature-extraction-and-selection/data/GA-structure-volumes-preterm.csv', 'temp/GA-structure-volumes-preterm.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "id": "6PXtCsjXMW58"
   },
   "source": [
    "### Exercise 1: Training Random Forests to Predict Gestational Age from Regional Brain Volumes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "id": "SdSMif7JMW58"
   },
   "source": [
    "Apply Scikit-Learn's Random Forest Regressor to the task of predicting gestational age (GA) from regional brain volumes. Complete the code below to:\n",
    "\n",
    "   1. Implement the training and testing of the ```DecisionTreeRegressor()```\n",
    "   2. Implement the training and testing of the  ```RandomForestRegressor()``` with 100 trees\n",
    "   3. Return and plot feature importances (see the [scikit-learn tutorial](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html) for guidance)\n",
    "   4. Compare and discuss the performance of the two implemented regressors\n",
    "   \n",
    "Data pre-processing has been performed for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "id": "2G6XFOzOMW58"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "DATAMAT = pd.read_csv('temp/GA-structure-volumes-preterm.csv', header=None)\n",
    "\n",
    "# Separate out data from labals\n",
    "DATA = DATAMAT.loc[:,1:] # volumes - we have 86 features and 164 samples\n",
    "LABELS = DATAMAT[0] # GA - 164\n",
    "\n",
    "# Split data into test and train\n",
    "X_train_GA, X_test_GA, y_train_GA, y_test_GA = train_test_split(DATA, LABELS, test_size=.4, random_state=42)\n",
    "\n",
    "# 1.1 Get baseline prediction from decision tree (no param optimisation)\n",
    "# 1.1.1 Create instance of DecisionTreeRegressor with default parameters\n",
    "tree_model = None\n",
    "\n",
    "# 1.1.2 Train\n",
    "None\n",
    "\n",
    "# 1.1.3 Test\n",
    "score_DT = None\n",
    "print('Decision Tree Score', score_DT)\n",
    "\n",
    "# 1.2 Get baseline prediction from Random Forest (no param optimisation)\n",
    "# 1.2.1 Create instance of RandomForestRegressor with default parameters. Use 100 trees (```n_estimators```)\n",
    "forest_model = None\n",
    "\n",
    "# 1.2.2 Train\n",
    "None\n",
    "# 5.2.3 Test\n",
    "score_RF1 = None\n",
    "print('Random Forest initial Score', score_RF1)\n",
    "\n",
    "# 1.3. Get feature importances\n",
    "importances = None\n",
    "std = None\n",
    "indices = None\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X_train_GA.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "# Plot the impurity-based feature importances of the 20 most important features in the forest\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(20), importances[indices][0:20],\n",
    "        color=\"r\", yerr=std[indices][0:20], align=\"center\")\n",
    "plt.xticks(range(20), indices[0:20])\n",
    "plt.xlim([-1, 20])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uHtbi6hdpCyD"
   },
   "source": [
    "1.4 Compare and discuss the performance of the two implemented regressors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GwmIFYhFpWzk"
   },
   "source": [
    "*Add your discussion here:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hideOutput": true,
    "hidePrompt": false,
    "id": "fL008ABrMW59"
   },
   "source": [
    "The Scikit-learn module allows for tuning of several key parameters of the model. Of these, perhaps the most important are:\n",
    "\n",
    "- Number of features\n",
    "- Max depth of trees\n",
    "- Number of estimators\n",
    "\n",
    "We will now use Scikit-Learn's ```GridSearchCV``` function to optimise the parameters of our Random Forest and improve performance on the above problem.\n",
    "\n",
    "For help on how to implement parameter optimisation using GridSearch, look at the scikit learn [documentation](https://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html#sphx-glr-auto-examples-model-selection-plot-grid-search-digits-py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "id": "cP8tzjxqMW59"
   },
   "source": [
    "### Exercise 2:  Perform Parameter Optimisation for Random Forests using ```GridSearchCV```\n",
    "\n",
    "Implement parameter optimisation using ```GridSearchCV```:\n",
    "   1. Create a ```dict``` called ```param_dist``` which specifies parameters for ```max_depth```, ```max_features``` and ```n_estimators ```\n",
    "   2. Create an instance (```grid```) of  ```GridSearchCV``` with ```RandomForestRegressor()``` as the model and ```param_dist``` as the search grid (with cross validation: ```cv=5```); then run on the training data\n",
    "   3. Apply the tuned parameters to the model and get a new test prediction\n",
    "   4. Compare the performance with the previously implemented Random Forest regressor\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "id": "I6MgpR4zMW59"
   },
   "outputs": [],
   "source": [
    "# 2.1 Specify parameters and distributions to sample from in the form of a dict\n",
    "# Hint: try selecting 5 values in the range [3; 50] for max_depth and 4 values in the range [10; 100] for n_estimators\n",
    "param_dist = None\n",
    "model = None\n",
    "\n",
    "\n",
    "# 2.2 Create an instance of GridSearchCV\n",
    "grid = None\n",
    "\n",
    "# 2.2.1 Run gridsearch on the training data\n",
    "None\n",
    "\n",
    "# Summarize the results of the grid search\n",
    "print('Best classification score achieved using grid search:', grid.best_score_)\n",
    "print('The parameters resulting in the best score are depth: {}, max_f: {}, and n_estimators: {} '\\\n",
    "      .format(grid.best_estimator_.max_depth,grid.best_estimator_.max_features,\\\n",
    "              grid.best_estimator_.n_estimators))\n",
    "\n",
    "\n",
    "# 2.3 Apply the tuned parameters to the model and get a new test prediction\n",
    "# 2.3.1 Create RF model using optimised parameters\n",
    "model = None\n",
    "\n",
    "# 2.3.2 Fit model to training data\n",
    "None\n",
    "\n",
    "# 2.3.3 Test\n",
    "score_RF2 = None\n",
    "print('Random Forest refined test Score', score_RF2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wkLsWpAzq6v7"
   },
   "source": [
    "2.4 Compare the performance with the previously implemented Random Forest regressor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1IWl6ZPTq-tu"
   },
   "source": [
    "*Add your discussion here:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Y5kcaQIMW5-"
   },
   "source": [
    "\n",
    "### Exercise 3: Training Adaboost to Predict Gestational Age from Regional Brain Volumes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YkE1PEMiMW5-"
   },
   "source": [
    "Apply Scikit-Learn's Adaboost regressor to the task of predicting gestational age (GA) from regional brain volumes.\n",
    "\n",
    "Using the test training data split from above (```X_train_GA, X_test_GA, y_train_GA, y_test_GA```) complete the code below to:\n",
    "   1.  Try Adaboost Regression using default parameters\n",
    "   2.  Try optimising parameters (such as min_samples_leaf and n_estimators) using GridSearchCV. Note, the entry for min_samples_leaf in the parameter dict has been completed for you to demonstrate how to tune the parameters of a base learner (in this case a decision tree)\n",
    "   3.  Apply the optimised parameters to train and test a new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PdVDfIoxMW5_"
   },
   "outputs": [],
   "source": [
    "# 3 Get baseline prediction from Adaboost (no param optimisation)\n",
    "# 3.1 Create instance of Adaboost with default parameters\n",
    "clf = None\n",
    "\n",
    "# 3.1.1 Train on data X_train_GA, y_train_GA\n",
    "None\n",
    "\n",
    "# 3.1.2 Test\n",
    "score_AB1 = None\n",
    "print('Adaboost initial Score', score_AB1)\n",
    "\n",
    "\n",
    "# 3.2 Optimise parameters using GridSearchCV\n",
    "# 3.2.1 Specify parameters and distributions to sample from in the form of a dict\n",
    "param_dist = {\"estimator__min_samples_leaf\": [1, 2, 3, 5, 10],\n",
    "              \"n_estimators\": None}\n",
    "model = None\n",
    "\n",
    "# 3.2.2 Create an instance of GridSearchCV\n",
    "grid = None  # Hint: use scoring='neg_mean_squared_error'\n",
    "\n",
    "# 3.2.3 Run gridsearch on the training data\n",
    "None\n",
    "\n",
    "print('Best regression score achieved using grid search:', grid.best_score_)\n",
    "print('The parameters resulting in the best score are min samples per leaf: {}, \\\n",
    "      and n_estimators {} '.format(\n",
    "    grid.best_estimator_.estimator_.min_samples_leaf,grid.best_estimator_.n_estimators))\n",
    "\n",
    "\n",
    "# 3.3 create RF model using optimised parameters\n",
    "model = None\n",
    "\n",
    "# 3.3.1 Fit model to training data\n",
    "None\n",
    "\n",
    "# 3.3.2 Test\n",
    "score_AB2 = None\n",
    "print('Adaboost refined test Score', score_AB2)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Hide code",
  "colab": {
   "provenance": [
    {
     "file_id": "1g_k4ZjinWddZ0NkPlhVaMCW1_ZP6nb0u",
     "timestamp": 1698090869436
    }
   ]
  },
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
