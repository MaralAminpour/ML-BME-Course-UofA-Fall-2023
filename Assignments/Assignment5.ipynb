{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RnIyuluH2bQV"
   },
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/MaralAminpour/ML-BME-Course-UofA-Fall-2023/blob/main/Assignments/Assignment5.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TIef0-Rw2X8z"
   },
   "source": [
    "### Assignment 5\n",
    "### Deadline: Thursday, November 30 at 11:59 PM\n",
    "#### The assignment must be submitted in the form of a Jupyter notebook and uploaded to eClass.\n",
    "\n",
    "## Marks:\n",
    "<ul type=\"none\">\n",
    "  <li><b>Exercise 1: 2 marks</b></li>\n",
    "  <li><b>Exercise 2: 4 marks</b></li>\n",
    "  <li><b>Exercise 3: 4 marks</b></li>\n",
    "</ul>   \n",
    "\n",
    "**Total: 10 marks**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Part 1: Predicting age from brain structures - Regression\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-8-Neural-Networks/imgs/BrainVolumes.png\" width = \"600\" style=\"float: right;\">\n",
    "\n",
    "In exercises 1 and 2 of this assignment we will apply what we learned in week 8 to our example of prediction of age of a baby from the volumes of brain structures. We will learn how to tune the parameters of the network and design a non-linear solution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6sogPDEGFDxm"
   },
   "source": [
    "Run the cell below to download and load the dataset with 86 structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nFC9uaHv2X81"
   },
   "outputs": [],
   "source": [
    "# This code will download the required data files from GitHub\n",
    "import requests\n",
    "import os\n",
    "def download_data(source, dest):\n",
    "    base_url = 'https://raw.githubusercontent.com/'\n",
    "    owner = 'MaralAminpour'\n",
    "    repo = 'ML-BME-Course-UofA-Fall-2023'\n",
    "    branch = 'main'\n",
    "    url = '{}/{}/{}/{}/{}'.format(base_url, owner, repo, branch, source)\n",
    "    r = requests.get(url)\n",
    "    f = open(dest, 'wb')\n",
    "    f.write(r.content)\n",
    "    f.close()\n",
    "\n",
    "# Create the temp directory, if it doesn't already exist\n",
    "if not os.path.exists('temp'):\n",
    "   os.makedirs('temp')\n",
    "\n",
    "download_data('Week-8-Neural-Networks/data/GA-brain-volumes-86-features.csv', 'temp/GA-brain-volumes-86-features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gnUPsj0HF_EQ"
   },
   "source": [
    "This block of code will load the dataset of 86 brain volumes and convert the feature matrix and target vector to tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kVrFT6x12X82",
    "outputId": "855cba74-7c13-4491-d9ba-0312327c8c7e"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "def CreateFeaturesTargets(filename):\n",
    "\n",
    "    df = pd.read_csv(filename, header=None)\n",
    "\n",
    "    # Convert from 'DataFrame' to numpy array\n",
    "    data = df.values\n",
    "\n",
    "    # Features are in columns one to end\n",
    "    X = data[:, 1:]\n",
    "\n",
    "    # Scale features\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    # Labels are in the column zero\n",
    "    y = data[:, 0].reshape(-1, 1)\n",
    "\n",
    "    # Return Features and Labels\n",
    "    return X, y\n",
    "\n",
    "X,y = CreateFeaturesTargets('temp/GA-brain-volumes-86-features.csv')\n",
    "\n",
    "# Perform scaling of the target values to support better convergence\n",
    "target_scaler = StandardScaler()\n",
    "y = target_scaler.fit_transform(y)\n",
    "\n",
    "print('Number of samples is', X.shape[0])\n",
    "print('Number of features is', X.shape[1])\n",
    "\n",
    "# Convert to tensors\n",
    "X = torch.from_numpy(X).float()\n",
    "y = torch.from_numpy(y).float()\n",
    "print('X shape: ', X.shape)\n",
    "print('y shape: ', y.shape)\n",
    "print('X type: ', X.type())\n",
    "print('y type: ', y.type())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xiNgj_Ca2X82"
   },
   "source": [
    "**Note 1:** We have converted the target values to a two dimensional vector and both feature matrix and target vector are PyTorch tensors of type `float`. This is required by PyTorch.\n",
    "\n",
    "**Note 2:** Unlike before we performed scaling of the target values as well. This improves convergence of stochastic gradient descent (the regression techniques we used before used analytical solutions or different optimisers).\n",
    "\n",
    "Below is the function `PlotTargets` to display true and predicted target values. Take a note of the function and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gf9Jl1OH2X82"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def PlotTargets(y_pred, y, label='Target values', plot_line=True):\n",
    "    if plot_line:\n",
    "        plt.plot([-3,3], [-3,3], 'r', label='$y=\\hat{y}$')\n",
    "    plt.plot(y, y_pred, 'o', label=label)\n",
    "\n",
    "    plt.xlabel('Expected target values')\n",
    "    plt.ylabel('Predicted target values')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9OwIHSY2X82"
   },
   "source": [
    "## Exercise 1: 2 marks\n",
    "\n",
    "In this exercise you will train and evaluate a single layer perceptron to predict the age of a baby from the volumes of 86 brain structures. First we will split the dataset into training, validation and test set.\n",
    "\n",
    "Cross-validation is rarely used in deep learning, due to long training times. You will see later in this exercise how these three sets are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yPd3Cl4-2X82",
    "outputId": "23bc2152-9bf3-4831-e405-71d62c96b5a7"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Extract test set\n",
    "groups = np.round(y/3)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=groups)\n",
    "\n",
    "# Extract validation set\n",
    "groups_val = np.round(y_train/3)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.15, random_state=42, stratify=groups_val)\n",
    "\n",
    "# Display info\n",
    "print('Training samples: ', y_train.shape[0])\n",
    "print('Validation samples: ', y_val.shape[0])\n",
    "print('Test samples: ', y_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q0s-mK5v2X83"
   },
   "source": [
    "To simplify the code, you are given three functions below:\n",
    "* `train` will perform one training epoch and return the current loss value\n",
    "* `validate` will return the loss value without performing any training.\n",
    "* `RMSE` will calculate the root mean squared error for the trained network and dataset that you specify. It will account for the scaling of the target values as well. The result is in **weeks GA**.\n",
    "\n",
    "Look the at functions and run the code to define the functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U7SwbWR72X83"
   },
   "outputs": [],
   "source": [
    "# 'train' performs one training epoch and returns MSE loss\n",
    "def train(net, X, y):\n",
    "    # 1. Clear gradients\n",
    "    optimizer.zero_grad()\n",
    "    # 2. Forward pass\n",
    "    prediction = net(X)\n",
    "    # 3. Compute loss\n",
    "    loss = loss_function(prediction, y)\n",
    "    # 4. Calculate gradients\n",
    "    loss.backward()\n",
    "    # 5. Update network parameters\n",
    "    optimizer.step()\n",
    "    # Return MSE loss\n",
    "    return loss.data  # we want only the loss value, not the gradients\n",
    "\n",
    "# 'validate' calculates and returns the loss any training\n",
    "def validate(net, X, y):\n",
    "    with torch.no_grad():  # no need to calculate gradients\n",
    "        # Forward pass\n",
    "        prediction = net(X)\n",
    "        # Calculate loss\n",
    "        loss = loss_function(prediction, y)\n",
    "        # Return MSE loss\n",
    "        return loss\n",
    "\n",
    "# 'RMSE' calculates RMSE in weeks GA\n",
    "def RMSE(net, X, y):\n",
    "    loss = validate(net, X, y).numpy()\n",
    "    rmse = np.sqrt(loss*target_scaler.var_[0])\n",
    "    return np.round(rmse, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fr-UkItq2X83"
   },
   "source": [
    "Below is the code that we used to fit linear regression model to the predict brain volume from age in Notebook ```8.1-Starting-with-Pytorch.ipynb```. The code will not work for our dataset, you need to modify it.\n",
    "\n",
    "**Task 1.1: 0.5 marks** Adjust the **architecture** of the network so that it can be used to predict age from 86 structures.\n",
    "\n",
    "**Task 1.2: 0.5 marks** You will see that the network does not train properly. The first thing we notice is that the training loss is increasing. This may be a sign that the **learning rate** is too high. Test smaller learning rates to see whether this will solve the problem. Choose the highest learning rate that is still small enough so that the training loss does not increase.\n",
    "\n",
    "**Task 1.3: 0.25 marks** Once you tuned your learning rate, you will probably find that the training loss is still steeply decreasing and the network has not yet converged to a good solution. Increase the number of **epochs** to 1000 and see what happens. Looking at the MSE loss plot, how many epochs did you need for the network to converge?\n",
    "\n",
    "**Task 1.4: 0.5 marks** You may notice that the number of epochs we have is rather arbitrary, and we do not know whether it is too few, and the network did not converge yet, or too many, and the network started overfitting. You may also wonder why do we have the **validation set**. We will use it to monitor the performance of the network during training. In this task, you will implement monitoring of the training during epochs using the validation set as follows:\n",
    "* Create a variable `val_losses` where you will save the validation loss at each epoch. Initialise it before the `for` loop similarly to `train_losses`.\n",
    "* At each epoch call the function `validate` to calculate loss on the validation set `X_val`, `y_val`. Append the validation loss returned by the function `validate` to the variable `val_losses`.\n",
    "* In the subplot `133`, plot the validation loss in addition to the training loss.\n",
    "* If needed, change the number of epochs to 10000 to find out when the validation loss starts increasing.\n",
    "\n",
    "**Task 1.5: 0.25 marks** We would of course like to choose the model that performs best on the validation set as our final trained model. We therefore need to keep training, while the loss on validation set is decreasing. Once it starts increasing, we will stop training to prevent overfitting. This technique is called **early stopping** and in fact acts as regularisation. To implement early stopping we need to `break` the `for` loop once the validation loss starts increasing. To do that, add this code at the end of the `for` loop:\n",
    "\n",
    "    if(i>1):\n",
    "        if val_losses[i-1]>val_losses[i-2]:\n",
    "            print('Final iteration: ', i)\n",
    "            break\n",
    "\n",
    "**Note:** Not all runs of the network will perform equally well. This is because we are using gradient descent and the weights of the network are initialised to random values. Therefore the fit will not always converge to an optimal solution. But you will also see that some runs produce a good solution, similar to the penalised regression techniques that we have covered earlier in this module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-e6lBrBN2X83"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class ANRegressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ANRegressor, self).__init__()\n",
    "        self.layer = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)\n",
    "        return x\n",
    "\n",
    "# Create network\n",
    "net = ANRegressor()\n",
    "print(net)\n",
    "\n",
    "# Mean squared error loss\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "# Stochastic gradient descent optimiser\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.02)\n",
    "\n",
    "# Train\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "for i in range(10):\n",
    "    loss = train(net, X_train, y_train)\n",
    "    train_losses.append(loss) # we save the losses to display them at the end\n",
    "    val_losses.append(validate(net, X_val, y_val))\n",
    "\n",
    "# Calculate training and test performance\n",
    "rmse_train = RMSE(net, X_train, y_train)\n",
    "print('Training RMSE: ', rmse_train)\n",
    "rmse_val = RMSE(net, X_val, y_val)\n",
    "print('Validation RMSE: ', rmse_val)\n",
    "rmse_test = RMSE(net, X_test, y_test)\n",
    "print('Test RMSE: ', rmse_test)\n",
    "\n",
    "# Display results\n",
    "plt.figure(figsize=(14,4))\n",
    "\n",
    "# Plot training set predictions\n",
    "plt.subplot(131)\n",
    "PlotTargets(net(X_train).data,y_train)\n",
    "plt.title('Training set')\n",
    "\n",
    "# Plot validation and test set predictions\n",
    "plt.subplot(132)\n",
    "PlotTargets(net(X_val).data, y_val, label='val targets')\n",
    "PlotTargets(net(X_test).data, y_test, label='test targets', plot_line=False)\n",
    "plt.title('Validation and Test set')\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.subplot(133)\n",
    "plt.plot(train_losses, label='training loss')\n",
    "plt.title('MSE loss')\n",
    "plt.plot(val_losses, label='validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ycKWLY2r2X83"
   },
   "source": [
    "## Exercise 2: 4 marks\n",
    "\n",
    "We will now tune a multi-layer perceptron to predict age from 6 brain volumes.\n",
    "\n",
    "First, we will download and load the dataset with 6 brain structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bKn2b0Is2X83"
   },
   "outputs": [],
   "source": [
    "# This code will download the required data files from GitHub\n",
    "import requests\n",
    "import os\n",
    "def download_data(source, dest):\n",
    "    base_url = 'https://raw.githubusercontent.com/'\n",
    "    owner = 'MaralAminpour'\n",
    "    repo = 'ML-BME-Course-UofA-Fall-2023'\n",
    "    branch = 'main'\n",
    "    url = '{}/{}/{}/{}/{}'.format(base_url, owner, repo, branch, source)\n",
    "    r = requests.get(url)\n",
    "    f = open(dest, 'wb')\n",
    "    f.write(r.content)\n",
    "    f.close()\n",
    "\n",
    "# Create the temp directory, if it doesn't already exist\n",
    "if not os.path.exists('temp'):\n",
    "   os.makedirs('temp')\n",
    "\n",
    "download_data('Week-8-Neural-Networks/data/GA-brain-volumes-6-features.csv', 'temp/GA-brain-volumes-6-features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4qApuZjGiC-"
   },
   "source": [
    "**Task 2.1: 0.5 marks** Fill in the missing code to convert the feature matrix and target vector from numpy arrays to a format suitable for training in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W30MRbUu2X84"
   },
   "outputs": [],
   "source": [
    "X, y = CreateFeaturesTargets('temp/GA-brain-volumes-6-features.csv')  # Note that this will overwrite X, y that we created in the previous exercise\n",
    "\n",
    "# Perform scaling of the target values to support better convergence\n",
    "target_scaler = StandardScaler()\n",
    "y = target_scaler.fit_transform(y)\n",
    "\n",
    "print('Number of samples is', X.shape[0])\n",
    "print('Number of features is', X.shape[1])\n",
    "\n",
    "# Convert to tensors\n",
    "X = None\n",
    "y = None\n",
    "print('X shape: ', X.shape)\n",
    "print('y shape: ', y.shape)\n",
    "print('X type: ', X.type())\n",
    "print('y type: ', y.type())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQBfSw1i2X84"
   },
   "source": [
    "**Task 2.2: 0.5 marks** Create training, validation and test set similarly to exercise 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "20nV0dyE2X84"
   },
   "outputs": [],
   "source": [
    "# Extract test set\n",
    "### Add your code here ###\n",
    "\n",
    "# Extract validation set\n",
    "### Add your code here ###\n",
    "\n",
    "# Display info\n",
    "print('Training samples: ', y_train.shape[0])\n",
    "print('Validation samples: ', y_val.shape[0])\n",
    "print('Test samples: ', y_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g01TAJQU2X84"
   },
   "source": [
    "**Task 2.3: 1 mark** Perform training and evaluation of the performance using the same code as you developed in Exercise 1. You need to adapt the architecture to take 6 input features, but other than that the code should work. Adjust the learning rate to achieve optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CMVd4-ZY2X84"
   },
   "outputs": [],
   "source": [
    "### Add you code here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45Vp-2WrHmJA"
   },
   "source": [
    "**Task 2.4: 1.5 marks** Modify the code from task 2.3 further to implement a multi-layer perceptron architecture as follows:\n",
    "* First `Linear` layer with 6 outputs\n",
    "* `ReLU` activation\n",
    "* Second `Linear` layer with 6 inputs and 1 output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nUymWKR2KwZV"
   },
   "outputs": [],
   "source": [
    "### Add your code here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PPv71uO-Ky1h"
   },
   "source": [
    "**Question 2.1: 0.5 marks** Can you achieve better performance with this non-linear network? Can you hypothesize why/why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oJx9lmVfHD4K"
   },
   "source": [
    "## Part 2: Your own Single Layer Perceptron - Classification\n",
    "\n",
    "## Exercise 3: 4 marks\n",
    "\n",
    "In this exercise you will implement and train your own single layer perceptron classifier using the Wisconsin Breast Cancer dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eo-4ojKSHkNK"
   },
   "source": [
    "Run the cells to load the feature matrix and the label vector and split the data into training and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V2b9Yqe-Hor5"
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# Load feature matrix and label vector\n",
    "data = datasets.load_breast_cancer()\n",
    "X = StandardScaler().fit_transform(data.data)\n",
    "y = data.target\n",
    "\n",
    "print('Features dim: ', X.shape)\n",
    "print('Labels dim: ', y.shape)\n",
    "print('We have {} samples and {} features.'.format(X.shape[0], X.shape[1]))\n",
    "print('We have labels: ', np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T_HO3pp5Hqgc"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "print('Training samples: ', y_train.shape[0])\n",
    "print('Validation samples: ', y_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "frzbldW-Ht3e"
   },
   "source": [
    "### Architecture\n",
    "Our single layer perceptron will consist of\n",
    "\n",
    "1 linear layer with 30 inputs and 1 output\n",
    "A sigmoid activation function\n",
    "We will minimise the mean squared error loss.\n",
    "\n",
    "The functions below implement these elements of the architecture, plus there is a function to calculate a generic derivative. Review this functions and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bANd-GuJHx8u"
   },
   "outputs": [],
   "source": [
    "# Sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# Linear layer - we do not have a bias here for simplicity\n",
    "def linear(x, w):\n",
    "    return np.matmul(x, w)\n",
    "\n",
    "# Mean squared error loss\n",
    "def mse(y, y_pred):\n",
    "    return np.mean(np.square(y - y_pred))\n",
    "\n",
    "# A general derivative of a function (no need to implement individual derivatives)\n",
    "def derivative(f, z, eps=0.000001):\n",
    "    return (f(z + eps) - f(z - eps))/(2 * eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kfMtV5KaH2tq"
   },
   "source": [
    "### Theory review\n",
    "The single layer perceptron is composed of one **linear layer** of neurons that can be expressed as\n",
    "$$z=Xw$$\n",
    "where $X$ is the feature matrix, $w$ are the parameters of the network and $z$ is the output of the linear layer.\n",
    "**Note**: we will not implement the bias term in this exercise for simplicity.\n",
    "\n",
    "The output of the linear layer is passed through the activation function, which in our case is **sigmoid** $\\sigma$:\n",
    "$$p=\\sigma(z)$$\n",
    "The output $p$ is the probability of the positive class 1.\n",
    "\n",
    "We will find the network parameter vector $w$ by **minimising the mean squared error loss** between predicted probability $p$ and true labels $y$:\n",
    "$$loss=\\sum_{n=1}^N(y_n-p_n)^2$$\n",
    "\n",
    "We can iteratively **update the weight vector** $w$ with the gradient descent equation:\n",
    "$$w^{iter+1}=w^{iter}-\\eta \\frac{\\partial loss}{\\partial w}$$\n",
    "where $w^{iter}$ is the estimated weight vector at the iteration $iter$ and $\\eta$ is the learning rate.\n",
    "\n",
    "In the case of MSE loss and sigmoid activation, the derivative is\n",
    "$$\\frac{\\partial loss}{\\partial w}=(y-p)\\sigma'(z)X$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qN7QZSwNIAZZ"
   },
   "source": [
    "### Training\n",
    "\n",
    "Complete the code below to train the single layer perceptron. The learning rate and number of epoch has been already set for you. We start by initialising the weight vector `w` to random values.\n",
    "\n",
    "**Task 3.1: 0.25 marks** Implement the forward pass. To do that:\n",
    "* evaluate the linear layer for the training feature matrix `X_train`;\n",
    "* pass the result through the sigmoid activation.\n",
    "\n",
    "**Task 3.2: 0.25 marks** Calculate the training loss. There is also code to save it for plotting after we finished training.\n",
    "\n",
    "**Task 3.3: 0.5 marks** Implement the backward pass. To do that you need to implement the calculation of the gradients of the:\n",
    "* loss\n",
    "* activation\n",
    "* linear layer\n",
    "The code to multiply them together using the chain rule is provided.\n",
    "\n",
    "**Task 3.4: 0.25 marks** Implement the update of the network weights.\n",
    "\n",
    "Once you successfully complete the code, run it. You will see how the loss is evolving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yoUznBQkIGMN"
   },
   "outputs": [],
   "source": [
    "# Set learning rate\n",
    "lr = 0.01\n",
    "\n",
    "# Set number of epochs\n",
    "epochs = 20\n",
    "\n",
    "# Initialize the weights randomly with mean 0\n",
    "w = 2*np.random.random(np.array(X).shape[1]) - 1\n",
    "print('Weight vector dimension: ', w.shape)\n",
    "\n",
    "# Training loop\n",
    "train_loss = []\n",
    "for iter in range(epochs):\n",
    "\n",
    "    # 3.1 Forward pass - predict network output\n",
    "    # Implement linear layer\n",
    "    z = None\n",
    "    # Predict output probabilities\n",
    "    p_pred = None\n",
    "\n",
    "    # 3.2 Calculate and save training loss\n",
    "    loss = None\n",
    "    train_loss.append(loss)\n",
    "\n",
    "    print('Epoch {}: MSE = {}'.format(iter, np.round(loss, 4)))\n",
    "\n",
    "    # 3.3 Backward pass - calculate gradients\n",
    "    # Gradient of the loss: error between predicted and true training labels\n",
    "    loss_grad = None\n",
    "    # Gradient of activation: derivative of sigmoid\n",
    "    act_grad = None\n",
    "    # Gradient of linear layer: feature matrix\n",
    "    lin_grad = None\n",
    "    # Multiply gradients (chain rule)\n",
    "    grad = np.matmul(loss_grad*act_grad, lin_grad)\n",
    "\n",
    "    # 3.4 Update the weights of the network\n",
    "    w -= None\n",
    "\n",
    "# Plot training loss during epochs\n",
    "plt.plot(train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9NDk8GCeII3h"
   },
   "source": [
    "**Task 3.5: 0.5 marks** Evaluate the accuracy of the trained network on training and test set. To do that:\n",
    "* implement the forward pass of the network on the training set to calculate the predicted labels (code given);\n",
    "* evaluate the accuracy on the training set using the given function `accuracy`;\n",
    "* implement the forward pass of the network in test set;\n",
    "* evaluate the accuracy for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sCJ6rlF-INog"
   },
   "outputs": [],
   "source": [
    "def accuracy(y,y_pred):\n",
    "    a = np.sum(y==y_pred)/y.shape[0]\n",
    "    return a\n",
    "\n",
    "# Predict on training set\n",
    "y_pred = np.around(sigmoid(linear(X_train, w)))\n",
    "acc_train = None\n",
    "print('Train accuracy: ', np.round(acc_train, 2))\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_test = None\n",
    "acc_test = None\n",
    "print('Test accuracy: ', np.round(acc_test, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sLxoUZJ7IQCX"
   },
   "source": [
    "### Cross Entropy Loss\n",
    "\n",
    "You already know that binary cross-entropy loss is more common to use for binary classification rather than MSE loss. **Binary cross-entropy loss** is given by\n",
    "$$BCELoss=\\sum_{n=1}^N(-y_n \\log(p_n)-(1-y_n)\\log(1-p_n))$$\n",
    "\n",
    "We have used MSE loss in the lecture instead, because it is easier to calculate the derivatives. However, the derivative of BCE loss with sigmoid activation with respect to the weights actually results in a much simple derivative:\n",
    "\n",
    "$$\\frac{\\partial BCELoss}{\\partial w}=(y-p)X$$\n",
    "\n",
    "**Task 3.6: 0.25 marks** Complete the implementation of BCE loss in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QH9DT43DIT_T"
   },
   "outputs": [],
   "source": [
    "# Binary cross-entropy loss\n",
    "def BCE(y,p):\n",
    "    return -np.mean(np.log(p)*y_train + None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XlwZtTPRIWGK"
   },
   "source": [
    "**Task 3.7: 2 marks** Implement the gradient of the binary cross entropy loss and train the single layer perceptron using the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IR0_1BV7IYtV"
   },
   "outputs": [],
   "source": [
    "# Set learning rate\n",
    "lr = 0.001\n",
    "\n",
    "# Set number of epochs\n",
    "epochs = 20\n",
    "\n",
    "# Initialize the weights randomly with mean 0\n",
    "w = None\n",
    "\n",
    "# Training loop\n",
    "train_loss = []\n",
    "for iter in range(epochs):\n",
    "\n",
    "    # Forward pass - predict labels\n",
    "    z = None\n",
    "    p_pred = None # probabilities\n",
    "\n",
    "    # Calculate and save training loss\n",
    "    loss = BCE(None, None)\n",
    "    train_loss.append(loss)\n",
    "\n",
    "    print('Epoch {}: BCE = {}'.format(iter, np.round(loss, 4)))\n",
    "\n",
    "    # Backward pass - calculate gradients\n",
    "    # Error between predicted and true training labels\n",
    "    error = None\n",
    "    # Gradient\n",
    "    grad = np.matmul(None, None)  # Hint: the formula for the derivative of BCE loss with sigmoid activation with respect to the weights is given above\n",
    "\n",
    "    # Update the weights of the network\n",
    "    w -= None\n",
    "\n",
    "plt.plot(train_loss)\n",
    "\n",
    "# Predict on training set\n",
    "y_pred = None\n",
    "acc_train = None\n",
    "print('Train accuracy: ', np.round(acc_train, 2))\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_test = None\n",
    "acc_test = None\n",
    "print('Test accuracy: ', np.round(acc_test, 2))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
