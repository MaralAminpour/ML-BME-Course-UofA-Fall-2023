{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNXVgx55OdOi5hyZ92akdo8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaralAminpour/ML-BME-Course-UofA-Fall-2023/blob/main/Week-6-PCA/ensemble_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Weak Learners/Decision stump\n",
        "\n",
        "A weak learner is a classifier that has a slight correlation with the actual classification, allowing it to label examples marginally better than mere random guessing. Such a learner could be as simple as a threshold set on one feature, a linear classifier, or even potentially a non-linear classifier.\n",
        "\n",
        "In the realm of ensemble learning, which we'll delve into next week, it typically refers to a basic learning rule applied to a portion of the feature space.\n",
        "\n",
        "The underlying concept is that while each weak learner might perform just a tad better than random chance, combining multiple weak learners can lead to a much stronger predictive power.\n",
        "\n",
        "On the other hand, a strong learner is a robust classifier that aligns closely and effectively with the true classification."
      ],
      "metadata": {
        "id": "sh6nELJ8Bfwk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two choices for deciding on a threshold for a classification problem:\n",
        "\n",
        "Information gain: Decrease in entropy after a dataset is split on an attribute.\n",
        "$$ I(S_j) = H(S_j) - \\sum_i \\frac{S_j^i}{S_j} H(S_j^i) $$\n",
        "Here, \\( H(S_j) \\) is the entropy of the root node and \\( H(S_j^i) \\) is the entropy of the leaf node.\n",
        "\\( S_j \\) = total number of data points reaching parent node j;\n",
        "\\( S_j^i \\) = total number of data points reaching child i from parent j;\n",
        "\n",
        "Gini index: Indicates how mixed the classes are following the split.\n",
        "$$ I(S_j) = \\sum_i \\frac{S_j^i}{S_j} Gini_i $$\n",
        "where\n",
        "$$ Gini = 1 - \\sum_{y_k \\in Y} p(y_k)^2 $$\n",
        "\\( p(y_k) \\) is the proportion at a given node that are of class k."
      ],
      "metadata": {
        "id": "zQ-h4KFxG4ze"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### 1. Estimate the squared distances between all points:\n",
        "The squared distance between two points \\( x_i \\) and \\( x_j \\) in a Euclidean space is given by:\n",
        "\\[ d_{ij}^2 = ||x_i - x_j||^2 \\]\n",
        "\n",
        "### Explanation:\n",
        "This step quantifies the dissimilarity between every pair of data points in the dataset. The result is a distance matrix where the entry in the \\(i^{th}\\) row and \\(j^{th}\\) column represents the squared distance between the \\(i^{th}\\) and \\(j^{th}\\) data points.\n",
        "\n",
        "### 2. Calculate the k-nearest neighbour adjacency graph \\( A \\):\n",
        "\\[ A_{ij} =\n",
        "\\begin{cases}\n",
        "1 & \\text{if } x_j \\text{ is a k-nearest neighbour of } x_i \\\\\n",
        "0 & \\text{otherwise}\n",
        "\\end{cases}\n",
        "\\]\n",
        "\n",
        "### Explanation:\n",
        "The adjacency graph represents relationships between data points. If two points are close (i.e., one is among the k-nearest neighbors of the other), their corresponding entry in the adjacency matrix \\( A \\) is set to 1; otherwise, it's 0.\n",
        "\n",
        "### 3. Make this symmetric:\n",
        "This step ensures that if \\( x_i \\) is a neighbor of \\( x_j \\), then \\( x_j \\) is also considered a neighbor of \\( x_i \\). This is achieved by taking the maximum value for each pair:\n",
        "\\[ A_{ij} = \\max(A_{ij}, A_{ji}) \\]\n",
        "\n",
        "### 4. Calculate the degree matrix \\( D \\):\n",
        "A diagonal matrix where each diagonal entry \\( D_{ii} \\) is the sum of the entries in the \\(i^{th}\\) row of \\( A \\):\n",
        "\\[ D_{ii} = \\sum_j A_{ij} \\]\n",
        "\n",
        "### Explanation:\n",
        "The degree matrix captures how connected each point is to other points. The diagonal entry \\( D_{ii} \\) represents the total number of connections (or edges) the \\(i^{th}\\) data point has in the graph.\n",
        "\n",
        "### 5. Estimate graph Laplacian \\( L \\):\n",
        "\\[ L = D - A \\]\n",
        "\n",
        "### Explanation:\n",
        "The Laplacian matrix captures the structure of the graph. It is a difference between the degree matrix and the adjacency matrix.\n",
        "\n",
        "### 6. Estimate the embedding from the eigenvectors of \\( L \\):\n",
        "For Laplacian embedding, the final low-dimensional representation is obtained from the eigenvectors of \\( L \\) corresponding to its smallest eigenvalues (excluding 0).\n",
        "\n",
        "### Explanation:\n",
        "Eigenvectors corresponding to the smallest eigenvalues of the Laplacian matrix best capture the structure of the data manifold. By selecting a subset of these eigenvectors, a lower-dimensional embedding of the data is obtained that preserves local manifold structures."
      ],
      "metadata": {
        "id": "0twpsfSAHEV6"
      }
    }
  ]
}