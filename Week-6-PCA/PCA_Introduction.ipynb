{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPkBecpjWTLoeC7wKgDLx7h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaralAminpour/ML-BME-Course-UofA-Fall-2023/blob/main/Week-6-PCA/PCA_Introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dimensionality reduction\n",
        "\n",
        "Let's get started on how these neat concepts can be used for dimensionality reduction, especially with Principal Component Analysis (PCA). And guess what? We'll also explore how to practically bring PCA to life using scikit-learn. Excited? Let's jump in!"
      ],
      "metadata": {
        "id": "l0OKNi7Bhvco"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## When to Use PCA?\n",
        "\n",
        "High-dimensional data is a common issue experienced in machine learning practices, as we typically feed a large amount of features for model training. This results in the limitation of models having **less interpretability and higher complexity** - also known as the curse of dimensionality.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-5-Clustering%20and%20Features/imgs/curse.png' width=500px >\n",
        "\n",
        "PCA can be beneficial when the dataset is **high-dimensional (i.e. contains many features)** and it is widely applied for dimensionality reduction.\n",
        "\n",
        "Additionally, PCA is also used for discovering the **hidden relationship among feature variables** and **reveal underlying patterns** that could be very insightful. PCA attempts to find linear components that capture as much variance in the data as possible. And **the first principal component (PC1) is typically composed of features that contributes most to the model predictions.**"
      ],
      "metadata": {
        "id": "rXt6J_4WidPb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How Does PCA Work?\n",
        "\n",
        "Think of PCA (Principal Component Analysis) as a way to simplify a lot of data. Here's how it works in simple terms:\n",
        "\n",
        "1. We try to understand how different parts of our data change together using something called a \"covariance matrix.\"\n",
        "\n",
        "2. We then find some special directions (called \"eigenvectors\") where the data changes the most.\n",
        "\n",
        "3. These special directions help us represent our big, complicated data in a simpler way.\n",
        "\n",
        "Now, terms like \"covariance matrix\" and \"eigenvector\" might sound fancy. But they're just tools to help us **see our data in a clearer way. So, instead of thinking about them as tricky math things, picture them as cool visual changes to our data. It makes understanding everything a whole lot easier**!"
      ],
      "metadata": {
        "id": "m3vS1hsWkxuE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You might remember us chatting about how matrices can be used in math operations, like the dot product. But did you know they can also show us cool visual changes? Let's break it down:\n",
        "\n",
        "Imagine a 2x2 grid or a tiny chessboard. This is our matrix. If you slice it down the middle, you get two lists (we call them vectors). The list on the left is like our horizontal ruler (x-axis), and the one on the right is our vertical ruler (y-axis). Together, they help create a mini world of a 2D space.\n",
        "\n",
        "Now, there's a special matrix called the \"identity matrix\". It has [1,0] for the horizontal ruler and [0,1] for the vertical one. The cool thing about it? If you mix any list (vector) with this special matrix, you get the same list back!\n",
        "\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-5-Clustering%20and%20Features/imgs/matrix1.png.webp' width=500px >\n",
        "\n",
        "Matrix transformation boils down to shifting the scale and direction of the x-axis and y-axis. Imagine the x-axis and y-axis as stretchy rubber bands on a plane. Matrix transformation is like pulling or twisting these bands in different ways.\n",
        "\n",
        "Let's say you tug the x-axis rubber band to make it twice as long. What you're doing is changing its basic direction from [1,0] to [2,0]. This means everything on that axis gets stretched out and becomes double its original size!\n",
        "\n",
        "**In short, playing around with matrices is like giving a new look to our familiar x and y axes.**\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-5-Clustering%20and%20Features/imgs/matrix2.png.webp' width=500px >\n",
        "\n",
        "We can additionally combine both the x-axis and y-axis for more **complicated scaling, rotating or shearing transformation**. A typically example is the **mirror matrix **where we swap the x and y axis. For a given vector [1,2], we will get [2,1] after the mirror transformation.\n",
        "\n",
        "Alright, let's jazz things up a bit. We can **play with both the x-axis and y-axis together** for some cool effects like stretching, spinning, or even skewing!\n",
        "\n",
        "Ever played with a mirror? Well, matrices can do something similar. Imagine a matrix that acts like a mirror, **flipping the x and y axes.** So, if you had a point at [1,2], after using our \"mirror\" matrix, it'll land at [2,1]. It's like giving the point a fun house mirror effect!\n",
        "\n",
        "In simple words, with matrices, we can create all sorts of fun and funky changes to our points."
      ],
      "metadata": {
        "id": "hgs6AIO5nKYZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you would like to practice these transformations in python and skip the manual calculations, we can use following code to perform these dot products and visualize the result of the transformation using plt.quiver() function."
      ],
      "metadata": {
        "id": "K1ruAwsOnyL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# define matrices and vector\n",
        "x_scaled_matrix = np.array([[2,0],[0,1]])\n",
        "mirror_matrix = np.array([[0,1],[1,0]])\n",
        "v = np.array([1,2])\n",
        "\n",
        "# matrix transformation\n",
        "mirrored_v = mirror_matrix.dot(v)\n",
        "x_scaled_v = x_scaled_matrix.dot(v)\n",
        "\n",
        "# plot transformed vectors\n",
        "origin = np.array([[0, 0], [0, 0]])\n",
        "plt.quiver(*origin, v[0], v[1], color=['black'],scale=10, label='original vector')\n",
        "plt.quiver(*origin, mirrored_v[0], mirrored_v[1] , color=['#D3E7EE'], scale=10, label='mirrored vector' )\n",
        "plt.quiver(*origin, x_scaled_v[0], x_scaled_v[1] , color=['#C6A477'], scale=10, label='x_scaled vector')\n",
        "plt.legend(loc =\"lower right\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "n7y2NLB7pycZ",
        "outputId": "8b69f049-cb31-465b-f963-d201fc6274b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7b31e204aec0>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Covariance Matrix: The Heartbeat of PCA!\n",
        "\n",
        "*In Short: covariance matrix represents the pairwise correlations among a group of variables in a matrix form.*\n",
        "\n",
        "Covariance matrix is another critical concept in PCA process that represents the data variance in the dataset. To understand the details of covariance matrix, we firstly need to know that covariance measures the magnitude of how one random variable varies with another random variable. For two random variable x and y, their covariance is formulated as below and higher covariance value indicates stronger correlation between two variables.\n",
        "\n",
        "let's chat about something called the \"covariance matrix.\" Think of it as a magical table that tells us how different pieces of our data move together. It's super important for the PCA process because it captures the heartbeat (or variance) of our dataset.\n",
        "\n",
        "Now, let's break down \"covariance\" a bit. Imagine you and your friend are dancing. Sometimes you both move in sync, sometimes in opposite directions, and sometimes there's no connection at all in your dance moves. Covariance is like a scorecard that tells you how in-sync (or not) your dance moves are!\n",
        "\n",
        "If we want to get a bit technical, for two dancers (or random variables) x and y, their dance score (or covariance) can be calculated using a formula. A higher score means you both are more in tune with each other's moves. In data terms, a higher covariance value indicates a stronger connection or correlation between two variables.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-5-Clustering%20and%20Features/imgs/matrix5.png.webp' width=300px >\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "m6bWTd2nqAk_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Covariance Matrix: The Relationship Checker\n",
        "\n",
        "When given a set of variables (e.g. x1, x2, ... xn) in a dataset, covariance matrix is typically used for representing the covariance value between each variable pairs in a matrix format.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-5-Clustering%20and%20Features/imgs/matrix6.png.webp' width=400px >\n",
        "\n",
        "\n",
        "Imagine you have a bunch of friends: x1, x2,... xn. Some of these friends are super close and always hang out together, while others might not get along that well. Now, if you wanted to map out how close each friend is to every other friend, that's where the covariance matrix comes into play!\n",
        "\n",
        "Think of the covariance matrix as a big chart. Each row and column represents one of your friends. The chart then tells you the \"closeness score\" between each pair of friends. If x1 and x2 are always together, there'll be a high score in the box where their row and column meet. If they aren't close, the score will be lower.\n",
        "\n",
        "In simple terms, the covariance matrix helps us see the relationship strength between each pair of variables (or friends) in our dataset. It's like a big friendship chart for data!\n",
        "\n"
      ],
      "metadata": {
        "id": "l7Lik9blrPyb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Covariance Matrix: The Trend Magnifier\n",
        "\n",
        "Multiplying the covariance matrix will transform any vector towards the **direction that captures the trend of variance in the original dataset.**\n",
        "\n",
        "let's think of the covariance matrix as a special pair of glasses. When a vector (let's call it a \"light beam\") passes through these glasses, it gets bent or shifted towards the main trend or direction of our data.\n",
        "\n",
        "So, if our data has a trend where, let's say, things generally increase, our vector or \"light beam\" will also point in that increasing direction after passing through our **covariance matrix glasses**.\n",
        "\n",
        "In simpler words, multiplying by the covariance matrix is like using a tool that emphasizes the main story or trend in our data. It's helping highlight where most of the action is happening!"
      ],
      "metadata": {
        "id": "prxJotKJr5bF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us use a simple example to simulate the effect of this transformation. Firstly, we randomly generate the variable x0, x1 and then compute the covariance matrix."
      ],
      "metadata": {
        "id": "4ZlFS49IsG9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate random variables x0 and x1\n",
        "import random\n",
        "x0 = [round(random.uniform(-1, 1),2) for i in range(0,100)]\n",
        "x1 = [round(2 * i + random.uniform(-1, 1) ,2) for i in x0]\n",
        "\n",
        "# compute covariance matrix\n",
        "X = np.stack((x0, x1), axis=0)\n",
        "covariance_matrix = np.cov(X)\n",
        "print('covariance matrix\\n', covariance_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfH7RGf_sSt5",
        "outputId": "b759aa76-8a43-47fe-be6a-f273c64c04c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "covariance matrix\n",
            " [[0.34249312 0.66848301]\n",
            " [0.66848301 1.62123026]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then transform some random vectors by taking the dot product between each of them and the covariance matrix."
      ],
      "metadata": {
        "id": "JVfiCCtQsWDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot original data points\n",
        "plt.scatter(x0, x1, color=['#D3E7EE'])\n",
        "\n",
        "# vectors before transformation\n",
        "v_original = [np.array([[1,0.2]]), np.array([[-1,1.5]]), np.array([[1.5,-1.3]]), np.array([[1,1.4]])]\n",
        "\n",
        "# vectors after transformation\n",
        "for v in v_original:\n",
        "    v_transformed = v.dot(covariance_matrix)\n",
        "    origin = np.array([[0, 0], [0, 0]])\n",
        "    plt.quiver(*origin, v[:, 0], v[:, 1], color=['black'], scale=4)\n",
        "    plt.quiver(*origin, v_transformed[:, 0], v_transformed[:, 1] , color=['#C6A477'], scale=10)\n",
        "\n",
        "plt.axis('scaled')\n",
        "plt.xlim([-2.5,2.5])\n",
        "plt.ylim([-2.5,2.5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "r_52L4B3sW66",
        "outputId": "632587a0-619a-44fa-9587-7501a660913b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-2.5, 2.5)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original vectors prior to transformation are in black, and after transformation are in brown. As you can see, the original vectors that are pointing at different directions have become more conformed to the general trend displayed in the original dataset (i.e. the blue dots). Because of this property, covariance matrix is important to PCA in terms of describing the relationship between features.\n",
        "\n",
        "Imagine a scatter plot filled with blue dots, representing our original dataset. These dots are scattered, showing various patterns and relationships.\n",
        "\n",
        "Now, picture a few black arrows (our original vectors) scattered on the same plot. They're pointing in different directions, representing various patterns within the data.\n",
        "\n",
        "Enter the magic of the covariance matrix!\n",
        "\n",
        "After passing these black arrows through the transformation of the covariance matrix, they change into brown arrows. These brown arrows now more closely follow the main pattern or trend that the blue dots are showing. **Instead of being scattered randomly, they align or \"conform\" better to the general direction that most of the blue dots are heading towards.**\n",
        "\n",
        "This ability of the covariance matrix to align vectors with the core trends of the data is why it's so crucial in PCA. By understanding and capturing these main trends (or relationships between features), **PCA can represent the essence of the data in fewer dimensions, making our analyses more focused and efficient.**\n",
        "\n",
        "So, in essence, the covariance matrix acts like a guide, helping PCA spotlight the major patterns and relationships in the data. It's like turning those scattered black arrows into coordinated brown arrows that march in line with where the data is leading them!"
      ],
      "metadata": {
        "id": "Znrb7IENsp1a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Eigenvalue and Eigenvector\n",
        "\n",
        "*In Short: Eigenvector (v) of a matrix (A) remains at the same direction after the matrix transformation, hence Av = λv where v represents the corresponding eigenvalue. Representing data using eigenvector and eigenvalue reduces the dimensionality while maintaining the data variance as much as possible.*\n",
        "\n",
        "To make this idea clearer, let's look at an easy-to-follow example. Say we have a matrix [[0,1],[1,0]]. One of its special vectors, called an eigenvector, is [1,1], and it has a matching value, known as an eigenvalue, of 1.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-5-Clustering%20and%20Features/imgs/matrix8.png.webp' width=300px >\n",
        "\n",
        "let's see what's happening with our special matrix: [[0,1],[1,0]]. This matrix is like a magic mirror. When a vector looks into it, its x and y coordinates get swapped around.\n",
        "\n",
        "Let's use our vector [1,1] as an example. When it looks in this \"mirror\", its reflection is... itself! The x and y coordinates don't really change. This is why it's a special vector for our matrix, or what we call an \"eigenvector\".\n",
        "\n",
        "Now, the \"eigenvalue\" of 1 is like a size tag. It tells us that our vector, after transformation, remains the same size and points in the same direction. No stretching or squishing happens.\n",
        "\n",
        "What's super cool is this: instead of using a full-blown matrix to talk about changes, we can use just a single number, the eigenvalue, to summarize the transformation's effect. This eigenvalue is a powerful indicator. It tells us how much of the original data's patterns or \"variance\" is captured by our eigenvector.\n",
        "\n",
        "Now, to visualize this, imagine a scatter plot filled with blue dots showing our data spread. Add in our special vector, the eigenvector, as a big red arrow. You'll notice that this red arrow aligns beautifully with the main trend in our data. It's like the backbone, showing where most of the action is happening!\n",
        "\n",
        "In coding terms, we could use a script to draw this picture, highlighting the dominant eigenvector in vibrant red, revealing its alignment with the data's primary direction or variance. It's all about making the abstract concrete and visual!\n",
        "\n"
      ],
      "metadata": {
        "id": "VKFGI5eas-4l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.linalg import eig\n",
        "eigenvalue,eigenvector = eig(covariance_matrix)\n",
        "plt.quiver(*origin, eigenvector[:,1][0], eigenvector[:,1][1] , color=['red'], scale=4, label='eigenvector')"
      ],
      "metadata": {
        "id": "cjA7ydcjzDBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Principal Components\n",
        "\n",
        "Making Sense of Data Magic with PCA!\n",
        "\n",
        "Okay, let’s piece this puzzle together!\n",
        "\n",
        "1. **Covariance Matrix Magic**: Remember our fancy table, the covariance matrix? It shows how different parts of our data move together. Especially useful when we have lots of variables!\n",
        "\n",
        "2. **Eigenvectors - The Heroes**: These special vectors (eigenvectors) have a knack for capturing the main patterns in data but in a simpler, less cluttered way.\n",
        "\n",
        "3. **Eigenvalues - The Power Meters**: Each eigenvector has a buddy called an eigenvalue. This buddy tells us how strong or influential its corresponding eigenvector is.\n",
        "\n",
        "Now, when we mix the covariance matrix with these eigenvectors and eigenvalues, magic happens!\n",
        "\n",
        "**Enter Principal Components (PCs)**:\n",
        "Think of PCs as the all-stars team of eigenvectors. When we calculate the eigenvectors for our covariance matrix, we get these PCs. And just like in sports, there's a ranking!\n",
        "\n",
        "- **PC1**: The top player. The eigenvector with the highest eigenvalue. This is the one that captures the biggest trends in our data. If our data were a story, PC1 would be the headline!\n",
        "  \n",
        "- **PC2, PC3, ... PCn**: The supporting cast. They also capture important patterns but maybe not as prominently as PC1.\n",
        "\n",
        "Remember that red vector in the image? That's our superstar, PC1. It's showing the main direction where our data has the most action.\n",
        "\n",
        "**Why Does This Matter?**\n",
        "Let's say we want a simpler view of our data without losing too much detail. We'd pick the top PCs (like PC1, PC2) because they hold the most information about the original data, thanks to their high eigenvalues.\n",
        "\n",
        "In short, PCA is like a clever camera that captures the essence of our data using fewer, but super informative, shots!"
      ],
      "metadata": {
        "id": "ogLNn3_ezKX3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PCA Implementation in Machine Learning\n",
        "\n"
      ],
      "metadata": {
        "id": "fWv0BeaDz9uv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Take-Home Message\n",
        "\n",
        "In the previous article, we have introduced using linear algebra for data representation in machine learning. Now we introduced another common use case of linear algebra in ML for dimensionality reduction - Principal Component Analysis (PCA). We firstly discussed the theory behind PCA:\n",
        "\n",
        "1. represent the data variance using covariance matrix\n",
        "\n",
        "2. use eigenvector and eigenvalue to capture data variance in a lower dimension\n",
        "\n",
        "3. The principal component is the eigenvector and eigenvalue of the covariance matrix\n",
        "\n",
        "Furthermore, we utilize scikit-learn to implement PCA through the following procedures:\n",
        "\n",
        "1. standardize data into the same scale\n",
        "\n",
        "2. apply PCA on the scaled data\n",
        "\n",
        "3. visualize explained variance using scree plot\n",
        "\n",
        "4. interpret the principal components composition\n",
        "\n",
        "5. use principal components in ML algorithm"
      ],
      "metadata": {
        "id": "UkdDbEAM0X3d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Very intresting article on PCA**\n",
        "\n",
        "[Are you googling PCA again?](https://lazymodellingcrew.com/post/post_17_are_you_googling_pca_again/)\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-5-Clustering%20and%20Features/imgs/beer_vodka.png' width=400px >\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9Ww0rduD39Q4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "source: [Linear Algebra for ML Part 2 | Principal Component Analysis](https://www.visual-design.net/post/linear-algebra-for-ml-part2-principal-component-analysis)"
      ],
      "metadata": {
        "id": "vsuGIBYK0l5V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PCA\n",
        "\n",
        "Let's kick things off by chatting about manifold learning. You know, today's medical imaging datasets? They're huge! Think about a single magnetic resonance brain volume. One of those images, **on a typical grid**, can have a **resolution like 255 by 255 by 180**. That's a lot, right?\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-6-PCA/imgs/film1.png' width=400px >\n",
        "\n",
        "\n",
        "back of envelope calculation:\n",
        "\n",
        "- Typical brain image size is **255 × 255 × 180**, which equals **11,704,500 voxels**. Woah!\n",
        "\n",
        "- If we're talking about **100 images** in our dataset, just imagine the size of our data matrix X. Massive!\n",
        "\n",
        "\n",
        "## Manifold learning: why do we need it?\n",
        "\n",
        "If we're counting every data point as a unique feature, that's a whopping **11 million features** for just one image! But here's the catch: getting MRI scans is pricey, so getting millions of them? Not likely. So, what we end up with is **datasets with way more features than actual examples**. This gets tricky. Why? Because as we pile on more dimensions, **our data kinda starts getting lost in all that space. This is what folks call the \"curse of dimensionality**\".\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9MfbruzWwx4u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adding more dimensions\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-6-PCA/imgs/film7.png' width=400px >\n",
        "\n",
        "So, why manifold learning? Well:\n",
        "\n",
        "- When we add more dimensions, our data **gets spread thin**.\n",
        "\n",
        "- Imagine a **grid** that keeps expanding with **every added dimension**. It's growing at an insane rate.\n",
        "\n",
        "- The more spaced out our data, the easier it might be to mistakenly find a gap or separator in the data."
      ],
      "metadata": {
        "id": "JfnVUEI5fovp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The curse of dimensionality\n",
        "\n",
        "Why is this problematic?\n",
        "\n",
        "Let's think about trying to **create a gender classifier based on stereotypes**. Let's paint a picture here. Say we give a computer details about **11 people**:\n",
        "- their height,\n",
        "- hair length,\n",
        "- shoe size, and\n",
        "- their favorite sport, book, or movie.\n",
        "\n",
        "**Our goal?** To see if it **can guess who's male or female**.\n",
        "\n",
        "If we just look at height, it's doing okay. So, if we base a **binary gender prediction** **solely on heigh**t, we might get results that seem better than random guessing. For instance, if we label gender based on whether someone is taller or shorter than 170 centimeters, we get the following results:\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-6-PCA/imgs/film9.png' width=400px >\n",
        "\n",
        "- **5 out of 6** girls are labeled correctly.\n",
        "- 3 out of 5 boys are labeled correctly.\n",
        "- In total, 8 out of 11 people are labeled correctly, which is a **72.7% accuracy**.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-6-PCA/imgs/film2.png' width=400px >\n",
        "\n",
        "\n",
        "At first glance, this might seem like a decent approach. For instance, imagine giving a computer details about 11 people, such as their height, hair length, shoe size, and favorite sport, book, or movie. Then, we ask the computer to guess who is male or female. If we only use height as the determining factor, we get the 72.7% accuracy mentioned earlier.\n",
        "\n",
        "**But what if we added more features to improve our predictions?** Let's consider hair length as an additional feature. While this might not be **as reliable**, in this sample, **girls generally have longer hair**, with a few exceptions. If we use both **height and hair length** to make our predictions, drawing a line to differentiate between the two groups, **we see that above the line are mostly girls and below the line are boys**. With this approach, we correctly classify four out of six girls and all five boys, giving us an **accuracy of 82%**. So, this method could be a potential stopping point.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-6-PCA/imgs/film3.png' width=400px >\n",
        "\n",
        "**But what if we continued to refine our model by adding more questions?** Suppose we delve deeper into stereotypes and consider **favorite film genres**. In this sample, the top group might prefer romance, the middle group leans towards comedies, and, interestingly, one woman in the lower group favors horror movies.\n",
        "\n",
        "**It seems like we've hit the jackpot with everyone correctly classified, right?** Wrong.\n",
        "\n",
        "What we've done is overfit our model to our specific dataset.\n",
        "\n",
        "Without adding new data points and continually refining our criteria, we're likely just finding patterns by pure coincidence. Let's stay on the stereotype path: imagine we added **a biker and some Goths**, who, predictably, enjoy horror movies. Then there's a hippie who loves romance and a group of girls who, unsurprisingly, like comedies. Now, when we test our refined model, its **accuracy has dropped to 61%**.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-6-PCA/imgs/film4.png' width=400px >\n",
        "\n",
        "**In conclusion, when your model has significantly more features than samples, the curse of dimensionality can lead to easy overfitting**. Overfitting, as discussed in week two, occurs when a model is **excessively parametrized**, providing it with too many degrees of freedom. This is precisely the case here. By equipping the model with an abundance of features or parameters, it becomes overly adaptable to the data's noise."
      ],
      "metadata": {
        "id": "5lm4sx5phGX8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How can Manifold learning help?\n",
        "\n",
        "So, how does manifold learning come into play? In real-world scenarios, datasets stemming from biological or physical processes often **operate within a restricted range of freedom or dimensionality.** This limitation mirrors the **physical laws governing their formation**. For instance, consider a sequence of photos stitched together to create a **panoramic view**. While each photo might have a resolution of **16 megapixels**, making it seem challenging for a machine to determine overlapping points, the act of capturing these photos is bound by **three dimensions**: the camera's **pitch, roll, and yaw**. Therefore, the data points inherently exist in a dimensionality that's notably less expansive than it appears, albeit slightly more than 3D due to factors like encoding variations and lighting differences.\n",
        "\n",
        "**NOTE:** When we talk about \"datasets stemming from biological or physical processes,\" we're referring to information that arises from natural phenomena. These could be anything from the way a plant grows, the path a planet takes around a sun, to the manner in which our heartbeats fluctuate.\n",
        "\n",
        "The **\"restricted range of freedom or dimensionality\"** bit is really about simplicity at the heart of complexity. **Even if we gather massive amounts of data about a particular phenomenon, the underlying patterns or rules governing that data might be surprisingly straightforward**.\n",
        "\n",
        "Think about it like this: imagine you're watching a swarm of birds in the sky, making intricate patterns. If you tried to track the path of **every single bird**, it'd be incredibly complex. But, if you understood a few simple rules about how birds react to their neighbors, you could predict the swarm's patterns. That’s the \"restricted range\" in action. **The countless individual bird movements (high dimensionality) can be understood with a few simple rules (low dimensionality)**.\n",
        "\n",
        "The reason this happens is because of \"physical laws governing their formation.\" In our bird example, those laws might include aerodynamics, the bird’s instinctual behavior, and so on. In other cases, it might be laws of physics, chemistry, or biology. These laws provide a framework or a set of rules that the process has to follow, which often simplifies the underlying patterns we see in the data.\n",
        "\n",
        "**In essence, even in the face of overwhelming data and complexity, nature often operates on a few basic principles, making things much more manageable and understandable than they first appear!**\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-6-PCA/imgs/film6.png' width=300px >\n",
        "\n",
        "\n",
        "Real-world data, like the countless photos you take, isn't scattered randomly. It's molded by natural rules and processes, which often restrict its complexity. So, you know how manifold learning sounds all fancy and complex? Let's break it down a bit. Imagine you're taking a bunch of photos to make one of those cool panoramic shots. Each photo is super detailed with 16 megapixels. Now, you might think, \"Gosh, how does a computer figure out where these photos overlap? Even though each photo brims with 16 million pixels, **there's an underlying pattern**. The act of taking these photos is akin to a dance with three moves: the tilt (pitch), the side-to-side sway (roll), and the spin (yaw) of the camera. These constraints mean that, despite the apparent complexity of millions of pixels, **the real challenge is understanding this 3D dance**. Consecutive photos will be super similar, making the task much more manageable. The beauty of it all? **By recognizing and exploiting this underlying structure or geometry, we can simplify tasks that might initially seem daunting**. And that's the magic behind it – finding simplicity within a sea of data."
      ],
      "metadata": {
        "id": "Bn79xrjyooR2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model of breathing\n",
        "\n",
        "Consider the model of breathing. Though it might seem like a process **involving millions of data points**, it's not quite that complex. Take, for instance, the task of **piecing together snapshots** or MRI volumes captured at varying moments in the breathing cycle. The fascinating thing is, **these innumerable data points can be boiled down to a representation in just two dimensions**. When mapped, neighboring points on this 2D plane **symbolize consecutive volumes in the time sequence**.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-6-PCA/imgs/breathing.gif' width=400px >"
      ],
      "metadata": {
        "id": "9jzoarNmwQhb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Swiss roll toy model\n",
        "\n",
        "A classic way to visualize **how data in lower dimensions can be wrapped in higher dimensions** is the Swiss roll toy model. Envision a flat 2D sheet, which is then curled up to form a 3D shape. This vividly illustrates a scenario where we know the data originates from a 2D plane but is presented in a 3D space. Interesting, right?\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-6-PCA/imgs/swiss_roll.png' width=400px >\n",
        "\n",
        "\n",
        "## Swiss Roll Model\n",
        "\n",
        "The Swiss roll model is a popular toy dataset used to illustrate the concept of manifold learning. Imagine a flat, rectangular sheet of paper. If you roll this paper into a **cylindrical shape**, you've essentially created a one-dimensional manifold in a two-dimensional space. Now, if you roll this cylinder in a spiral shape (like a rolled cake or a \"Swiss roll\"), you get a **two-dimensional manifold embedded in a three-dimensional space**.\n",
        "\n",
        "The key idea here is that **even though the Swiss roll exists in a three-dimensional space, the data on the roll itself is intrinsically two-dimensional**. If you were an ant walking on the surface of the roll, you would perceive only two dimensions, even though your path might look complex in the 3D space.\n"
      ],
      "metadata": {
        "id": "2s0RTuyHzCiX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## So, what do non-linear manifold learning techniques do?\n",
        "\n",
        "When we measure the straight-line (or Euclidean) distances between data points, it doesn't always represent the true relationships, especially on curvy surfaces or manifolds. For example, as depicted, this straight-line approach might take shortcuts across what's really a winding path on a 2D surface. So, what do non-linear manifold learning techniques do? **They aim to uncover the genuine, low-dimensional space by focusing on local neighborhoods rather than just straight-line distances.**\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-6-PCA/imgs/ecludian.png' width=500px >\n",
        "\n",
        "But before diving deep into non-linear techniques, we need to touch on linear manifold learning. You might be wondering what distinguishes the two. In simple terms, linear methods try to keep the **straight-line geometry by focusing on a direct subspace of the data**. In contrast, non-linear methods add a twist by adapting the geometry in various ways. **Their main strategy?** Pay close attention to the local surroundings of each data point. Cool, huh?"
      ],
      "metadata": {
        "id": "XrNaZzVLwZpl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Manifold\n",
        "\n",
        "In English, outside of the mathematical context, \"manifold\"  describe something as having many different forms or being varied in nature.\n",
        "Example: \"The reasons for his decision are manifold.\"\n",
        "\n",
        "\n",
        "A \"manifold\" is a mathematical concept used in various fields, such as **differential geometry and topology**. At its core, a manifold is a space that locally resembles Euclidean space (like a line, plane, or higher-dimensional analogs) but can have a more complex global structure.\n",
        "\n",
        "Certainly! Let's break down the concept of a manifold, especially in the context of differential geometry and topology.\n",
        "\n",
        "### Differential Geometry and Topology\n",
        "\n",
        "**Differential Geometry** and **Topology** are two branches of mathematics that study the properties and behaviors of \"spaces\" or \"shapes.\" While they overlap in many ways, they have different emphases:\n",
        "\n",
        "- **Differential Geometry** deals with the properties and structures of manifolds that require the concept of a derivative, like curvature. It often uses calculus to study the geometry of curves, surfaces, and their higher-dimensional analogs.\n",
        "\n",
        "- **Topology** is concerned with the properties of space that are preserved under continuous deformations, like stretching, crumpling, and bending, but not tearing or gluing. It doesn't care about exact distances or angles but instead focuses on broader properties.\n",
        "\n",
        "### Manifold in Depth\n",
        "\n",
        "When we say a manifold is a space that \"locally resembles Euclidean space,\" we're drawing a distinction between **local properties** (what things look like up close) and **global properties** (what things look like from a distance or in their entirety).\n",
        "\n",
        "1. **Locally Resembles Euclidean Space**:\n",
        "    - For any point on a manifold, if you zoom in closely enough, that tiny region will look like a flat Euclidean space.\n",
        "    - For a 1-dimensional manifold (like a curve), local regions look like straight lines.\n",
        "    - For a 2-dimensional manifold (like a surface), local regions look like flat planes.\n",
        "    - This idea extends to higher dimensions.\n",
        "\n",
        "2. **Complex Global Structure**:\n",
        "    - While the local view of a manifold might be simple, the overall shape or structure can be more complicated.\n",
        "    - Consider the Earth's surface. If you stand in a field and look around, the ground appears flat (like a 2D plane). However, globally, we know the Earth is a sphere.\n",
        "\n",
        "### Examples to Illustrate the Concept:\n",
        "\n",
        "1. **Circle**:\n",
        "    - Locally: Any small segment of a circle looks like a straight line (1D Euclidean space).\n",
        "    - Globally: The entire shape is a circle, not a straight line.\n",
        "\n",
        "2. **Surface of a Doughnut (Torus)**:\n",
        "    - Locally: If you inspect a tiny patch, it looks like a flat plane (2D Euclidean space).\n",
        "    - Globally: The entire shape is a torus, which has a hole in the middle and is more complex than a flat plane.\n",
        "\n",
        "3. **Mobius Strip**:\n",
        "    - Locally: A tiny section appears as a 2D plane.\n",
        "    - Globally: It has a unique property where if you travel along its surface, you'll end up on the opposite side without lifting your finger. This non-orientable property makes it different from regular surfaces.\n",
        "\n",
        "**Non-Examples**\n",
        "\n",
        "The Union of a Line and a Point not on the Line: This isn't a manifold because the point where the line and the point meet doesn't look like a line or a plane.\n",
        "The Union of Two Intersecting Lines: This also isn't a manifold because the point of intersection doesn't have a local structure that looks like a line or a plane.\n",
        "\n",
        "In essence, manifolds help mathematicians generalize ideas from familiar Euclidean spaces (like lines and planes) to more complex structures. By understanding things locally, we can use tools from calculus (in differential geometry) and study properties preserved under deformation (in topology) to gain insights into these more intricate spaces.\n",
        "\n",
        "**Intuitive Understanding**\n",
        "\n",
        "Imagine you're an ant walking on the surface of an object. If, from your tiny perspective, every small patch of the surface you walk on feels like a flat plane, then that surface is a manifold. Even if the overall shape of the object is curved or twisted, as long as every tiny piece of it looks and feels flat to you, it's a manifold."
      ],
      "metadata": {
        "id": "i89sj7TsVfvy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "To sum it up, are vast and multidimensional, often presenting more features than actual data points. This complexity presents a challenge. As dimensions increase, data starts to scatter and thin out across this vast space, raising the odds of accidentally drawing conclusions or overfitting the model. **Enter manifold learning techniques. These tools help by simplifying this high-dimensional data into more manageable forms, or \"embeddings**.\"\n",
        "\n",
        "There are two main types of manifold learning methods.\n",
        "\n",
        "1. First, we have linear techniques, like **principal component analysis and independent component analysis**, which we'll delve into this week.\n",
        "\n",
        "2. Then, we have **non-linear approaches, like Laplacian Eigenmaps**, which we'll explore in week six.\n",
        "\n",
        "And with that, we've wrapped up our overview of manifold learning's purpose and significance."
      ],
      "metadata": {
        "id": "U0KW_ycJ1ELj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Transformation\n",
        "\n",
        "**At the heart of grasping linear manifold learning is recognizing the significance of linear transformations.** In this notebook, we'll focus on what linear transformations mean and revisit the concepts of eigenvector decomposition and vector spaces.\n",
        "\n",
        "Now, breaking it down mathematically, a linear transformation is a special function that **upholds additivity and scalar multiplication, all while maintaining the origin.** This implies that if you take two vectors, sum them up, and then apply the transformation, the outcome would be the same as if you transformed each vector individually and then summed them up. The same rule applies for scalar multiplication, **where the sequence - doing it before or after the transformation - doesn’t change the outcome**.\n",
        "\n",
        "**But what does this mean in real-world terms?**\n",
        "\n",
        "Simply put, think of it this way: **when a matrix acts on a vector, that action is a linear transformation**. For instance, imagine we have a 2x2 matrix with values 3 and 1 on its diagonal. When this matrix acts on a vector (X, Y), it shifts that data point to a new position (X', Y'). Here, X' is computed as 3 times X (with no influence from Y), and Y' is just Y. **This essentially means that we've scaled the X direction by threefold.**\n",
        "\n",
        "\n",
        "A linear transformation, $T: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$, is a function which satisfies:\n",
        "- $T(\\mathbf{x_1} + \\mathbf{x_2}) = T(\\mathbf{x_1}) + T(\\mathbf{x_2})$ for all $\\mathbf{x_1}, \\mathbf{x_2} \\in \\mathbb{R}^n$\n",
        "- $T(\\alpha \\mathbf{x}) = \\alpha T(\\mathbf{x})$ for all $\\mathbf{x} \\in \\mathbb{R}^n$ and $\\alpha \\in \\mathbb{R}$\n",
        "- Preserves origin\n",
        "- The action of a matrix on a vector space is a linear transform.\n",
        "- i.e., consider the action of a matrix on a vector:\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "3 & 0 \\\\\n",
        "0 & 1 \\\\\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "x \\\\\n",
        "y \\\\\n",
        "\\end{bmatrix}\n",
        "=\n",
        "x \\begin{bmatrix}\n",
        "3 \\\\\n",
        "0 \\\\\n",
        "\\end{bmatrix}\n",
        "+\n",
        "y \\begin{bmatrix}\n",
        "0 \\\\\n",
        "1 \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "**Additivity and scalar multiplication are also key properties of linear transformations.**"
      ],
      "metadata": {
        "id": "DOY_eB1S-Gun"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comic time\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-6-PCA/imgs/comic.png.webp' width=500px >"
      ],
      "metadata": {
        "id": "wUY2BdZOdBuo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear transform of a regular Grid points\n",
        "\n",
        "If we possess a collection of data points forming a square, and we apply the same transformation matrix to all of them, our **square would be transformed—either stretched or scaled—into a rectangle**. In the context of a given linear transformation, eigenvectors are characterized as **the directions that stay consistent**, **only differing by a scaling factor after the transformation is applied**.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-6-PCA/imgs/linear_transform.png' width=600px >\n",
        "\n",
        "Transformations of this nature result in the rotation of data. For instance, a **rotation by 45°**. When transformed using the matrix\n",
        "$$\n",
        "R =\n",
        "\\begin{bmatrix}\n",
        "\\cos(\\theta) & -\\sin(\\theta) \\\\\n",
        "\\sin(\\theta) & \\cos(\\theta) \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "it symbolizes a rotation.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-6-PCA/imgs/rotation.png' width=600px >\n",
        "\n",
        "In the operation applied to the Mona Lisa example, the **red vector**, originally aligned with the y-axis, is altered due to the transform. Meanwhile, the **blue arrow** aligned with the x-axis remains unaffected. Here, the blue vector serves as an eigenvector. Eigenvectors in linear transforms are **non-zero vectors** that, aside from a potential scale factor, **stay unaltered after the transform is applied**. In essence, an eigenvector remains in its direction when acted upon by the linear transformation. Conversely, non-eigenvectors **see a change in direction**.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-6-PCA/imgs/monalisa.jpg' width=500px >\n",
        "\n"
      ],
      "metadata": {
        "id": "LAFXl4wDBTPK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Transforms: Eigenvectors and Values\n",
        "\n",
        "Imagine we've got a matrix $A$ and a vector $v$. Every time we multiply our vector $v$ by this matrix $A$, the vector might stretch or squish, but its core essence remains. This stretching factor is called $\\lambda$ (pronounced lambda).\n",
        "\n",
        "This process is what we refer to as eigenvectors and eigenvalues. If our vector $v$ maintains its direction (only experiencing a stretch or squish) when multiplied by matrix $A$, then $v$ is termed an eigenvector. The amount it gets stretched or squished by is the eigenvalue, $\\lambda$.\n",
        "\n",
        "Eigenvectors in linear transformations are non-zero vectors which, barring a scale factor $λ$, remain invariant after the application of the transformation. They are defined by:\n",
        "\n",
        "To find the eigenvalues, we establish the equations:\n",
        "\n",
        "To determine which vectors $v$ and values $\\lambda$ make this happen, we use a special equation. Simplified, we follow these steps:\n",
        "\n",
        "1. We want $A$ times $v$ to yield the same result as $\\lambda$ times $v$.\n",
        "\n",
        "$$ A \\cdot v = λ \\cdot v $$\n",
        "\n",
        "2. Rearranging a bit, we express this as \"(Matrix $A$ minus this stretch factor $\\lambda$ times an identity matrix) times $v$ equals zero.\"\n",
        "\n",
        "$$ A \\cdot v - λ \\cdot v = 0 $$\n",
        "$$ (A - λI) \\cdot v = 0 $$\n",
        "\n",
        "3. If $v$ isn’t zero (because that's just basic and won't give us anything new), then the determinant of the rearranged equation should be zero.So, given that $v$ is non-zero, we derive the characteristic equation.\n",
        "\n",
        "**Characteristic equation**: $$ |A - λI| = 0 $$\n",
        "\n",
        "The identity matrix is like a basic grid in our process. It's a square matrix where the diagonal from the top-left to bottom-right has all ones, and every other number is a zero. It appears as:\n",
        "\\[\n",
        "\\begin{bmatrix}\n",
        "1 & 0 \\\\\n",
        "0 & 1\n",
        "\\end{bmatrix}\n",
        "\\]\n",
        "\n",
        "By applying this equation and the principles of math, we can identify the right vectors and values for our matrix $A$!"
      ],
      "metadata": {
        "id": "5V-9KFwAFGl1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solving the characteristic Equation\n",
        "\n",
        "Here, the determinant is represented by:\n",
        "$$ \\text{det}B $$ or $$ |B| $$\n",
        "\n",
        "Estimating the determinant of a 2x2 matrix:\n",
        "$$\n",
        "B = \\begin{bmatrix}\n",
        "a & b \\\\\n",
        "c & d \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "The determinant is:\n",
        "$$ \\text{det}B = |B| = ac - bd $$\n",
        "\n",
        "For the matrix:\n",
        "$$\n",
        "A = \\begin{bmatrix}\n",
        "3 & 0 \\\\\n",
        "0 & 1 \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "The expression \\(A - λI\\) is:\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "3 & 0 \\\\\n",
        "0 & 1 \\\\\n",
        "\\end{bmatrix}\n",
        "-\n",
        "\\begin{bmatrix}\n",
        "λ & 0 \\\\\n",
        "0 & λ \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "This results in:\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "3-λ & 0 \\\\\n",
        "0 & 1-λ \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "The determinant of this matrix is:\n",
        "$$ (3-λ)(1-λ) = 0 $$\n",
        "\n",
        "Eigenvalues are therefore 1 and 3!\n"
      ],
      "metadata": {
        "id": "yMruDo5SOho2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Estimating eigenvectors\n",
        "\n",
        "For the eigenvalue \\( \\lambda = 3 \\):\n",
        "$$ Av - \\lambda v = 0 $$\n",
        "$$ (A - \\lambda I) v = 0 $$\n",
        "$$ |A - \\lambda I| = 0 $$\n",
        "\n",
        "This implies a stretch in the x-axis by a factor of 3:\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "3 & 0 \\\\\n",
        "0 & 1 \\\\\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "v_1 \\\\\n",
        "v_2 \\\\\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "3v_1 \\\\\n",
        "v_2 \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "From the equations:\n",
        "\\begin{align*}\n",
        "3v_1 - \\lambda v_1 &= 0 \\\\\n",
        "v_2 &= 0\n",
        "\\end{align*}\n",
        "\n",
        "The eigenvector corresponding to \\( \\lambda = 3 \\) is:\n",
        "$$ v = \\begin{bmatrix}\n",
        "1 \\\\\n",
        "0 \\\\\n",
        "\\end{bmatrix}\n",
        "$$"
      ],
      "metadata": {
        "id": "OEXfBDmjOtzq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Transforms: A recap\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-6-PCA/imgs/stretch.png' width=300px >\n",
        "\n",
        "Given a linear transformation with eigenvalues \\( \\lambda = 3 \\) and \\( \\lambda = 1 \\), and the corresponding eigenvectors:\n",
        "\n",
        "$$ v_1 = \\begin{bmatrix}\n",
        "1 \\\\\n",
        "0 \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "and\n",
        "$$ v_2 = \\begin{bmatrix}\n",
        "0 \\\\\n",
        "1 \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "This transformation signifies a stretch in the x-direction, represented by the matrix:\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "3 & 0 \\\\\n",
        "0 & 1 \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n"
      ],
      "metadata": {
        "id": "aZycKbYxO5hN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Eigenvector decomposition (with off diagonals)\n",
        "\n",
        "To find the eigenvalues and eigenvectors for the matrix\n",
        "$$ A = \\begin{bmatrix}\n",
        "2 & 1 \\\\\n",
        "1 & 2 \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "First, let's determine the eigenvalues:\n",
        "\n",
        "Set the determinant of $ A - \\lambda I $ to zero:\n",
        "$$ \\begin{vmatrix}\n",
        "2 - \\lambda & 1 \\\\\n",
        "1 & 2 - \\lambda \\\\\n",
        "\\end{vmatrix} = 0\n",
        "$$\n",
        "\n",
        "Expanding the determinant:\n",
        "$$ (2 - \\lambda)(2 - \\lambda) - 1 = 0 $$\n",
        "$$ 4 - 4\\lambda + \\lambda^2 - 1 = 0 $$\n",
        "$$ \\lambda^2 - 4\\lambda + 3 = 0 $$\n",
        "\n",
        "Factoring:\n",
        "$$ (\\lambda - 3)(\\lambda - 1) = 0 $$\n",
        "From which we find the eigenvalues $ \\lambda = 3 $ and $ \\lambda = 1 $.\n",
        "\n",
        "Next, we'd calculate the eigenvectors for each eigenvalue, but for this explanation, I've focused on the eigenvalues. The approach is to plug the eigenvalues into the matrix equation and solve for the vector.\n",
        "\n",
        "**NOTE:** To compute the determinant (often denoted as $ \\text{det}B $ or $ |B| $) of a 2x2 matrix, you can use a simple formula. For the matrix\n",
        "$$ B = \\begin{bmatrix}\n",
        "a & b \\\\\n",
        "c & d \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "the determinant is given by:\n",
        "$$ \\text{det}B = |B| = ad - bc $$\n",
        "\n",
        "So, based on the provided matrix elements \\( a, b, c, \\) and \\( d \\), the determinant is the product of the diagonal elements \\( a \\) and \\( d \\) minus the product of the off-diagonal elements \\( b \\) and \\( c \\)."
      ],
      "metadata": {
        "id": "JhOrNjBaNqRZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To determine the eigenvectors and eigenvalues for the matrix\n",
        "$$ A = \\begin{bmatrix}\n",
        "2 & 1 \\\\\n",
        "1 & 2 \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "**Eigenvectors and Eigenvalues:**\n",
        "\n",
        "For $ \\lambda = 3 $:\n",
        "Using the equation $ (A - \\lambda I)v = 0 $, we get:\n",
        "$$ \\begin{bmatrix}\n",
        "2-3 & 1 \\\\\n",
        "1 & 2-3 \\\\\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "v_1 \\\\\n",
        "v_2 \\\\\n",
        "\\end{bmatrix}\n",
        "= 0\n",
        "$$\n",
        "\n",
        "This results in the system:\n",
        "- $ -v_1 + v_2 = 0 $ implying $ v_1 = v_2 $\n",
        "\n",
        "Thus, one eigenvector for $ \\lambda = 3 $ is:\n",
        "$$ v = \\begin{bmatrix}\n",
        "1 \\\\\n",
        "1 \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "For $ \\lambda = 1 $:\n",
        "Using the same equation, we have:\n",
        "$$ \\begin{bmatrix}\n",
        "2-1 & 1 \\\\\n",
        "1 & 2-1 \\\\\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "v_1 \\\\\n",
        "v_2 \\\\\n",
        "\\end{bmatrix}\n",
        "= 0\n",
        "$$\n",
        "\n",
        "From this system, we derive:\n",
        "From the given system:\n",
        "$$ -v_1 + v_2 = 0 $$\n",
        "we can deduce that:\n",
        "$$ v_1 = v_2 $$\n",
        "\n",
        "Hence, the eigenvector for $ \\lambda = 1 $ is:\n",
        "$$ v = \\begin{bmatrix}\n",
        "1 \\\\\n",
        "-1 \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "In summary, the matrix $ A $ has eigenvalues $ \\lambda = 3 $ and $ \\lambda = 1 $ with corresponding eigenvectors $ \\begin{bmatrix}\n",
        "1 \\\\\n",
        "1 \\\\\n",
        "\\end{bmatrix} $ and $ \\begin{bmatrix}\n",
        "1 \\\\\n",
        "-1 \\\\\n",
        "\\end{bmatrix} $ respectively."
      ],
      "metadata": {
        "id": "ssKrN5SYO7vf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Transformation: stretch on diagonal\n",
        "\n",
        "**These two matrices represent the same transformation but on different coordinate bases.** The axes of these bases are determined by the eigenvectors of the transformation. When we project our transformation matrix onto this new basis, it clarifies the transformation, changing it from a **diagonal stretch to separate stretches in X and Y directions**.\n",
        "\n",
        "Moreover, we can also represent our data in this new basis. By projecting data onto the eigenvectors of the transformation, it's essentially rotating the coordinate bases. **So both the standard basis (x, y) and the rotated basis (x', y') serve as valid coordinate systems for this 2D space. It's just a matter of choosing the one that provides a clearer perspective on the transformation.**\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-6-PCA/imgs/stretch2.png' width=600px >\n",
        "\n",
        "\n",
        "**Through the calculation of eigenvectors and eigenvalues, we can represent the transformation using a new basis and diagonalize it.** This means the same operation is performed, but along a different axis.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-6-PCA/imgs/stretch3.png' width=600px >\n",
        "\n",
        "\n",
        "**Analogy**\n",
        "\n",
        "Imagine you have two maps of the same city, but one is rotated a bit. Even though they look different, they're showing the same streets, buildings, and parks. Similarly, these two matrices are like those two maps; **they're describing the same transformation but from different viewpoints or \"angles\"**.\n",
        "\n",
        "Think of these viewpoints as being set by the **transformation's eigenvectors**. When we overlay our transformation onto this new \"map\", we can see it in a clear, straightforward manner. **Instead of just stretching things diagonally, we see a stretch horizontally (X) and vertically (Y).**\n",
        "\n",
        "Now, let's say you have some data, like the locations of your favorite spots in the city. You can choose to plot them on either map. By placing this data onto our transformation's eigenvectors, it's like rotating your list of favorite places to fit the rotated map.\n",
        "\n",
        "To put it simply, both the classic grid of streets (x, y) and the slightly tilted one (x', y') are great ways to navigate the city, or in our case, this 2D space. **Just pick the one that helps you see things clearer!**\n",
        "\n",
        "Let's wrap up with a key takeaway as we dive into PCA: **PCA loves unit eigenvectors!** That means when you're giving PCA eigenvectors, make sure to spruce them up by **normalizing them**. Wondering how? Just divide by their magnitude using good ol' Pythagoras's theorem.\n",
        "\n",
        "**In a nutshell:**\n",
        "\n",
        "- A matrix's action on a vector space is a linear transformation.\n",
        "\n",
        "- Eigenvectors are unique in that they only undergo a scale change (not direction change) during this transformation, making them an ideal basis to simplify our understanding of the transformation.\n",
        "\n",
        "- To identify these eigenvectors and their associated eigenvalues, we solve the characteristic equation. For 2x2 matrices, it's essential to be familiar with solving this manually.\n",
        "\n",
        "- Remember, if someone asks for PCA eigenvectors, give them the polished, normalized version. So always remember, PCA prefers unit eigenvectors. If you're providing eigenvectors for PCA, ensure they're normalized.\n",
        "\n",
        "- Data can be viewed differently by projecting it onto these eigenvectors. This process involves aligning your data with the eigenvectors, offering a new perspective. You can also look at your data through the lens of these eigenvectors. It's like giving your data a new perspective by aligning it with these special vectors.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4xURuMv4QvQM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vector Space\n",
        "\n",
        "Eigenvectors provide a special insight into vector spaces. Imagine you have a system, represented by a matrix, that transforms vectors in a certain way, like stretching, rotating, or compressing them. Some vectors, when subjected to this transformation, change direction, while others only get scaled (stretched or shrunk). These special vectors that only get scaled are what we call eigenvectors.\n",
        "\n",
        "The scalar by which an eigenvector is stretched or shrunk is called the eigenvalue associated with that eigenvector. What's powerful about eigenvectors is that they give us a new way to view our system. Instead of using the standard coordinate system (like x and y axes on a plane), we can use the eigenvectors as our new axes. This new system is what we're referring to as an **equally valid** coordinate basis $ U $.\n",
        "\n",
        "Now,** when we talk about representing our data in this basis, it's a bit like changing the perspective or viewpoint from which we're looking at our data.** This is often done to simplify or gain a clearer understanding of complex systems. Projection in this context refers to the mathematical operation that transforms our data from its representation in the standard basis to its representation in the eigenvector basis.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-6-PCA/imgs/vector_space.png' width=400px >\n",
        "\n",
        "The formula $ Y = U^TX $ captures this idea. Here,\n",
        "\n",
        "- $ X $ is our original data,\n",
        "\n",
        "- $ U^T $ represents the transpose of the matrix whose columns are the eigenvectors, and\n",
        "\n",
        "- $ Y $ is our data represented in the new eigenvector basis.\n",
        "\n",
        "This transformation can be particularly useful in various applications, especially in reducing the dimensionality of data in fields like machine learning or making certain mathematical operations more tractable.\n",
        "\n"
      ],
      "metadata": {
        "id": "acY_UQzcLAd5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalizing Eigenvectors for PCA\n",
        "\n",
        "- In PCA (Principal Component Analysis), it's crucial to work with eigenvectors that have been normalized.\n",
        "  \n",
        "- Let's consider the matrix:\n",
        "  $$ A = \\begin{bmatrix}\n",
        "  2 & 1 \\\\\n",
        "  1 & 2 \\\\\n",
        "  \\end{bmatrix} $$\n",
        "\n",
        "- This matrix has eigenvalues:\n",
        "  $$ \\lambda_1 = 3 \\text{ and } \\lambda_2 = 1 $$\n",
        "\n",
        "- The corresponding eigenvectors for these eigenvalues are:\n",
        "  $$ v_1 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} \\text{ and } v_2 = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix} $$\n",
        "\n",
        "- Before we proceed with PCA, we need these eigenvectors to be of unit length, meaning their magnitudes should be 1.\n",
        "\n",
        "- To normalize an eigenvector, we'll divide each component by the vector's magnitude.\n",
        "\n",
        "- The magnitude (or length) of a vector can be found using Pythagoras' theorem.\n",
        "\n",
        "  For $ v_1 $:\n",
        "  $$ ||v_1|| = \\sqrt{1^2 + 1^2} = \\sqrt{2} $$\n",
        "\n",
        "  For $ v_2 $:\n",
        "  $$ ||v_2|| = \\sqrt{1^2 + (-1)^2} = \\sqrt{2} $$\n",
        "\n",
        "- So, the normalized eigenvectors become:\n",
        "  $$ v_1' = \\begin{bmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\end{bmatrix} \\text{ and } v_2' = \\begin{bmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{-1}{\\sqrt{2}} \\end{bmatrix} $$\n",
        "\n",
        "Now, with these normalized eigenvectors, we're set to apply PCA effectively."
      ],
      "metadata": {
        "id": "warbg-ArORnd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recap:\n",
        "\n",
        "- Matrices operate on vector spaces through linear transformations.\n",
        "\n",
        "- When applying these linear transformations, certain vectors, known as eigenvectors, retain their original direction but may be scaled.\n",
        "\n",
        "- These eigenvectors provide an alternate basis where the transformation appears as a diagonal matrix.\n",
        "\n",
        "- To determine the eigenvectors and their associated eigenvalues, we solve the equation: \\( |A - \\lambda I| = 0 \\).\n",
        "\n",
        "- In the realm of PCA, it's crucial to ensure that eigenvectors are of unit length or normalized.\n",
        "\n",
        "- Data can be depicted in this eigenvector basis using the transformation: \\( Y = U^T X \\)."
      ],
      "metadata": {
        "id": "EYDBJd8MO28w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ΙΙΙ Principal Component Analysis (PCA)\n",
        "(for notes open 5.1.Linear_Manifold_Learning_PCA.ipynb)"
      ],
      "metadata": {
        "id": "qd56ObIZQOnM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Principal Component Analysis (PCA)\n",
        "\n",
        "**Objective:** Let's imagine we're trying to find the best angle to view a spread-out collection of data points. What PCA does is it rotates our perspective to see the data in a way where its spread or variation is maximized. O in other words, PCA's goal is to reorient our data to better understand its variation. It helps us look at the data in the most informative way.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-6-PCA/imgs/u1.png' width=400px >\n",
        "\n",
        "- Here's a fun fact: If we measure things like GA at scan and brain volume, they might be closely related, showing similar patterns.\n",
        "\n",
        "- But there's a question: Could our data matrix be repeating some information? It's like recording the same song multiple times on a playlist.\n",
        "\n",
        "- An exciting part of PCA: It helps us find the most representative direction (let's call it u1) that captures the core trends in our data.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6GYL_ouNQQN5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Essential Idea Behind PCA**\n",
        "\n",
        "**Key Assumption:**\n",
        "\n",
        "- Data is linearly associated ( correlated). So, our data has some kind of straight-line relationship, meaning things move together in a predictable way.\n",
        "\n",
        "**Goal:**\n",
        "\n",
        "- To transform a number of (possibly) correlated features/variables onto a (smaller)\n",
        "number of uncorrelated variables called principal components. Imagine having a jigsaw puzzle with many pieces that look very alike. Our goal with PCA is to represent this puzzle with fewer, unique pieces. These unique pieces are what we call principal components.\n",
        "\n",
        "- This is achieved by rotating data into a new basis (the principle components) which\n",
        "better represent the variance in the data. In simpler terms, we're trying to reduce redundant or overlapping information, and instead focus on the big, distinct patterns.\n",
        "\n",
        "**So, How Do We Do This?**\n",
        "\n",
        "Bases 𝑼 (principle components) are estimated by eigen-decomposition of the data\n",
        "covariance matrix\n",
        "\n",
        "- Think of it as tuning a radio. We're adjusting our data to the clearest channel (the principal components) where we can understand the most with the least noise.\n",
        "\n",
        "- Technically, we find these \"clear channels\" or bases (denoted by 𝑼) by breaking down the data's covariance matrix—a fancy term that describes how different data points interact with each other."
      ],
      "metadata": {
        "id": "JRzEhwcTRz2c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variance:\n",
        "\n",
        "Variance is a measure that indicates how far a set of numbers are from their average value.\n",
        "\n",
        "For a vector $X$, the variance $\\text{cov}_{XX}$ is computed using the formula:\n",
        "$$ \\text{cov}_{XX} = \\frac{1}{N-1} \\sum_{n=1}^{N} (X_n - \\bar{x})^T (X_n - \\bar{x}) $$\n",
        "\n",
        "This equation takes every element $X_n$ in the vector $X$, subtracts the mean value $\\bar{x}$ of the vector, and squares the result. The summation sums up these squared differences for all elements of the vector. Dividing by $N-1$ (where $N$ is the number of elements) gives an average of these squared differences, which is the variance.\n",
        "\n",
        "### Covariance:\n",
        "\n",
        "Covariance measures the extent to which two variables change together. If the variables tend to increase and decrease together, the covariance is positive. If one variable tends to increase when the other decreases, the covariance is negative.\n",
        "\n",
        "The covariance between vectors $X$ and $Y$ is given by:\n",
        "$$ \\text{cov}_{XY} = \\frac{1}{N-1} \\sum_{n=1}^{N} (X_n - \\bar{x})^T (Y_n - \\bar{y}) $$\n",
        "\n",
        "This formula works similarly to the variance formula but looks at the product of the differences from the mean for both $X$ and $Y$.\n",
        "\n",
        "### Mean:\n",
        "\n",
        "The mean (or average) of a vector is the sum of its elements divided by the number of elements. For vector $X$, the mean $\\bar{x}$ is:\n",
        "$$ \\bar{x} = \\frac{1}{N} \\sum_{n=1}^{N} X_n $$\n",
        "\n",
        "### Covariance Matrix:\n",
        "\n",
        "For two vectors, $X$ and $Y$, the covariance matrix $S$ is a $2 \\times 2$ matrix that captures the variance and covariance between them:\n",
        "$$\n",
        "S = \\begin{bmatrix}\n",
        "\\text{cov}_{XX} & \\text{cov}_{XY} \\\\\n",
        "\\text{cov}_{YX} & \\text{cov}_{YY}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "- The element $\\text{cov}_{XX}$ is the variance of $X$.\n",
        "- The element $\\text{cov}_{YY}$ is the variance of $Y$.\n",
        "- The element $\\text{cov}_{XY}$ is the covariance between $X$ and $Y$.\n",
        "- The element $\\text{cov}_{YX}$ is also the covariance between $X$ and $Y$ (since covariance is symmetric, $\\text{cov}_{XY} = \\text{cov}_{YX}$).\n",
        "\n",
        "In essence, the diagonal of the covariance matrix represents the variances of the vectors, while the off-diagonal elements represent their covariance."
      ],
      "metadata": {
        "id": "hV-KI72jU3aR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Principal Component Analysis\n",
        "\n",
        "Principal Component Analysis aims to project our data onto a new basis vector $u_1$.\n",
        "\n",
        "- The goal is to have the orthogonal projection of data points (represented in blue) onto this subspace (depicted by the black line) maximize the variance of the projected points (shown in red).\n",
        "\n",
        "#### Projecting $x$ onto $u$\n",
        "\n",
        "For the projection of a point $x$ onto a vector $u$, the dot product is used:\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-6-PCA/imgs/cos.png' width=300px >\n",
        "\n",
        "\n",
        "**Dot product**:\n",
        "$$ u^T x_n = |u_1| |cos \\theta| $$\n",
        "Where:\n",
        "- $u^T x_n$ is the dot product of vectors $u$ and $x_n$.\n",
        "- $|u_1|$ represents the magnitude (or norm) of the vector $u_1$.\n",
        "- $|cos \\theta|$ indicates the cosine of the angle between $u$ and $x_n$.\n",
        "\n",
        "The relationship between the dot product and the magnitudes of the vectors with the angle between them can be further described as:\n",
        "$$ |x_n| cos \\theta = \\frac{x_n \\cdot u_1}{|u_1|} $$\n",
        "\n",
        "If the norm of the vector $u_1$ is 1 (i.e., it's a unit vector), then the dot product simplifies to:\n",
        "$$ u^T x_n = u^T x_n \\quad \\text{(if } |u_1| = 1 \\text{)} $$\n",
        "\n",
        "\n",
        "Additionally, the projection of $x$ in terms of its components $x_1n$ and $x_2n$ onto $u_1$which has components $u_11$ and $u_12$ is:\n",
        "\n",
        "$$ = [u_11, u_12] \\begin{bmatrix} x_1n \\\\ x_2n \\end{bmatrix} = u_11 x_1n + u_12 x_2n $$\n"
      ],
      "metadata": {
        "id": "6eRZTxd5WAfT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Projecting $x$ onto $u$\n",
        "\n",
        "When projecting a point $x$ onto a vector $u$, the dot product is an essential mathematical tool:\n",
        "\n",
        "**Dot product**:\n",
        "\n",
        "The dot product between the vectors $u_1$ and $x_n$ can be represented as:\n",
        "$$ u^T x_n = |u_1| |x_n| \\cos \\theta $$\n",
        "Where:\n",
        "- $u^T x_n$ is the dot product of vectors $u$ and $x_n$.\n",
        "- $|u_1|$ represents the magnitude (or norm) of the vector $u_1$.\n",
        "- $\\cos \\theta$ indicates the cosine of the angle between $u$ and $x_n$.\n",
        "\n",
        "This relationship can be further described as:\n",
        "$$ |x_n| \\cos \\theta = \\frac{x_n \\cdot u_1}{|u_1|} $$\n",
        "\n",
        "If the vector $u_1$ is a unit vector (i.e., its norm is 1), then the dot product simplifies to:\n",
        "$$ u^T x_n = u^T x_n \\quad \\text{(if } |u_1| = 1 \\text{)} $$\n",
        "\n",
        "Additionally, the projection of $x$ in terms of its components $x_1n$ and $x_2n$ onto $u_1$ which has components $u_11$ and $u_12$ is:\n",
        "$$ = [u_11, u_12] \\begin{bmatrix} x_1n \\\\ x_2n \\end{bmatrix} = u_11 x_1n + u_12 x_2n $$\n"
      ],
      "metadata": {
        "id": "JVpLkvAYawgc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Derivation of Principal Component Analysis (PCA)\n",
        "\n",
        "**Objective of PCA:**\n",
        "- The goal of PCA is to find a new orthogonal basis $U$, represented by vectors $u_1, u_2, ... , u_n$, onto which we can project the data such that the variance of this projected data is maximized.\n",
        "\n",
        "**Defining the First Principal Component:**\n",
        "- The first principal component, $u_1$, can be understood as the direction in the data space which, when data points are projected onto it, maximizes the variance of those projected points. This means that $u_1$ captures the most significant pattern or variance in the data.\n",
        "\n",
        "**Data Covariance Matrix (S):**\n",
        "- The covariance matrix, $S$, provides a measure of the degree to which each pair of features in the dataset vary together. The covariance matrix is given by:\n",
        "$$ S = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x})(x_i - \\bar{x})^T $$\n",
        "Where:\n",
        "    - $x_i$ is the data vector for the i-th sample.\n",
        "    - $\\bar{x}$ is the mean of the data vectors, calculated as:\n",
        "$$ \\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i $$\n",
        "    - $n$ is the number of data samples.\n",
        "\n",
        "**Variance of Projected Data onto $u_1$:**\n",
        "- After projecting the data onto the first principal component $u_1$, the variance is given by:\n",
        "$$ \\frac{1}{n} \\sum_{i=1}^{n} |u_1^T (x_i - \\bar{x})|^2 = u_1^T S u_1 $$\n",
        "This equation gives the variance of the data when it's projected onto the vector $u_1$.\n",
        "\n",
        "**Dimensionality of Vectors:**\n",
        "- The vectors $x_i$, which represent the individual data samples, as well as the mean vector $\\bar{x}$, are $m$-dimensional vectors.\n",
        "    - Here, $m$ denotes the number of features or attributes in the dataset.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### Breaking Down Principal Component Analysis (PCA)\n",
        "\n",
        "**What's the Big Idea of PCA?**\n",
        "- Imagine you have a ton of data points scattered around, and you want to find the best \"line\" or \"direction\" that goes right through the heart of that cloud of points. This \"line\" is the place where most of our data's action happens. PCA helps us find these \"lines\", with the first one being the most important!\n",
        "\n",
        "**What's This First \"Line\" All About?**\n",
        "- This \"line\" is called the first principal component. Think of it as the main highway of our data. If our data was traffic, then this highway would have the busiest traffic.\n",
        "\n",
        "**Talking About Data Variance with a Fun Analogy:**\n",
        "- Suppose we're taking photos of a group of people from different angles. Some photos capture more faces than others. In PCA, we're trying to find that angle (or snapshot) where we can see the most action! The \"action\" here is variance in our data.\n",
        "\n",
        "**What's the Deal with the Covariance Matrix (S)?**\n",
        "- Think of this as a tool that tells us how different features in our data move with respect to each other. For example, if we were studying how often people eat ice cream and how hot it is outside, the covariance matrix would help us see if people really eat more ice cream when it's hotter.\n",
        "    - Math fun: $S$ is like a camera calculating the best angle to capture the most faces.\n",
        "    - Quick note: A feature is just a specific piece of data we're studying, like \"temperature\" or \"number of ice creams sold\".\n",
        "\n",
        "**What Happens When We Project Data?**\n",
        "- It's like casting shadows! If you have a flashlight shining on an object, the shadow it casts on the wall is a \"projection\". In PCA, when we project our data onto our \"main highway\" ($u_1$), we're seeing the shadow of our data on that line.\n",
        "    - Math magic: The length and position of that shadow tell us how varied or spread out our data is along our \"main highway\".\n",
        "\n",
        "**What's the Deal with $m$?**\n",
        "- $m$ is just geek speak for the number of features we're looking at. Like in our ice cream example, we'd have two features: \"temperature\" and \"number of ice creams sold\".\n",
        "\n"
      ],
      "metadata": {
        "id": "rtrMyoZ-eUbz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How Do We Determine the Vector $u_1$?\n",
        "\n",
        "**Goal:** We aim to identify $u_1$, essentially pinpointing the optimal direction or \"line\" in our data where things are most spread out. So, we're trying to find the best direction, represented by the vector $u_1$, where our data varies the most.\n",
        "\n",
        "**Steps to Achieve This:**\n",
        "\n",
        "1. **Maximize the Projected Variance:** Think of projection as casting shadows of our data onto a line. We want to pick a line (or direction) such that the shadows are as dispersed as possible. In mathematical terms, this means we're aiming to optimize the formula $u^T S u_1$, where $S$ is the matrix that tells us how our data points vary relative to each other.\n",
        "\n",
        "2. **Finding the Best Direction:** We can identify the best direction for $u_1$ by employing calculus. By taking the derivative with respect to $u_1$, we can find where our function achieves its maximum value, giving us the most dispersed shadows. So, we want our data to be spread out as much as possible when it's projected onto this direction. In fancy terms, we're maximizing $u^T S u_1$, where $S$ is like a snapshot of how our data moves around.\n",
        "\n",
        "3. **Restraining our Vector (Setting Some Ground Rules):** Without any limitations, $u_1$ could grow infinitely large. To prevent this, we ensure its magnitude is constrained such that $u^T u_1 = 1$. So, there's a catch. Without rules, $u_1$ could become ridiculously big. So, we have a rule that says $u^T u_1$ has to equal 1, keeping things in check.\n",
        "\n",
        "4. **The Role of the Lagrange Multiplier (Introducing a Helper, $\\lambda$):** This is a mathematical method used to solve problems that have constraints. We bring in a new variable, $\\lambda$, to enforce the constraint from the previous step. Our equation then becomes:\n",
        "$$ u^T S u_1 + \\lambda(1 - u^T u_1) $$. So, to make sure we stick to the above rule, we use a little trick called the Lagrange multiplier. Our equation now looks like this:\n",
        "$$ u^T S u_1 + \\lambda(1 - u^T u_1) $$\n",
        "\n",
        "5. **Optimization Time:** We take the derivative of our latest formula with respect to $u_1$. This gives us the equation:\n",
        "$$ S u_1 = \\lambda u_1 $$\n",
        "\n",
        "6. **Linking to Eigenvectors:** The above equation indicates that $u_1$ is an eigenvector of our covariance matrix $S$. Eigenvectors are special directions tied to matrices.\n",
        "\n",
        "7. **Eigenvector's Significance (Why Should We Care?):** The first eigenvector is the direction of maximum variance in our data. Additionally, the corresponding eigenvalue indicates the magnitude of this variance. In other words, the direction of $u_1$, our first eigenvector, is where our data has the most spread or variance. And the number $\\lambda$ tells us just how much spread there is.\n",
        "\n",
        "With this setup, we've pinpointed the direction in our data where the variance is highest, a core principle of PCA! And there we have it! The direction in our data with the most action happening, all thanks to a bit of math."
      ],
      "metadata": {
        "id": "4eXeOm_zfo0g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extending PCA to Higher Dimensions\n",
        "\n",
        "**The Basic Idea:**\n",
        "PCA isn't just about finding one special direction ($u_1$) where data varies most; it can find multiple such directions. Let's explore how to find the next best direction, $u_2$.\n",
        "\n",
        "**Ground Rules:**\n",
        "\n",
        "1. **Orthogonality is Key:** A major assumption in PCA is that the directions we find, like $u_1$ and $u_2$, are perpendicular to each other. This ensures that each direction captures unique patterns.\n",
        "\n",
        "2. **Maximizing the Spread Again:** We want our data to have a good spread when projected onto $u_2$. To quantify this, we look at the expression $u^T S u_2$.\n",
        "\n",
        "3. **Adding Some Constraints:** We make sure that the length of $u_2$ is normalized, meaning $u^T u_2 = 1$. Also, $u_2$ and $u_1$ shouldn't have any correlation or overlap in capturing patterns. This is ensured by making them uncorrelated:\n",
        "$$ cov(u_1, u_2) = u^T S u_1 = u^T \\lambda_1 u_1 $$\n",
        "\n",
        "4. **Lagrangian Time:** To keep all these rules in check, we introduce a Lagrange function:\n",
        "$$ u^T S u_2 + \\lambda (1 - u^T u_2) - \\phi u^T u_1 $$\n",
        "\n",
        "5. **Doing Some Calculus:** Taking the derivative with respect to $u_2$, we get:\n",
        "$$ S u_2 - \\lambda u_2 - \\phi u_1 = 0 $$\n",
        "\n",
        "6. **Some Simplifications:** Multiplying the above equation from the left by $u_1$ and simplifying, we notice that:\n",
        "$$ \\phi = 0 $$\n",
        "And so, we are left with:\n",
        "$$ S u_2 = \\lambda_2 u_2 $$\n",
        "\n",
        "\n",
        "This means our $u_2$ is yet another special direction (like $u_1$) but captures the second most variance in the data."
      ],
      "metadata": {
        "id": "afVXz8ySkDHS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How to estimate PCA?\n",
        "\n",
        "Principal Component Analysis (PCA) is a dimensionality reduction method that's used to emphasize variation and bring out strong patterns in a dataset. There are several ways to compute PCA, but I'll explain two common methods: the covariance method (Method 1) and the singular value decomposition (SVD) method (Method 2).\n",
        "\n",
        "---\n",
        "\n",
        "### Method 1: Covariance Method\n",
        "\n",
        "1. **Standardize the Data:** Before applying PCA, it's common to standardize the dataset, ensuring each feature has a mean of 0 and a standard deviation of 1.\n",
        "\n",
        "2. **Compute the Covariance Matrix:** The covariance matrix captures the mutual variances among different features.\n",
        "\n",
        "$$ S = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x})(x_i - \\bar{x})^T $$\n",
        "\n",
        "3. **Calculate Eigenvalues and Eigenvectors:** Diagonalize the covariance matrix to find its eigenvalues and eigenvectors. The eigenvectors represent the directions of maximum variance (principal components), and the eigenvalues indicate the magnitude of this variance in the respective directions.\n",
        "\n",
        "4. **Sort Eigenpairs:** Sort eigenvalues in decreasing order and rank their corresponding eigenvectors.\n",
        "\n",
        "5. **Choose Principal Components:** Retain as many components as you need to capture the desired variance. The first eigenvector corresponds to the highest eigenvalue and represents the direction in which the data has the highest variance. Subsequent eigenvectors capture decreasing amounts of variance.\n",
        "\n",
        "6. **Project Data:** Transform the original data onto the new basis (the selected eigenvectors).\n",
        "\n",
        "---\n",
        "\n",
        "### Method 2: Singular Value Decomposition (SVD)\n",
        "\n",
        "1. **Standardize the Data:** Just like in Method 1, ensure each feature has mean 0 and standard deviation 1.\n",
        "\n",
        "2. **Compute the Singular Value Decomposition:** Use SVD on the data matrix $X$:\n",
        "\n",
        "$$ X = U \\Sigma V^T $$\n",
        "\n",
        "Here,\n",
        "- $U$ contains the eigenvectors of $XX^T$ (these represent the principal components of $X$).\n",
        "- $\\Sigma$ is a diagonal matrix containing the square roots of the eigenvalues of $XX^T$ (these are the singular values of $X$).\n",
        "- $V^T$ has the eigenvectors of $X^TX$.\n",
        "\n",
        "3. **Principal Components:** The columns of $U$ are the principal components of $X$.\n",
        "\n",
        "4. **Variance Explanation:** The square of the singular values (divided by the number of observations) give the variance explained by each principal component.\n",
        "\n",
        "5. **Choose Principal Components and Project Data:** Similar to steps 5 and 6 in Method 1.\n",
        "\n",
        "\n",
        "Both methods will give you the same principal components, but SVD is often preferred for numerical stability. The choice between them depends on the specific application and computational considerations.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### PCA Made Simple: Method 1 (The Covariance Approach)\n",
        "\n",
        "**Step-by-Step Guide:**\n",
        "\n",
        "1. **Prepping the Data:** First, make sure all your data features (like height, weight, age, etc.) are on the same scale. It's like converting everything to a common unit, say, inches or centimeters.\n",
        "\n",
        "2. **Find the Relationship:** Imagine all your data points are stars in the night sky. The covariance matrix is like connecting these stars to find constellations. It tells us how one feature relates to another.\n",
        "\n",
        "3. **Spotting the Patterns:** By calculating something called eigenvalues and eigenvectors from this matrix, we find the major patterns in our data. Think of eigenvectors as the main directions where our data varies the most.\n",
        "\n",
        "4. **Rank 'Em Up:** Sort these patterns based on their importance. The most important pattern comes first, the second most important next, and so on.\n",
        "\n",
        "5. **Pick the Best Patterns:** Decide how many of the top patterns (or directions) you want to keep. This helps in simplifying your data without losing too much information.\n",
        "\n",
        "6. **Transform!** Now, change your data to fit these new important patterns. It's like changing your viewpoint to get the best view of the stars!\n",
        "\n",
        "\n",
        "\n",
        "### PCA Made Simple: Method 2 (The Magic of SVD)\n",
        "\n",
        "**Step-by-Step Guide:**\n",
        "\n",
        "1. **Ready, Set, Scale:** Just like in Method 1, make sure all your data is standardized.\n",
        "\n",
        "2. **Magic Trick - SVD:** Now, perform a magical trick on your data called Singular Value Decomposition (SVD). It's a fancy way to break down your data into three parts. These parts help us see the main patterns just like in the covariance approach.\n",
        "\n",
        "3. **Discovering the Main Actors:** The 'U' from the SVD shows the main patterns in your data. Imagine these as the leading roles in a movie.\n",
        "\n",
        "4. **How Important Are They?** The singular values (from the Σ of SVD) tell us the importance of each pattern. It's like movie reviews telling us which actor did the best job.\n",
        "\n",
        "5. **Pick the Stars and Watch the Movie:** Decide which leading roles (patterns) you want to focus on, and transform your data to match this new movie script!\n",
        "\n",
        "Think of PCA as a movie director trying to capture the best scenes with a limited number of cameras. These methods help in deciding where to place those cameras to capture the essence of the entire movie!"
      ],
      "metadata": {
        "id": "Hpmpaaj5kI2f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing PCA using the Covariance Approach with Numpy\n",
        "\n",
        "1. **Centering the Data Matrix**: First, you'll want to adjust the data matrix, $X$, so it's centered around zero. This means subtracting the mean vector $w_X$ (which is a column vector in ℝ^m) from every data point in $X$.\n",
        "   - To find the mean of your data, you can use the `numpy.mean` function. Just remember to choose the correct axis.\n",
        "\n",
        "2. **Compute the Covariance Matrix**: Once your data is centered, calculate the covariance matrix, $S$, using the formula:\n",
        "$$ S = \\frac{1}{n} XX^f $$\n",
        "Here, $n$ represents the number of data points.\n",
        "\n",
        "3. **Eigenvectors and Eigenvalues**: This is the meat of PCA. These values will help you understand the main patterns in your data.\n",
        "   - You'll be solving for the equation:\n",
        "$$ SU = \\Phi U $$\n",
        "Where $U$ (the matrix of eigenvectors) and $\\Phi$ (the diagonal matrix of eigenvalues) are both in ℝ^m×m.\n",
        "   - To get these values, use the `numpy.linalg.eig(S)` function. This will give you unit eigenvectors.\n",
        "   - Make sure to order the eigenvectors based on their eigenvalues, from largest to smallest. The largest eigenvalues point to where the data has the most variance.\n",
        "\n",
        "For a deeper access to the numpy function, you can check out: [numpy.linalg.eig documentation](https://numpy.org/doc/stable/reference/generated/numpy.linalg.eig.html).\n",
        "\n",
        "\n",
        "Again, the ordering is key. By prioritizing eigenvectors associated with the largest eigenvalues, you're ensuring you capture the main patterns or variances in your data first."
      ],
      "metadata": {
        "id": "nGrF7dEAm7gW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing PCA using Singular Value Decomposition (Method 2) with Numpy\n",
        "\n",
        "1. **Centering the Data Matrix**: You'll start by adjusting the data matrix. Update your data matrix $X$ by subtracting the mean vector:\n",
        "$$ X = X - w_X $$\n",
        "   - Note: The mean vector $w_X$ belongs to ℝ^m and is a column vector.\n",
        "\n",
        "2. **Evaluate Singular Values and Vectors**: Now, decompose your data matrix $X$ using the Singular Value Decomposition (SVD). The equation is:\n",
        "$$ X = U\\delta V^T $$\n",
        "   - Where:\n",
        "     - $U$ is in ℝ^m×m.\n",
        "     - $V$ is in ℝ^n×n.\n",
        "     - $\\delta$ is in ℝ^m×n.\n",
        "   - To perform the SVD, use the `numpy.linalg.svd(X)` function.\n",
        "\n",
        "For more details on how to use this numpy function, you might find the [numpy.linalg.svd documentation](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.linalg.svd.html) helpful.\n",
        "\n",
        "\n",
        "One of the advantages of SVD is that the singular values you get are already ordered by size, so that's one less step to worry about!\n",
        "\n",
        "**Quick Guide on `numpy.linalg.svd`**\n",
        "\n",
        "When you use the command `u, d, v = np.linalg.svd(X)`, here's what you get:\n",
        "\n",
        "- **u**: This contains the left-singular vectors as its columns.\n",
        "- **d**: This is a vector that holds the singular values. If you want to turn it into a matrix, just use `np.diag(d)`.\n",
        "- **v**: This has the right-singular vectors as its rows. Just to clear things up, the `v` you get from numpy's SVD function actually corresponds to what we've been calling $V^T$ in our discussions.\n"
      ],
      "metadata": {
        "id": "L9HalvU2nHel"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Relationship between PCA and SVD\n",
        "\n",
        "Principal Component Analysis (PCA) and Singular Value Decomposition (SVD) are both popular techniques in linear algebra and statistics, especially in the realm of data analysis and machine learning. They are closely related and can sometimes be used interchangeably for certain tasks, such as dimensionality reduction. Let's delve into their relationship:\n",
        "\n",
        "### 1. **Definitions**:\n",
        "- **PCA**: This is a method used to emphasize variation and bring out strong patterns in a dataset. It's commonly used to make data easy to explore and visualize. Essentially, PCA transforms the original data into a new coordinate system, where the axes (principal components) are orthogonal (perpendicular) and capture the maximum variance in the data.\n",
        "  \n",
        "- **SVD**: This is a matrix factorization technique that decomposes a matrix into three other matrices. If we have a matrix $A$, then the SVD of $A$ is given by $A = U\\Sigma V^T$, where $U$ and $V$ are orthogonal matrices and $\\Sigma$ is a diagonal matrix containing the singular values.\n",
        "\n",
        "### 2. **Relationship**:\n",
        "The covariance matrix of a data set can be decomposed using either PCA or SVD. Here’s how they are related:\n",
        "\n",
        "- When you perform PCA, you start by computing the covariance matrix of your data. The next step is to find the eigenvectors and eigenvalues of this covariance matrix. These eigenvectors correspond to the principal components of the data.\n",
        "\n",
        "- If you perform SVD on the data matrix $X$ such that $X = U\\Sigma V^T$, then the principal components of $X$ are given by the columns of $U$. Furthermore, the square of the singular values in $\\Sigma$ are proportional to the eigenvalues of the covariance matrix of $X$.\n",
        "\n",
        "- This means that you can use SVD to compute the principal components without explicitly calculating the covariance matrix, which can be computationally advantageous for large datasets.\n",
        "\n",
        "### 3. **Practical Implication**:\n",
        "In many software implementations, PCA is actually carried out using the SVD for numerical stability and efficiency reasons. This makes the relationship between PCA and SVD not just theoretical but also very practical.\n",
        "\n",
        "### 4. **Advantages of Using SVD in PCA**:\n",
        "1. SVD is a more stable numerical procedure.\n",
        "2. SVD works directly on the data matrix, so there's no need to compute the covariance matrix, which can be computationally intensive for large datasets.\n",
        "3. SVD ensures orthogonality of the principal components.\n",
        "\n",
        "In summary, while PCA and SVD have distinct definitions and purposes, they are closely intertwined, both mathematically and in practice. Understanding this relationship can be immensely helpful when working on data analysis tasks."
      ],
      "metadata": {
        "id": "jwZ25hnsokpz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "SVD provides a general decomposition for matrices, even if they're not square.\n",
        "\n",
        "- For any real matrix $X$ of size $n \\times m$, the SVD gives us the decomposition:\n",
        "  $$X = U\\Sigma V^T$$\n",
        "- Using $X$, we can form two symmetric covariance matrices, $XX^T$ and $X^TX$.\n",
        "- These matrices can be further broken down as:\n",
        "  $$XX^T = U\\Sigma V^T V \\Sigma^T U^T = U \\Sigma \\Sigma^T U^T$$\n",
        "  $$X^TX = V\\Sigma^T \\Sigma V^T$$\n",
        "- When $n > m$, both $XX^T$ and $X^TX$ will have $m$ shared eigenvalues.\n",
        "- The remaining $n - m$ eigenvalues of $X^TX$ will be zero.\n",
        "\n",
        "\n",
        "The Singular Value Decomposition (SVD) provides a way to generalize decomposition even when matrices aren't square.\n",
        "- For estimating the eigen-decomposition of the matrix product $XX^T$, it can be represented as:\n",
        "  $$XX^T = U \\Sigma^2 U^T$$\n",
        "- Let's consider a single eigenvector $u$ and its corresponding singular value $\\sigma$:\n",
        "  $$XX^Tu = \\sigma^2 u$$\n",
        "- If we multiply both sides by $X^T$, we get:\n",
        "  $$(X^TX)X^Tu = \\sigma^2 X^T u$$\n",
        "  This implies that both $XX^T$ and $X^TX$ share $m$ eigenvalues. Furthermore, the vector $v = X^Tu$ is an eigenvector of $X^TX$, and it has the eigenvalue $\\sigma^2$.\n",
        "- The remaining eigenvectors will have zero values. Here's why:\n",
        "  - If $X$ is a $1 \\times m$ matrix and its rank $r$ is less than or equal to the minimum of $m$ and $n$, then $X^TX$, which is an $m \\times m$ matrix, will have rank deficiency.\n",
        "  - The matrix $\\Sigma$ will be in a rectangular diagonal form, looking somewhat like this:\n",
        "  $$\n",
        "  \\Sigma =\n",
        "  \\begin{bmatrix}\n",
        "    \\sigma_1 & 0 & \\dots & 0 & \\dots & 0 \\\\\n",
        "    0 & \\sigma_2 & \\dots & 0 & \\dots & 0 \\\\\n",
        "    \\vdots & \\vdots & \\ddots & \\vdots & \\ddots & \\vdots \\\\\n",
        "    0 & 0 & \\dots & \\sigma_{m-1} & \\dots & 0 \\\\\n",
        "    0 & 0 & \\dots & 0 & \\sigma_m & \\dots & 0 \\\\\n",
        "  \\end{bmatrix}\n",
        "  $$\n",
        "\n"
      ],
      "metadata": {
        "id": "JbfP69kEorlA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PCA for dimensionality reduction\n",
        "\n",
        "\n",
        "For $n > m$, the difference $n-m$ of the eigenvalues will automatically be zero because the matrix rank, $r$, is always less than or equal to the minimum of $m$ and $n$ ($r \\leq \\min(m, n)$).\n",
        "\n",
        "To achieve even more dimensionality reduction, you can choose to keep only the eigenvectors that correspond to the $k$ largest eigenvalues, where $k < m$.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-6-PCA/imgs/k.png' width=700px >\n",
        "\n",
        "\n",
        "\n",
        "**How to choose k?**\n",
        "\n",
        "Wondering how to decide on the value of $k$? A common method is:\n",
        "\n",
        "1. Check the cumulative distribution of the sorted eigenvalues.\n",
        "2. Set a desired cut-off based on the explained variance.\n",
        "3. Remember, when doing this, always ensure that your eigenvalues (and the corresponding eigenvectors) are sorted in descending order from largest to smallest.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Alright, let's dive into some \"math magic\" with PCA and how we can use it to simplify our data.\n",
        "\n",
        "Imagine you have a bunch of data points scattered everywhere in a 3D space, like stars in the galaxy. But what if we could look at them from just the right angle that makes everything seem like it's in a neat 2D line? That's what PCA does—it helps us find that perfect angle!\n",
        "\n",
        "Now, here's a fun fact: if your data has more dimensions (let's say $n$) than data points (say $m$), some of these dimensions will automatically be... well, boring. In math-speak, this means that for any difference between $n$ and $m$ (like $n-m$), the values in those dimensions don't really change or add any new info—they're just flatlining at zero. It's like if you had a 3D movie but some parts of it didn't pop out—they just stayed flat.\n",
        "\n",
        "So, if we want to make our data even simpler (because who doesn't love simplicity?), we can choose to keep only the \"juiciest\" parts of it. How do we find these parts? We look for the \"brightest stars\" or, in math terms, the biggest eigenvalues. If we decide to keep only $k$ of these bright stars, we make sure $k$ is less than $m$.\n",
        "\n",
        "Now, the million-dollar question: how many stars or eigenvalues should we keep?  Well, one way to decide is by seeing how much they shine compared to all the stars combined. This is like checking the box office earnings of movies and picking the top-grossing ones to watch. For PCA, this means looking at the sorted list of eigenvalues and seeing where the most shine or variance is coming from. And always remember, when you're picking out these stars, start from the top and work your way down. Biggest to smallest, just like how we'd rank movies!\n"
      ],
      "metadata": {
        "id": "BafghLynpxJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PCA as a linear transformation\n",
        "\n",
        "- Think of PCA like this: It's a technique that finds out how to best describe your data using its own patterns. One way to do this is by looking at the data's \"covariance matrix\" and breaking it down. In fancy math terms, this breakdown is called the eigen-decomposition and is written as $S = U\\lambda U^1$.\n",
        "  \n",
        "- Now, imagine you're dancing. The \"rotation\" ($U$) is like spinning around on the dance floor. The \"scaling\" ($\\lambda$) is like moving closer or further away from the center of the room. So with PCA, it's like finding the best dance move to capture the rhythm of the music (or in this case, the patterns in the data).\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-6-PCA/imgs/sum.png' width=700px >\n"
      ],
      "metadata": {
        "id": "uPgjihZWrjgm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "Of course, I understand. Let's break it down in a straightforward manner:\n",
        "\n",
        "- **Goal of PCA**: PCA aims to reorient data in a way that highlights its primary patterns or variations.\n",
        "\n",
        "- **Implementation**: This is achieved by assessing the eigenvectors of the data covariance matrix. Only the eigenvectors related to non-zero eigenvalues are considered.\n",
        "\n",
        "- **Using SVD for Stability**: In cases where matrices have rank issues, it's more stable to derive the needed eigenvectors and eigenvalues using Singular Value Decomposition (SVD).\n",
        "\n",
        "- **Dimensionality Reduction**: To further reduce data complexity, eigenvalues (and their corresponding eigenvectors) are ranked by magnitude, from the largest to the smallest. The distribution of these values can then be examined to decide how many significant components to retain.\n",
        "\n",
        "- **Outcome**: Ultimately, PCA provides a transformed version of the data where the most significant patterns are easily distinguishable. This is achieved by minimizing less important variations or noise in the dataset."
      ],
      "metadata": {
        "id": "mdFIPvQ4r-uf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IIΙ Statistical independence\n",
        "\n",
        "(for notes open 5.3.Linear_Manifold_Learning_ICA.ipynb)\n",
        "\n",
        "## What does it mean for data to be independent?\n",
        "\n",
        "Two events, A and B, are independent when...\n",
        "Knowing the outcome of A doesn't influence or provide insight into the outcome of B, and the same goes the other way around.\n",
        "For instance, if we have a group with 12 boys and 12 girls:\n",
        "\n",
        "**Q1:** Given one person selected at random, what is the probability that it is a girl?\n",
        "\n",
        "If we randomly pick one individual, what's the likelihood it's a girl?\n",
        "\n",
        "**Q2:** What happens when we add another variable – favourite movie\n",
        "How does the probability change if we also consider their favorite movie?\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-6-PCA/imgs/table1.png' width=700px >\n",
        "\n",
        "**Q3:** What is the probability that we sample a person who’s favorite film is comedy?\n",
        "A\n",
        "1. P(M=C)=3/24\n",
        "2. P(M=C)=2/12\n",
        "3. P(M=C)=6/24\n",
        "4. P(M=C)=3/6\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-6-PCA/imgs/table3.png' width=700px >\n",
        "\n",
        "The marginal probabilities are represented as $P_M = \\frac{m}{c + N}$.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-6-PCA/imgs/table4.png' width=700px >\n",
        "\n",
        "\n",
        "The joint probabilities can be described as $P(G = g', M = m') = \\frac{n'}{N}$.\n",
        "\n",
        "**Q4** What is the joint probability that we sample a girl who’s favorite film is comedy?\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-6-PCA/imgs/table5.png' width=700px >\n",
        "\n",
        "**Q5** What is the conditional probability that we sample a person who prefers comedy\n",
        "if we know they are a girl?\n",
        "\n",
        "The conditional probabilities can be represented as $P(M = m' | G = g') = \\frac{n'}{c'}$.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-6-PCA/imgs/table6.png' width=700px >\n",
        "\n",
        "Answer: P(M=C| G=F)=3/12=6/24 --> Independent\n",
        "\n",
        "Therefore, if $P(M=C| G=F) = P(M=C)$, then M and G are independent.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-6-PCA/imgs/table7.png' width=700px >\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jT5CrT1otlAy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definition of independence\n",
        "\n",
        "Events A and B are independent when:\n",
        "\n",
        "- Knowing the outcome of event A doesn't provide any information about the outcome of event B, and the reverse is also true: $P(A|B) = P(A)$.\n",
        "- The joint probability of both events, $P(A, B)$, is the multiplication of their individual probabilities.\n",
        "\n",
        "From the rules of probability, we know:\n",
        "$P(A, B) = P(A|B) \\times P(B)$.\n",
        "If $P(A|B) = P(A)$, it implies:\n",
        "$P(A, B) = P(A) \\times P(B)$."
      ],
      "metadata": {
        "id": "PPpWqGF215kW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Independent events are uncorrelated\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-6-PCA/imgs/table8.png' width=700px >\n",
        "\n",
        "$$\\rho­,® = \\sum_{i} \\frac{(x_i - \\mu°)(y_i - \\mu±)}{\\sigma­ \\cdot \\sigma®}$$\n",
        "\n",
        "In the context of sinusoidal time series, when two such series are 90° out of phase, they are considered orthogonal and therefore uncorrelated. This property is important when imaging independent spatial patterns of functional brain activity, particularly when obtained from Independent Component Analysis (ICA), as it ensures that these patterns do not overlap.\n"
      ],
      "metadata": {
        "id": "Pkpt0lwk2dkG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## However uncorrelated events are not necessarily independent\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-6-PCA/imgs/table9.png' width=400px >\n",
        "\n",
        "\n",
        "Zero correlation is an inadequate condition for establishing independence.\n",
        "\n",
        "Correlation, denoted as $\\rho­,®$, is calculated as follows:\n",
        "\n",
        "$$\\rho­,® = \\frac{E[(X - \\mu_X)(Y - \\mu_Y)]}{\\sigma_X \\cdot \\sigma_Y}$$\n",
        "\n",
        "For instance, let's consider the case where $Y$ is defined as $X \\cdot P$. In this scenario, we have $E[X \\cdot Y] = 0$ and $E[X] = 0$, which implies $\\rho­,® = 0$.\n",
        "\n",
        "However, it's important to note that X completely determines Y, indicating that they are not independent.\n",
        "\n",
        "The correlation coefficient primarily detects linear (first-order) dependencies between variables. To uncover further dependencies, one can explore higher-order statistics such as skewness and kurtosis.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "So, here's the deal: just because two things have a correlation of zero, it doesn't mean they're completely independent. This might sound a bit counterintuitive, but bear with me, and I'll explain why.\n",
        "\n",
        "Correlation, which we often represent as $\\rho­,®$, is a way to measure how two things, let's call them X and Y, relate to each other. If $\\rho­,®$ is zero, it means there's no linear relationship between X and Y. In other words, they don't move together in a straight line.\n",
        "\n",
        "But here's the tricky part: imagine you have two variables, X and Y, where Y is defined as X multiplied by another variable, let's call it P. In this case, even though $\\rho­,®$ is zero, X completely determines Y because if you know X, you can figure out Y just by multiplying it by P. So, they're not truly independent; one depends on the other, even though it might not be in a straight-line fashion.\n",
        "\n",
        "The correlation coefficient, which is what we use to calculate $\\rho­,®$, can only pick up on these linear relationships, the first-order stuff. To uncover more complex relationships, you might need to dig into higher-order statistics like skewness and kurtosis.\n",
        "\n",
        "So, in a nutshell, zero correlation doesn't necessarily mean independence when it comes to variables. It's a bit like looking at the tip of the iceberg; there might be more going on beneath the surface!"
      ],
      "metadata": {
        "id": "WnlOlwma4yb1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## In summary:\n",
        "\n",
        "- When dealing with two independent events:\n",
        "  - The conditional probability, P(A|B), will be equal to the marginal probability, P(A).\n",
        "  - The joint probability is equal to the product of the marginals: P(A, B) = P(A) * P(B).\n",
        "  - The events will be uncorrelated.\n",
        "\n",
        "However, it's crucial to remember that uncorrelated events may still be dependent. So, while these are important concepts to understand, keep in mind that there could be more to the relationship between events. Make sure to explore the full context to draw accurate conclusions."
      ],
      "metadata": {
        "id": "YWhlisGe5qQc"
      }
    }
  ]
}