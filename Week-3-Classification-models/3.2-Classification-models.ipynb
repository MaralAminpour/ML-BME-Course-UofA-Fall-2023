{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaralAminpour/ML-BME-Course-UofA-Fall-2023/blob/main/Week-3-Classification-models/3.2-Classification-models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac043bc5-3dec-4331-961b-4ea610d5c029",
      "metadata": {
        "id": "ac043bc5-3dec-4331-961b-4ea610d5c029"
      },
      "source": [
        "# Binary Classification Models\n",
        "\n",
        "We will go through a series of examples creating different types of binary classification models for the same dataset. Binary classifiers predict two labels. Usually, we'll convert our labels to 0 and 1.  We will use the example of heart disease data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "33085db4-86c3-42fa-becc-dd1446cc1832",
      "metadata": {
        "id": "33085db4-86c3-42fa-becc-dd1446cc1832"
      },
      "outputs": [],
      "source": [
        "# Imports needed for this notebook\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "\n",
        "# Note that if you are using Anaconda, GraphViz won't be installed by default.\n",
        "# You will need to install graphviz and python-graphviz.\n",
        "import graphviz\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, classification_report, roc_curve, auc\n",
        "from sklearn.linear_model import Perceptron, LogisticRegression, SGDClassifier\n",
        "from sklearn.svm import LinearSVC, SVC\n",
        "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2f4d0d5-4598-4abe-a997-34998afeca25",
      "metadata": {
        "id": "f2f4d0d5-4598-4abe-a997-34998afeca25"
      },
      "source": [
        "First, let's load the data we'll need for all the examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "cbccbe98-7fb2-496d-a4fb-c5d16e81e611",
      "metadata": {
        "id": "cbccbe98-7fb2-496d-a4fb-c5d16e81e611"
      },
      "outputs": [],
      "source": [
        "# This code will download the required data files from GitHub\n",
        "import requests\n",
        "def download_data(source, dest):\n",
        "    base_url = 'https://raw.githubusercontent.com/'\n",
        "    owner = 'SirTurtle'\n",
        "    repo = 'ML-BME-UofA-data'\n",
        "    branch = 'main'\n",
        "    url = '{}/{}/{}/{}/{}'.format(base_url, owner, repo, branch, source)\n",
        "    r = requests.get(url)\n",
        "    f = open(dest, 'wb')\n",
        "    f.write(r.content)\n",
        "    f.close()\n",
        "\n",
        "# Create the temp directory, if it doesn't already exist\n",
        "import os\n",
        "if not os.path.exists('temp'):\n",
        "   os.makedirs('temp')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "80edc0aa-689f-4f10-8e3d-c8ff3215431d",
      "metadata": {
        "id": "80edc0aa-689f-4f10-8e3d-c8ff3215431d",
        "outputId": "b417d507-0f01-41e9-c77a-66b218e93ed0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          EF    GLS  HF\n",
              "0  50.922280 -19.57   0\n",
              "1  54.601227 -19.00   0\n",
              "2  50.000000 -21.00   0\n",
              "3  50.819672 -18.74   0\n",
              "4  53.191489 -19.78   0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-314161ac-edbe-4f8b-8494-3d97ab83c493\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>EF</th>\n",
              "      <th>GLS</th>\n",
              "      <th>HF</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>50.922280</td>\n",
              "      <td>-19.57</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>54.601227</td>\n",
              "      <td>-19.00</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>50.000000</td>\n",
              "      <td>-21.00</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>50.819672</td>\n",
              "      <td>-18.74</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>53.191489</td>\n",
              "      <td>-19.78</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-314161ac-edbe-4f8b-8494-3d97ab83c493')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-314161ac-edbe-4f8b-8494-3d97ab83c493 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-314161ac-edbe-4f8b-8494-3d97ab83c493');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-6961840d-7b8f-48cd-bddb-59e3b0721adc\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6961840d-7b8f-48cd-bddb-59e3b0721adc')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-6961840d-7b8f-48cd-bddb-59e3b0721adc button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Download the data\n",
        "download_data('Week-3-Classification-models/data/heart_failure_data.csv', 'temp/heart_failure_data.csv')\n",
        "\n",
        "# Read data file into a dataframe object\n",
        "df = pd.read_csv('temp/heart_failure_data.csv')\n",
        "\n",
        "# Print the first few lines of the dataframe\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b02a7ab9-d61e-48f9-af6d-c21daae35fef",
      "metadata": {
        "id": "b02a7ab9-d61e-48f9-af6d-c21daae35fef"
      },
      "source": [
        "## Data dictionary\n",
        "\n",
        "**EF**: Ejection Fraction. A measurement of how much blood the left ventricle pumps out with each contraction. Expressed as a percent in the range 0 to 100.\n",
        "\n",
        "**GLS**: Global Longitudinal Strain. A measurement of myocardial deformation along the longitudinal cardiac axis. Expressed as a negative percent in the range 0 to -100.\n",
        "\n",
        "**HF**: Heart Failure class\n",
        "- 0 = Healthy\n",
        "- 1 = Heart failure"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b2478d5-c11e-4275-9eb3-edf85461f8d5",
      "metadata": {
        "id": "6b2478d5-c11e-4275-9eb3-edf85461f8d5"
      },
      "source": [
        "### Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "63acb35f-76be-49a7-84f0-d76fd4956c81",
      "metadata": {
        "id": "63acb35f-76be-49a7-84f0-d76fd4956c81",
        "outputId": "ac9c2890-4b32-427e-b878-cdee043eb637",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HF\n",
              "0    60\n",
              "1    60\n",
              "Name: HF, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# Check balance of the output variables\n",
        "df.groupby(['HF'])['HF'].count()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5655d2d-5a11-4a41-9035-51d9a9697db8",
      "metadata": {
        "id": "c5655d2d-5a11-4a41-9035-51d9a9697db8"
      },
      "source": [
        "Our example has **an exactly equal number of samples in each class**. If we had an unbalanced dataset, we could balance it by\n",
        "\n",
        "1. **removing some random samples** from the larger class, or by\n",
        "\n",
        "2. **duplicating small samples from the smaller class**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "edf32438-f70e-4e27-a4ce-0049d9845350",
      "metadata": {
        "id": "edf32438-f70e-4e27-a4ce-0049d9845350",
        "outputId": "ab092821-49e6-4aa6-c21b-640f2ef84535",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature matrix X dimensions:  (120, 2)\n",
            "Target vector y dimensions:  (120,)\n"
          ]
        }
      ],
      "source": [
        "# Convert to numpy\n",
        "heart_failure_data = df.to_numpy()\n",
        "\n",
        "# Create feature matrix and target vector\n",
        "X = heart_failure_data[:,:2]   #all rows (:) and the first two columns (:2) of heart_failure_data\n",
        "y = heart_failure_data[:,2]    #ll rows (:) and only the third column (2) of heart_failure_data.\n",
        "\n",
        "\n",
        "print('Feature matrix X dimensions: ', X.shape)\n",
        "print('Target vector y dimensions: ', y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "X = heart_failure_data[:,:2]: We are creating the feature matrix X by selecting all rows (:) and the first two columns (:2) of heart_failure_data. So X will contain the data from the first and second columns of the original dataset.\n",
        "y = heart_failure_data[:,2]: We are creating the target vector y by selecting all rows (:) and only the third column (2) of heart_failure_data. This means that y will contain the data from the third column of the original dataset."
      ],
      "metadata": {
        "id": "Hforv_TQCRMG"
      },
      "id": "Hforv_TQCRMG"
    },
    {
      "cell_type": "markdown",
      "id": "07b74e6c-8f28-4ea3-869a-54dd89d7edc2",
      "metadata": {
        "id": "07b74e6c-8f28-4ea3-869a-54dd89d7edc2"
      },
      "source": [
        "## Plot data\n",
        "\n",
        "First, let's plot the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1ae6166-9c71-4c34-ae1a-1e5af84ffd2b",
      "metadata": {
        "id": "c1ae6166-9c71-4c34-ae1a-1e5af84ffd2b"
      },
      "outputs": [],
      "source": [
        "# This function will plot the heart failure data\n",
        "# We will plot the first feature (EF) on the x-axis and the second feature (GLS) on the y-axis\n",
        "def PlotData(X, y):\n",
        "\n",
        "    # Plot class 0\n",
        "    plt.plot(X[y==0,0], X[y==0,1], 'bo', alpha=0.75, markeredgecolor='k', label = 'Healthy')\n",
        "\n",
        "    # Plot class 1\n",
        "    plt.plot(X[y==1,0], X[y==1,1], 'rd', alpha=0.75, markeredgecolor='k', label = 'Heart Failure')\n",
        "\n",
        "    # Annotate the plot\n",
        "    plt.title('Diagnosis of Heart Failure')\n",
        "    plt.xlabel('EF')\n",
        "    plt.ylabel('GLS')\n",
        "    plt.legend()\n",
        "\n",
        "# Call the function to plot the dataset\n",
        "PlotData(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "129b4516-654d-4d65-be73-414c63c95b60",
      "metadata": {
        "id": "129b4516-654d-4d65-be73-414c63c95b60"
      },
      "source": [
        "## Standardize Data\n",
        "\n",
        "We'll use [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) to standardize the features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6439bddd-3a98-4123-b8b8-59e30801d9dd",
      "metadata": {
        "id": "6439bddd-3a98-4123-b8b8-59e30801d9dd"
      },
      "outputs": [],
      "source": [
        "# Create an object to scale the features to have zero mean and unit variance\n",
        "# We don't need to do this for all models, but let's do it here to be consistent\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Create a feature matrix containing EF and GLS\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Plot the scaled data\n",
        "PlotData(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b4153c8-4835-4165-9cd0-102ff0fb7391",
      "metadata": {
        "id": "9b4153c8-4835-4165-9cd0-102ff0fb7391"
      },
      "source": [
        "## Creating training and test sets\n",
        "\n",
        "We'll create training and test sets that we'll use for each example. For these examples, we'll just split up the data samples randomly, with 60% in the training set and 40% in the test set. A more common division would be (80%,20%) but since our dataset is small, we'll use more in the test set. We'll use the [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)  function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aaeddbf8-6575-4f8b-a660-cdbf6ff8a32a",
      "metadata": {
        "id": "aaeddbf8-6575-4f8b-a660-cdbf6ff8a32a"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)\n",
        "# Using a fixed random_state for consistency"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0682c83-a574-4018-95d1-284f7161b39c",
      "metadata": {
        "id": "f0682c83-a574-4018-95d1-284f7161b39c"
      },
      "source": [
        "## Perceptron"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8706829c-a3ca-487d-9947-84713f86aa46",
      "metadata": {
        "id": "8706829c-a3ca-487d-9947-84713f86aa46"
      },
      "source": [
        "### Fit the model\n",
        "This code fits the [Perceptron](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html) model to the training data.\n",
        "\n",
        "- max_iter: The maximum number of passes over the training data (aka epochs)\n",
        "- eta0: Constant by which the updates are multiplied\n",
        "\n",
        "Note that the Perceptron model is the same as the [SGDClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html) function with SGDClassifier(loss=\"perceptron\", eta0=1, learning_rate=\"constant\", penalty=None)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcf3e72a-3e1e-4e5c-bd4b-b448b9701524",
      "metadata": {
        "id": "fcf3e72a-3e1e-4e5c-bd4b-b448b9701524"
      },
      "outputs": [],
      "source": [
        "# Create and fit the model\n",
        "p_model = Perceptron(max_iter=100, eta0=0.2, random_state=0)\n",
        "p_model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5164537c-3694-4096-9e6a-fd238bf61072",
      "metadata": {
        "id": "5164537c-3694-4096-9e6a-fd238bf61072"
      },
      "source": [
        "### Evaluate the model\n",
        "\n",
        "We'll perform the same basic analysis for each model. First, we'll show the confusion matrix for the test set. Then we'll calculate several useful scores. This will help to evaluate the performance of each model and also to assess how much overfitting we have. Next, we'll also generate some interesting plots for each model.\n",
        "\n",
        "We'll use the [confusion_matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) and the [accuracy_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) function. We'll use the [recall_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html) to calculate the sensitivity specificity.\n",
        "\n",
        "Note that in binary classification, the recall of the positive class is also known as “sensitivity”; the recall of the negative class is “specificity”.\n",
        "\n",
        "In our example:\n",
        "- Negative: HF = 0 = healthy\n",
        "- Positive: HF = 1 = heart failure\n",
        "\n",
        "An alternative is to use the [classification_report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) function. This will return the recall, precision and F1 score on all classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b6e9805-c303-4063-be3b-1e65e5dac266",
      "metadata": {
        "id": "5b6e9805-c303-4063-be3b-1e65e5dac266"
      },
      "outputs": [],
      "source": [
        "def EvaluateModel(model, X_train, X_test, y_train, y_test):\n",
        "\n",
        "    # Calculate and print metrics for the training set\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "    train_sensitivity = recall_score(y_train, y_train_pred, pos_label=1)\n",
        "    train_specificity = recall_score(y_train, y_train_pred, pos_label=0)\n",
        "    print('Training set: Accuracy = {:0.2f} Sensitivity = {:0.2f} Specificity = {:0.2f}'.format(train_accuracy, train_sensitivity, train_specificity))\n",
        "\n",
        "    # Calculate and print metrics for the test set\n",
        "    y_test_pred = model.predict(X_test)\n",
        "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "    test_sensitivity = recall_score(y_test, y_test_pred, pos_label=1)\n",
        "    test_specificity = recall_score(y_test, y_test_pred, pos_label=0)\n",
        "    print('Test set: Accuracy = {:0.2f} Sensitivity = {:0.2f} Specificity = {:0.2f}'.format(test_accuracy, test_sensitivity, test_specificity))\n",
        "\n",
        "    # Display the confusion matrix for the test set\n",
        "    cm = confusion_matrix(y_test, y_test_pred)\n",
        "    sns.heatmap(cm, annot=True)\n",
        "    plt.gcf().set_size_inches(2, 2)\n",
        "    plt.show()\n",
        "\n",
        "EvaluateModel(p_model, X_train, X_test, y_train, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33532c2b-9bae-46ce-88e9-8922f4b959c7",
      "metadata": {
        "id": "33532c2b-9bae-46ce-88e9-8922f4b959c7"
      },
      "source": [
        "We have better sensitivity and worse specificity. Our model is better at correctly predicting that patients with heart disease have heart disease, but is worse at correctly predicting that patients without heart disease don't have heart disease."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10269e97-7b0e-49c5-ac1f-ed0eea7bb982",
      "metadata": {
        "id": "10269e97-7b0e-49c5-ac1f-ed0eea7bb982"
      },
      "outputs": [],
      "source": [
        "# This is an alternate function for evaluating the model results using the classification_report function\n",
        "def EvaluateModelClassificationReport(model, X_train, X_test, y_train, y_test):\n",
        "\n",
        "    # Calculate and print classification report for the training set\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    print('Classification report for training set:')\n",
        "    print(classification_report(y_train, y_train_pred))\n",
        "\n",
        "    # Calculate and print classification report for the test set\n",
        "    y_test_pred = model.predict(X_test)\n",
        "    print('Classification report for test set:')\n",
        "    print(classification_report(y_test, y_test_pred))\n",
        "\n",
        "    # Display the confusion matrix for the test set\n",
        "    cm = confusion_matrix(y_test, y_test_pred)\n",
        "    sns.heatmap(cm, annot=True)\n",
        "    plt.gcf().set_size_inches(2, 2)\n",
        "    plt.show()\n",
        "\n",
        "EvaluateModelClassificationReport(p_model, X_train, X_test, y_train, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a108bd6-4edf-4acb-bb5e-cd78995c1d81",
      "metadata": {
        "id": "6a108bd6-4edf-4acb-bb5e-cd78995c1d81"
      },
      "source": [
        "### Plot the model\n",
        "\n",
        "Let's also visualize the decision boundary itself. This specific visualization only works because we have two features. (In general, the decision boundary will be a hyperplane, so to use a similar visualization we would need to reduce the dimensionality of the features.) The result of the classification is plotted below. We'll also plot data points for the full dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a940ad40-c808-47ae-b1b8-ba8df58cbded",
      "metadata": {
        "id": "a940ad40-c808-47ae-b1b8-ba8df58cbded"
      },
      "outputs": [],
      "source": [
        "# Plot the decision boundary for a linear model\n",
        "# This function is only for linear models with 2 features\n",
        "def PlotDecisionBoundary(model, X, y):\n",
        "\n",
        "    # Plot the data\n",
        "    PlotData(X, y)\n",
        "\n",
        "    # Next, we will plot the decision boundary using the weights of the model\n",
        "\n",
        "    # Define y-coordinates\n",
        "    # We are just using the range (min and max) of the y-axis (GLS in this case)\n",
        "    x2 = np.array([X[:,1].min(), X[:,1].max()])\n",
        "\n",
        "    # Find the weights\n",
        "    # w0 is the\n",
        "    # w1 is the weight for the first feature\n",
        "    # w2 is the weight for the second feature\n",
        "    # i.e. The decision boundary is function h(x) = w0 + w1*x_1 + w2*x_2 where x_1 and x_2 are our 2 features (EF, GLS)\n",
        "    w0 = model.intercept_[0]\n",
        "    w1 = model.coef_[0][0]\n",
        "    w2 = model.coef_[0][1]\n",
        "\n",
        "    # Define x-coordinates\n",
        "    # Notice that this equation is just a rearrangement of the function for the decision boundary\n",
        "    x1 = -(w0 + w2*x2)/w1\n",
        "    # Now the points (x1[0],x2[0]) and (x1[1],x2[1]) are two points on our decision boundary\n",
        "\n",
        "    # Plot the decision boundary\n",
        "    plt.plot(x1, x2, \"k-\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "PlotDecisionBoundary(p_model, X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8218039a-dd95-4270-a2e4-e3ba0830d963",
      "metadata": {
        "id": "8218039a-dd95-4270-a2e4-e3ba0830d963"
      },
      "source": [
        "The line is our model's decision boundary between the two classes (healthy and heart failure).\n",
        "\n",
        "Let's show some more information instead of just the decision boundary. We can show confidence scores for predictions. In the case of the Perceptron model the confidence score is simply proportional to the signed distance from the decision boundary. In Scikit-Learn we use the model's decision_function method to find the confidence scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4721e84-1895-4a5d-8a91-cfce3797f308",
      "metadata": {
        "id": "e4721e84-1895-4a5d-8a91-cfce3797f308"
      },
      "outputs": [],
      "source": [
        "# Plot the decision boundary for a linear model\n",
        "# This function is only for linear models with 2 features\n",
        "def PlotConfidenceScores(model, X, y, label=1):\n",
        "\n",
        "    # Create 1D arrays of points for each feature (we have 2 features in this example)\n",
        "    x1 = np.linspace(X[:,0].min(), X[:,0].max(), 1000)\n",
        "    x2 = np.linspace(X[:,1].min(), X[:,1].max(), 1000).T # Note the transpose\n",
        "\n",
        "    # Create 2D arrays that hold the coordinates in 2D feature space\n",
        "    x1, x2 = np.meshgrid(x1, x2)\n",
        "\n",
        "    # Flatten x1 and x2 to 1D vectors and concatenate into a feature matrix\n",
        "    Xp = np.c_[x1.ravel(), x2.ravel()]\n",
        "\n",
        "    # Predict confidence scores for the whole feature space\n",
        "    df = model.decision_function(Xp)\n",
        "\n",
        "    # Select the class\n",
        "    # Note that the confidence scores are >0 for class 1 (positive class)\n",
        "    # If we want to show the confidence scores for class 0 (negative class) we multiply by -1\n",
        "    if label == 0:\n",
        "        df *= -1\n",
        "\n",
        "    # Reshape to 2D\n",
        "    df = df.reshape(x1.shape)\n",
        "\n",
        "    # Plot using contourf to generate colored regions\n",
        "    plt.contourf(x1, x2, df, cmap = 'summer')\n",
        "\n",
        "    # Add a colorbar\n",
        "    plt.colorbar()\n",
        "\n",
        "    # Also, plot the line where the confidence score == 0, i.e. the decision boundary\n",
        "    plt.contour(x1, x2, df, levels=[0], colors='k')\n",
        "\n",
        "    # Also plot the data\n",
        "    PlotData(X, y)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "PlotConfidenceScores(p_model, X, y, label=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3dee660-47ae-4226-a082-6433046711a3",
      "metadata": {
        "id": "f3dee660-47ae-4226-a082-6433046711a3"
      },
      "source": [
        "## Logistic Regression\n",
        "\n",
        "The next model we'll try is the [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35ea2e19-7c80-4641-926f-cf0ff589bb30",
      "metadata": {
        "id": "35ea2e19-7c80-4641-926f-cf0ff589bb30"
      },
      "outputs": [],
      "source": [
        "# Here's the code for the logistic regression classifier\n",
        "\n",
        "# Create and fit the model\n",
        "logreg_model = LogisticRegression(random_state=0)\n",
        "logreg_model.fit(X_train, y_train)\n",
        "logreg_pred = logreg_model.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a02b5bc5-7f2f-42b1-baab-719c35fb9d65",
      "metadata": {
        "id": "a02b5bc5-7f2f-42b1-baab-719c35fb9d65"
      },
      "source": [
        "We'll use the same code to evaluate the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53009ff5-427a-4261-9376-55ab35c33715",
      "metadata": {
        "id": "53009ff5-427a-4261-9376-55ab35c33715"
      },
      "outputs": [],
      "source": [
        "EvaluateModel(logreg_model, X_train, X_test, y_train, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c8c1a1c-0bfe-4a77-afe4-58f9cdb96a79",
      "metadata": {
        "id": "7c8c1a1c-0bfe-4a77-afe4-58f9cdb96a79"
      },
      "outputs": [],
      "source": [
        "# Plot the decision boundary\n",
        "PlotDecisionBoundary(logreg_model, X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec8b3d93-d1e2-4385-864e-863475104a5a",
      "metadata": {
        "id": "ec8b3d93-d1e2-4385-864e-863475104a5a"
      },
      "outputs": [],
      "source": [
        "# Plot the confidence scores\n",
        "PlotConfidenceScores(logreg_model, X, y, label=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c43378df-3692-4e5e-9116-767c3e706255",
      "metadata": {
        "id": "c43378df-3692-4e5e-9116-767c3e706255"
      },
      "source": [
        "An advantage of the LogisticRegression model compared with the Perceptron is that we can estimate probabilities for each class. This is more useful than just using the distance from the decision boundary as a confidence score. We'll plot the probabilities for each class using [predict_proba](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.predict_proba) method of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03aeed11-42b9-4c96-b7ed-b55eadf5155f",
      "metadata": {
        "id": "03aeed11-42b9-4c96-b7ed-b55eadf5155f"
      },
      "outputs": [],
      "source": [
        "# Plot the probabilities for a linear model\n",
        "# This function is only for linear models with 2 features\n",
        "def PlotProbabilities(model, X, y, label=1):\n",
        "\n",
        "    # Create 1D arrays of points for each feature (we have 2 features in this example)\n",
        "    x1 = np.linspace(X[:,0].min(), X[:,0].max(), 1000)\n",
        "    x2 = np.linspace(X[:,1].min(), X[:,1].max(), 1000).T # Note the transpose\n",
        "\n",
        "    # Create 2D arrays that hold the coordinates in 2D feature space\n",
        "    x1, x2 = np.meshgrid(x1, x2)\n",
        "\n",
        "    # Flatten x1 and x2 to 1D vectors and concatenate into a feature matrix\n",
        "    Xp = np.c_[x1.ravel(), x2.ravel()]\n",
        "\n",
        "    # Predict probabilities for the whole feature space\n",
        "    # Note the predict_proba function! This gives us the probabilities\n",
        "    proba = model.predict_proba(Xp)\n",
        "\n",
        "    # Select the class\n",
        "    # Note this is different from how we selected the class with the decision boundary function\n",
        "    p = proba[:, label]\n",
        "\n",
        "    # Reshape to 2D\n",
        "    p = p.reshape(x1.shape)\n",
        "\n",
        "    # Plot using contourf\n",
        "    plt.contourf(x1, x2, p, cmap = 'summer')\n",
        "\n",
        "    # Add colorbar\n",
        "    plt.colorbar()\n",
        "\n",
        "    # Also, plot the line where the probability == 0.5\n",
        "    plt.contour(x1, x2, p, levels=[0.5], colors='k')\n",
        "\n",
        "    # Also plot the data\n",
        "    PlotData(X, y)\n",
        "\n",
        "PlotProbabilities(logreg_model, X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af7dd539-7985-4ab7-9c76-b08c691803e8",
      "metadata": {
        "id": "af7dd539-7985-4ab7-9c76-b08c691803e8"
      },
      "source": [
        "## Support Vector Classifier\n",
        "\n",
        "#### Support vector classification\n",
        "\n",
        "Now we'll explore the Support Vector Classifier (SVC). Linear binary SVC is very similar to the perceptron and logistic regression in the sense that it finds the optimal hyperplane to separate two classes. These methods, however, have different objectives through which they decide what is the optimal decision boundary.\n",
        "\n",
        "There are three different SVC classifiers in `sklearn` library:\n",
        "1. [LinearSVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html) implements linear classifier optimised for performance but does not support the kernel trick.\n",
        "2. [SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) implements SVC with kernel trick. Setting `kernel='linear'` produces the same result as LinearSVC but is less efficient in terms of computational time. Setting `kernel='rbf'` produces non-linear classifier with Gaussian kernel.\n",
        "3. [SGDclassifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html) implements various classifiers that are optimised using stochastic gradient descent. Its default setting for loss function is `loss='hinge'` which is another implementation of a linear SVC.\n",
        "\n",
        "In practice, the SVC model may take a long time to run for very large datasets, so the LinearSVC or SGDclassifier may be a better choice. On the other hand, the SVC model supports the kernel trick and makes it easy to obtain and visualize the support vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af8d66b9-49bd-4a65-a81b-1e69e4269b5a",
      "metadata": {
        "id": "af8d66b9-49bd-4a65-a81b-1e69e4269b5a"
      },
      "outputs": [],
      "source": [
        "# First, a LinearSVC model\n",
        "# dual='auto' will select dual=False unless n_samples<n_features and other conditions are met\n",
        "# With dual=False the optimization variable has dimension=dimension of n_features (dual=True it is equal to n_samples)\n",
        "# Note that when dual=False, random_state has no effect since the algorithm is not random\n",
        "linearsvc_model = LinearSVC(dual='auto', random_state=0)\n",
        "linearsvc_model.fit(X_train, y_train)\n",
        "linearsvc_pred = linearsvc_model.predict(X_test)\n",
        "\n",
        "# Results\n",
        "EvaluateModel(linearsvc_model, X_train, X_test, y_train, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfc66f6e-413e-45bb-b2e7-ebaff2a72f3d",
      "metadata": {
        "id": "cfc66f6e-413e-45bb-b2e7-ebaff2a72f3d"
      },
      "outputs": [],
      "source": [
        "# This function is for plotting the different SVC models\n",
        "# It is similar to PlotProbilities, but can also plot the support vectors\n",
        "# If plotSV=True, plot the support vectors with circles\n",
        "#    Note that LinearSVC does not provide the support vectors, so we can't easily plot them\n",
        "# If plotDF=True, plot the confidence scores using a contour plot\n",
        "# If plotProb=True, plot the probabilities using a contour plot\n",
        "#    Note that LinearSVC does not provide probabilities\n",
        "def PlotSVC(model, X, y, label=1, plotSV=False, plotDF=False, plotProba=False):\n",
        "\n",
        "    # Create 1D arrays of points for each feature (we have 2 features in this example)\n",
        "    x1 = np.linspace(-2.5, 2, 1000)\n",
        "    x2 = np.linspace(-3, 3.5, 1000).T # note the transpose\n",
        "\n",
        "    # Create 2D arrays that hold the coordinates in 2D feature space\n",
        "    x1, x2 = np.meshgrid(x1, x2)\n",
        "\n",
        "    # Flatten x1 and x2 to 1D vectors and concatenate into a feature matrix\n",
        "    Xp = np.c_[x1.ravel(), x2.ravel()]\n",
        "\n",
        "    # Plot decision function\n",
        "    if plotDF:\n",
        "        # Predict confidence scores for the whole feature space\n",
        "        df = model.decision_function(Xp)\n",
        "\n",
        "        # Reshape to 2D\n",
        "        df = df.reshape(x1.shape)\n",
        "        if label == 0:\n",
        "            df *= -1\n",
        "\n",
        "        # Zero contour is decision boundary, isolines +-1 are the margins\n",
        "        contour = plt.contour(x1, x2, df, levels=[-1,0,1], colors='k', linestyles=('dashed', 'solid', 'dashed'))\n",
        "        plt.clabel(contour, inline=1, fontsize=14)\n",
        "\n",
        "    # Plot probabilities\n",
        "    if plotProba:\n",
        "\n",
        "        # Predict probabilities for the whole feature space\n",
        "        proba = model.predict_proba(Xp)\n",
        "\n",
        "        # Select the class\n",
        "        p = proba[:, label]\n",
        "\n",
        "        # Reshape to 2D\n",
        "        p = p.reshape(x1.shape)\n",
        "\n",
        "        # Plot using contourf\n",
        "        plt.contourf(x1, x2, p, cmap = 'summer')\n",
        "\n",
        "        # Add colorbar\n",
        "        plt.colorbar()\n",
        "\n",
        "        # Also, plot the line where the probability == 0.5\n",
        "        plt.contour(x1, x2, p, levels=[0.5], colors='k')\n",
        "\n",
        "    # Plot support vectors\n",
        "    if plotSV:\n",
        "        svs = model.support_vectors_\n",
        "        plt.scatter(svs[:, 0], svs[:, 1], s=180, facecolors='pink', label = 'Support vectors', edgecolor='k')\n",
        "\n",
        "    # plot data\n",
        "    PlotData(X,y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69aef96f-41c5-4717-8e91-34d4a07da2f2",
      "metadata": {
        "id": "69aef96f-41c5-4717-8e91-34d4a07da2f2"
      },
      "outputs": [],
      "source": [
        "# Plot boundary\n",
        "PlotSVC(linearsvc_model, X, y, plotSV=False, plotDF=True, plotProba=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17b3b3cb-0e58-4f8b-8547-b557618a0456",
      "metadata": {
        "id": "17b3b3cb-0e58-4f8b-8547-b557618a0456"
      },
      "source": [
        "## Support Vector Classifier\n",
        "\n",
        "Next we'll try the [SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3291e6b-24cd-45e8-8e6b-071bd7563b10",
      "metadata": {
        "id": "c3291e6b-24cd-45e8-8e6b-071bd7563b10"
      },
      "outputs": [],
      "source": [
        "# A linear SVC using the SVC class (instead of LinearSVC)\n",
        "# probability=True will allow us to use predict_proba on the fitted model\n",
        "svc_model = SVC(kernel='linear', probability=True, random_state=0)\n",
        "svc_model.fit(X_train, y_train)\n",
        "svc_pred = svc_model.predict(X_test)\n",
        "\n",
        "# Results\n",
        "EvaluateModel(svc_model, X_train, X_test, y_train, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2d53862-b1b8-4a66-9da4-3b1f501734ac",
      "metadata": {
        "id": "e2d53862-b1b8-4a66-9da4-3b1f501734ac"
      },
      "outputs": [],
      "source": [
        "# Plot probabilities and support vectors\n",
        "PlotSVC(svc_model, X, y, plotSV=True, plotDF=True, plotProba=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52da9a12-5d30-4739-9872-0268c398343d",
      "metadata": {
        "id": "52da9a12-5d30-4739-9872-0268c398343d"
      },
      "source": [
        "## Support Vector Classifier with Kernel Trick\n",
        "\n",
        "The kernel trick we'll allow us to have a nonlinear boundary between the classes, instead of a simple line (or hyperplane)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8c0e1f0-80cd-45b5-9469-1afc5f4f1e93",
      "metadata": {
        "id": "f8c0e1f0-80cd-45b5-9469-1afc5f4f1e93"
      },
      "outputs": [],
      "source": [
        "# Create SVC model using the kernel trick\n",
        "kernelsvc_model = SVC(kernel='rbf', probability=True, random_state=0)\n",
        "\n",
        "# Fit the model\n",
        "kernelsvc_model.fit(X_train, y_train)\n",
        "EvaluateModel(kernelsvc_model, X_train, X_test, y_train, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "797dafc7-0357-4576-a07f-eb0b6c6f1d2e",
      "metadata": {
        "id": "797dafc7-0357-4576-a07f-eb0b6c6f1d2e"
      },
      "outputs": [],
      "source": [
        "# Plot decision boundary and margins\n",
        "PlotSVC(kernelsvc_model, X, y, plotSV=True, plotDF=True, plotProba=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b127e43-602a-4c19-a032-63b2ed57d0cb",
      "metadata": {
        "id": "6b127e43-602a-4c19-a032-63b2ed57d0cb"
      },
      "source": [
        "## Decision Tree\n",
        "\n",
        "Now let's check out the [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a83e67e-18d5-477f-942a-5826aecb1fc6",
      "metadata": {
        "id": "6a83e67e-18d5-477f-942a-5826aecb1fc6"
      },
      "outputs": [],
      "source": [
        "# Create and fit a DecisionTreeClassifier model\n",
        "# max_depth is the maximum depth of the tree\n",
        "tree_model = DecisionTreeClassifier(max_depth=2, random_state=0)\n",
        "tree_model.fit(X_train, y_train)\n",
        "EvaluateModel(tree_model, X_train, X_test, y_train, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "baa07586-f155-42aa-84ca-5301c5e6b51b",
      "metadata": {
        "id": "baa07586-f155-42aa-84ca-5301c5e6b51b"
      },
      "outputs": [],
      "source": [
        "# Plot the DecisionTreeClassifier model\n",
        "PlotProbabilities(tree_model, X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88123d15-858f-4aba-9197-bb1f0a8ed39f",
      "metadata": {
        "id": "88123d15-858f-4aba-9197-bb1f0a8ed39f"
      },
      "source": [
        "What happens if we increase max_depth?\n",
        "\n",
        "It can be interesting to visualize the decision tree itself. We'll use the GraphViz library to visualize the tree.\n",
        "\n",
        "Note that if you are using Anaconda, GraphViz won't be installed by default. You will need to install graphviz and python-graphviz."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c292f2ac-916f-4a78-9d26-da80aeb3a621",
      "metadata": {
        "id": "c292f2ac-916f-4a78-9d26-da80aeb3a621"
      },
      "outputs": [],
      "source": [
        "dot_data = export_graphviz(tree_model,\n",
        "                           feature_names=['EF', 'GLS'],\n",
        "                           class_names=['Healthy', 'Heart Failure'],\n",
        "                           filled=True, rounded=True,\n",
        "                           special_characters=True,\n",
        "                           out_file=None)\n",
        "graph = graphviz.Source(dot_data)\n",
        "graph"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb61e3da-ba19-4cff-b3e6-a66f4a72934b",
      "metadata": {
        "id": "cb61e3da-ba19-4cff-b3e6-a66f4a72934b"
      },
      "source": [
        "The gini parameter ranges between 0 and 1 and measures how \"pure\" the data is, with lower values indicating the data at particular node in the tree is closer to being all one class. gini = 0.5 means the data is 50% in each class. Note that the tree includes one leaf node where gini is still 0.5, although it only contains 4 samples."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29cb84d2-26d3-4969-a4d6-ab7348700a5f",
      "metadata": {
        "id": "29cb84d2-26d3-4969-a4d6-ab7348700a5f"
      },
      "source": [
        "## Random Forest\n",
        "\n",
        "Finally, let's see if we can improve on the DecisionTree with an ensemble of decision trees using the [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f60d26ca-fca8-419e-a58e-47fde0122ded",
      "metadata": {
        "id": "f60d26ca-fca8-419e-a58e-47fde0122ded"
      },
      "outputs": [],
      "source": [
        "forest_model = RandomForestClassifier(n_estimators=100, max_depth=2, random_state=0)\n",
        "forest_model.fit(X_train, y_train)\n",
        "EvaluateModel(forest_model, X_train, X_test, y_train, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49b11ff1-8fb2-4a8b-8a6c-e48ef7aa071e",
      "metadata": {
        "id": "49b11ff1-8fb2-4a8b-8a6c-e48ef7aa071e"
      },
      "outputs": [],
      "source": [
        "PlotProbabilities(forest_model, X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43ac08b9-81b8-41b1-a84f-1e1da5625adf",
      "metadata": {
        "id": "43ac08b9-81b8-41b1-a84f-1e1da5625adf"
      },
      "source": [
        "Notice that the decision boundary is a little closer to the lines obtained with the linear methods."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "683c192b-84a7-48c8-9818-a9b709f83cf8",
      "metadata": {
        "id": "683c192b-84a7-48c8-9818-a9b709f83cf8"
      },
      "source": [
        "## Comparing all the models\n",
        "\n",
        "Finally, we'll compare all the models using the [roc_curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html) and the ROC-AUC for the test set. We'll use the probabilities where available, otherwise the confidence scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "354a19f3-19b7-441b-8cba-087d57945d2b",
      "metadata": {
        "id": "354a19f3-19b7-441b-8cba-087d57945d2b"
      },
      "outputs": [],
      "source": [
        "def PlotROC():\n",
        "    pass\n",
        "\n",
        "label = 'Perceptron'\n",
        "model = p_model\n",
        "fpr, tpr, threshold = roc_curve(y_test, model.decision_function(X_test))\n",
        "roc_auc = auc(fpr, tpr)\n",
        "plt.plot(fpr, tpr, label=label+' (area = %0.2f)' % roc_auc, linewidth=1)\n",
        "\n",
        "label = 'Logistic Regression'\n",
        "model = logreg_model\n",
        "fpr, tpr, threshold = roc_curve(y_test, model.predict_proba(X_test)[:,1])\n",
        "roc_auc = auc(fpr, tpr)\n",
        "plt.plot(fpr, tpr, label=label+' (area = %0.2f)' % roc_auc, linewidth=1)\n",
        "\n",
        "label = 'Linear SVC'\n",
        "model = linearsvc_model\n",
        "fpr, tpr, threshold = roc_curve(y_test, model.decision_function(X_test))\n",
        "roc_auc = auc(fpr, tpr)\n",
        "plt.plot(fpr, tpr, label=label+' (area = %0.2f)' % roc_auc, linewidth=1)\n",
        "\n",
        "label = 'SVC'\n",
        "model = svc_model\n",
        "fpr, tpr, threshold = roc_curve(y_test, model.predict_proba(X_test)[:,1])\n",
        "roc_auc = auc(fpr, tpr)\n",
        "plt.plot(fpr, tpr, label=label+' (area = %0.2f)' % roc_auc, linewidth=1)\n",
        "\n",
        "label = 'Kernel SVC'\n",
        "model = kernelsvc_model\n",
        "fpr, tpr, threshold = roc_curve(y_test, model.predict_proba(X_test)[:,1])\n",
        "roc_auc = auc(fpr, tpr)\n",
        "plt.plot(fpr, tpr, label=label+' (area = %0.2f)' % roc_auc, linewidth=1)\n",
        "\n",
        "label = 'Decision Tree'\n",
        "model = tree_model\n",
        "fpr, tpr, threshold = roc_curve(y_test, model.predict_proba(X_test)[:,1])\n",
        "roc_auc = auc(fpr, tpr)\n",
        "plt.plot(fpr, tpr, label=label+' (area = %0.2f)' % roc_auc, linewidth=1)\n",
        "\n",
        "label = 'Random Forest'\n",
        "model = tree_model\n",
        "fpr, tpr, threshold = roc_curve(y_test, model.predict_proba(X_test)[:,1])\n",
        "roc_auc = auc(fpr, tpr)\n",
        "plt.plot(fpr, tpr, label=label+' (area = %0.2f)' % roc_auc, linewidth=1)\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', linewidth=2)\n",
        "plt.xlim([-0.05, 1.0])\n",
        "plt.ylim([-0.05, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.title(\"ROC curves\", fontsize=17)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}