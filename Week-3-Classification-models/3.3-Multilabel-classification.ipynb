{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaralAminpour/ML-BME-Course-UofA-Fall-2023/blob/main/Week-3-Classification-models/3.3-Multilabel-classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkeVaIJM2qV9"
      },
      "source": [
        "# Multi-label classification\n",
        "\n",
        "In this notebook we will consider three classes of patients:\n",
        "* Healthy: Label 0\n",
        "* Mild Heart Failure: Label 1\n",
        "* Severe Heart Failure: Label 2\n",
        "\n",
        "The data is otherwise similar to the previous notebook. We will consider the same two features - **EF** and **GLS**.\n",
        "\n",
        "### Libraries and dataset\n",
        "Run the code below to load the libraries, dataset, create the feature matrix `X` and label vector `y` and finally plot the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjx-KiFL2qV-"
      },
      "outputs": [],
      "source": [
        "# Imports needed for this notebook\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import requests\n",
        "import graphviz\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, f1_score, classification_report, roc_curve, auc\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, GridSearchCV\n",
        "from sklearn.linear_model import Perceptron, LogisticRegression, SGDClassifier\n",
        "from sklearn.svm import LinearSVC, SVC\n",
        "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTa1HTL22qV_"
      },
      "outputs": [],
      "source": [
        "# This code will download the required data files from GitHub\n",
        "def download_data(source, dest):\n",
        "    base_url = 'https://raw.githubusercontent.com/'\n",
        "    owner = 'SirTurtle'\n",
        "    repo = 'ML-BME-UofA-data'\n",
        "    branch = 'main'\n",
        "    url = '{}/{}/{}/{}/{}'.format(base_url, owner, repo, branch, source)\n",
        "    r = requests.get(url)\n",
        "    f = open(dest, 'wb')\n",
        "    f.write(r.content)\n",
        "    f.close()\n",
        "\n",
        "# Create the temp directory, if it doesn't already exist\n",
        "import os\n",
        "if not os.path.exists('temp'):\n",
        "   os.makedirs('temp')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PlmyrjNk2qV_"
      },
      "outputs": [],
      "source": [
        "# Download the data\n",
        "download_data('Week-3-Classification-models/data/heart_failure_data_complete.csv', 'temp/heart_failure_data_complete.csv')\n",
        "\n",
        "# Read data file into a dataframe object\n",
        "df = pd.read_csv('temp/heart_failure_data_complete.csv')\n",
        "\n",
        "# Print the first few lines of the dataframe\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82JIsVga2qV_"
      },
      "source": [
        "## Data dictionary\n",
        "\n",
        "**HF**: Heart Failure class\n",
        "- 0 = Healthy\n",
        "- 1 = Mild HF (heart failure)\n",
        "- 2 = Severe HF (heart failure)\n",
        "\n",
        "**EF**: Ejection Fraction. A measurement of how much blood the left ventricle pumps out with each contraction. Expressed as a percent in the range 0 to 100.\n",
        "\n",
        "**GLS**: Global Longitudinal Strain. A measure ment of myocardial deformation along the longitudinal cardiac axis. Expressed as a negative percent in the range 0 to -100.\n",
        "\n",
        "**QRS**: QRS interval (milliseconds)GLS. (This column will not be used for this notebook.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czFyIDlN2qWA"
      },
      "source": [
        "### Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkpBJfwH2qWA"
      },
      "outputs": [],
      "source": [
        "# Check balance of the output variables\n",
        "df.groupby(['HF'])['HF'].count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7csdrN8Q2qWA"
      },
      "source": [
        "Note that the classes are extremely unbalanced."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqxW_Hdg2qWB"
      },
      "outputs": [],
      "source": [
        "# Convert dataframe to numpy array\n",
        "heart_failure_data = df.to_numpy()\n",
        "\n",
        "# Create feature matrix with EF and GLS\n",
        "# We'll ignore the QRS data for this example\n",
        "X = heart_failure_data[:,[1,2]]\n",
        "\n",
        "# Create label vector\n",
        "y = heart_failure_data[:,0]\n",
        "\n",
        "# print properties\n",
        "print('Feature matrix X dimensions: ', X.shape)\n",
        "print('Target vector y dimensions: ', y.shape)\n",
        "print('Labels: ', np.unique(y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUQlAX752qWB"
      },
      "source": [
        "Note that we updated the function `PlotData` to deal with 3 classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-7P8NNw2qWC"
      },
      "outputs": [],
      "source": [
        "def PlotData(X,y):\n",
        "\n",
        "    # Plot class 0\n",
        "    plt.plot(X[y==0,0], X[y==0,1], 'bo', alpha=0.75, markeredgecolor='k', label = 'Healthy')\n",
        "\n",
        "    # Plot class 1\n",
        "    plt.plot(X[y==1,0], X[y==1,1], 'rd', alpha=0.75, markeredgecolor='k', label = 'Mild HF')\n",
        "\n",
        "    # Plot class 2\n",
        "    plt.plot(X[y==2,0], X[y==2,1], 'g^', alpha=0.75,markeredgecolor='k',label = 'Severe HF')\n",
        "\n",
        "    # Annotate the plot\n",
        "    plt.title('Diagnosis of Heart Failure')\n",
        "    plt.xlabel('EF')\n",
        "    plt.ylabel('GLS')\n",
        "    plt.legend()\n",
        "\n",
        "# Call the function to plot the dataset\n",
        "PlotData(X,y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMCqAzp-2qWC"
      },
      "source": [
        "## Standardize Data\n",
        "\n",
        "We'll use [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) to standardize the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALnaUE762qWC"
      },
      "outputs": [],
      "source": [
        "# Create an object to scale the features to have zero mean and unit variance\n",
        "# We don't need to do this for all models, but let's do it here to be consistent\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Create a feature matrix containing EF and GLS\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Plot the scaled data\n",
        "PlotData(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Up1mmNa2qWC"
      },
      "source": [
        "## Creating training and test sets\n",
        "\n",
        "We'll create training and test sets as in the previous example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DSnD4l62qWC"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "# Using a fixed random_state for consistency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CAZ4IcC2qWC"
      },
      "source": [
        "### Functions\n",
        "The cell below contains some of the functions that we have created in the previous notebook and which have been modified for multilabel classification. These functions are\n",
        "* `PlotDecisionBoundary`\n",
        "* `PlotConfidenceScores`\n",
        "* `PlotProbabilities`\n",
        "* `PlotSVCDecisionBoundary`\n",
        "* `EvaluateModel`\n",
        "\n",
        "Run the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Stk_HDIE2qWD"
      },
      "outputs": [],
      "source": [
        "def PlotProbabilities(model, X, y, label=1):\n",
        "\n",
        "    # Create an 1D array of samples for each feature\n",
        "    x1 = np.linspace(-2.5, 2, 1000)\n",
        "    x2 = np.linspace(-3, 3.5, 1000).T # note the transpose\n",
        "    # Creates 2D arrays that hold the coordinates in 2D feature space\n",
        "    x1, x2 = np.meshgrid(x1, x2)\n",
        "    # Flatten x1 and x2 to 1D vector and concatenate into a feature matrix\n",
        "    Feature_space = np.c_[x1.ravel(), x2.ravel()]\n",
        "\n",
        "    # Predict labels for the whole feature space\n",
        "    # Note the predict_proba function! This gives us the probabilities\n",
        "    proba = model.predict_proba(Feature_space)\n",
        "\n",
        "    # Select the class\n",
        "    p = proba[:, label]\n",
        "\n",
        "    # Reshape to 2D\n",
        "    p = p.reshape(x1.shape)\n",
        "\n",
        "    # Plot using contourf\n",
        "    plt.contourf(x1, x2, p, cmap = 'summer')\n",
        "\n",
        "    # Add colorbar\n",
        "    plt.colorbar()\n",
        "\n",
        "    # Plot data\n",
        "    PlotData(X, y)\n",
        "\n",
        "def EvaluateModel(model, X_train, X_test, y_train, y_test):\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    y_test_pred = model.predict(X_test)\n",
        "    cm = confusion_matrix(y_test, y_test_pred)\n",
        "    sns.heatmap(cm, annot=True)\n",
        "    plt.gcf().set_size_inches(2, 2)\n",
        "    plt.show()\n",
        "    print('Classification report for training set:')\n",
        "    print(classification_report(y_train, y_train_pred, zero_division=0.0))\n",
        "    print('Classfication report for test set:')\n",
        "    print(classification_report(y_test, y_test_pred, zero_division=0.0))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpLq82BI2qWD"
      },
      "source": [
        "### Perceptron\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJjapocP2qWD"
      },
      "outputs": [],
      "source": [
        "#p_model = Perceptron(max_iter=100, eta0=0.2)\n",
        "#p_model.fit(X_train, y_train)\n",
        "#EvaluateModel(p_model, X_train, X_test, y_train, y_test)\n",
        "#PlotConfidenceScores(model, X, y, label=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUx2jrIo2qWD"
      },
      "source": [
        "### Logistic Regression\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKQSjU9m2qWD"
      },
      "outputs": [],
      "source": [
        "logreg_model = LogisticRegression(solver='lbfgs', multi_class='multinomial')\n",
        "logreg_model.fit(X_train, y_train)\n",
        "EvaluateModel(logreg_model, X_train, X_test, y_train, y_test)\n",
        "plt.figure(figsize=(15,3.5))\n",
        "classes = ['Healthy', 'Mild Heart Failure', 'Severe Heart Failure']\n",
        "for i in range(3):\n",
        "    plt.subplot(1, 3, i+1)\n",
        "    PlotProbabilities(logreg_model, X, y, label=i)\n",
        "    plt.title(classes[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YVBrk5u2qWD"
      },
      "source": [
        "### Linear SVC\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3HSPAc7q2qWD"
      },
      "outputs": [],
      "source": [
        "#linearsvc_model = LinearSVC()\n",
        "#linearsvc_model.fit(X_train, y_train)\n",
        "#EvaluateModel(p_model, X_train, X_test, y_train, y_test)\n",
        "#plt.figure(figsize=(15,3.5))\n",
        "#classes = ['Healthy','Mild Heart Failure', 'Severe Heart Failure']\n",
        "#for i in range(3):\n",
        "#    plt.subplot(1, 3, i+1)\n",
        "#    PlotSVCDecisionBoundary(linearsvc_model, X, y, plotSV=False, plotDF=True)\n",
        "#    plt.title(classes[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVa167wL2qWD"
      },
      "source": [
        "## Support Vector Classifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCjOVrnz2qWD"
      },
      "outputs": [],
      "source": [
        "svc_model = SVC(kernel='linear', probability=True)\n",
        "svc_model.fit(X_train, y_train)\n",
        "EvaluateModel(svc_model, X_train, X_test, y_train, y_test)\n",
        "plt.figure(figsize=(15,3.5))\n",
        "classes = ['Healthy', 'Mild Heart Failure', 'Severe Heart Failure']\n",
        "for i in range(3):\n",
        "    plt.subplot(1, 3, i+1)\n",
        "    PlotProbabilities(svc_model, X, y, label=i)\n",
        "    plt.title(classes[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBOgshD12qWE"
      },
      "source": [
        "## Support Vector Classifier with Kernel Trick\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJC2UhMe2qWE"
      },
      "outputs": [],
      "source": [
        "kernelsvc_model = SVC(kernel='rbf', probability=True)\n",
        "kernelsvc_model.fit(X_train, y_train)\n",
        "EvaluateModel(kernelsvc_model, X_train, X_test, y_train, y_test)\n",
        "plt.figure(figsize=(15,3.5))\n",
        "classes = ['Healthy', 'Mild Heart Failure', 'Severe Heart Failure']\n",
        "for i in range(3):\n",
        "    plt.subplot(1, 3, i+1)\n",
        "    PlotProbabilities(kernelsvc_model, X, y, label=i)\n",
        "    plt.title(classes[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JGnmw9J2qWE"
      },
      "source": [
        "## Decision Tree\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5PZxMlFu2qWE"
      },
      "outputs": [],
      "source": [
        "tree_model = DecisionTreeClassifier(max_depth=2)\n",
        "tree_model.fit(X_train, y_train)\n",
        "EvaluateModel(tree_model, X_train, X_test, y_train, y_test)\n",
        "plt.figure(figsize=(15,3.5))\n",
        "classes = ['Healthy', 'Mild Heart Failure', 'Severe Heart Failure']\n",
        "for i in range(3):\n",
        "    plt.subplot(1, 3, i+1)\n",
        "    PlotProbabilities(tree_model, X, y, label=i)\n",
        "    plt.title(classes[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxMdSQ7p2qWE"
      },
      "source": [
        "## Random Forest\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lq8VyQv2qWE"
      },
      "outputs": [],
      "source": [
        "forest_model = RandomForestClassifier(n_estimators=100, max_depth=2)\n",
        "forest_model.fit(X_train, y_train)\n",
        "EvaluateModel(forest_model, X_train, X_test, y_train, y_test)\n",
        "plt.figure(figsize=(15,3.5))\n",
        "classes = ['Healthy', 'Mild Heart Failure', 'Severe Heart Failure']\n",
        "for i in range(3):\n",
        "    plt.subplot(1, 3, i+1)\n",
        "    PlotProbabilities(forest_model, X, y, label=i)\n",
        "    plt.title(classes[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50MP6tL52qWE"
      },
      "source": [
        "## Exercise: Balanced multi-label classification (optional)\n",
        "\n",
        "One way to deal with class imbalance is to increase the weight of the samples of smaller classes in the classification loss. For `LogisticRegression` this can be done by setting `class_weight='balanced'`, which will weight the sample inversly to the class size, this giving equal importance to each class irrespective of its size.\n",
        "\n",
        "In this exercise you will fit the logistic regression with class weighting and analyse the performance of the balanced model.\n",
        "\n",
        "### Task: Fit and plot the balanced model\n",
        "\n",
        "Fit the logistic regression with `class_weight='balanced'` and plot the classification boundaries/labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akSxMQER2qWE"
      },
      "outputs": [],
      "source": [
        "# Add code here"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}