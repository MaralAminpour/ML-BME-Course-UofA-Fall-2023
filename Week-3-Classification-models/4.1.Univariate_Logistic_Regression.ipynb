{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaralAminpour/ML-BME-Course-UofA-Fall-2023/blob/main/Week-3-Classification-models/4.1.Univariate_Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SD3yYKUkqzzq"
      },
      "source": [
        "# Univariate Logistic Regression\n",
        "\n",
        "In this notebook we will review the main concepts of linear binary classification using an example of logistic regression. We will look at the univariate case - prediction of **heart failure** from a single feature, **Ejection Fraction**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byRxieJwqzzr"
      },
      "source": [
        "## Prepare dataset\n",
        "\n",
        "The code below\n",
        "* loads all the important libraries\n",
        "* implements function `accuracyCV` to calculate classification accuracy using cross-validation\n",
        "* implements function `plotData` to plot the dataset\n",
        "* creates feature matrix `X` that contains **Ejection Fraction**\n",
        "* creates label vector `y` that contains labels **Healthy=0** and **Heart Failure=1**\n",
        "\n",
        "Run this code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAIC6TAUqzzs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Calculate cross-validated accuracy\n",
        "def accuracyCV(model, X, y):\n",
        "    scores = cross_val_score(model, X, y, cv=5, scoring=\"accuracy\")\n",
        "    print('Mean cross-validated accuracy: ',round(scores.mean(),2))\n",
        "\n",
        "# Plot data\n",
        "def plotData(X,y,adjust_axis = True):\n",
        "    plt.figure(figsize=(7,3))\n",
        "    plt.plot(X[y==0], y[y==0], \"bo\", alpha=0.75, markeredgecolor='k', label = 'Healthy')\n",
        "    plt.plot(X[y==1], y[y==1], \"rd\", alpha=0.75, markeredgecolor='k', label = 'Heart Failure')\n",
        "    plt.plot(np.linspace(-2.5,2,100),np.zeros(100),'k:',alpha=0.5)\n",
        "\n",
        "    plt.xlabel('Feature: Scaled Ejection Fraction')\n",
        "    plt.ylabel('Label: Healthy=0, Heart Failure=1')\n",
        "    plt.legend()\n",
        "\n",
        "    if adjust_axis:\n",
        "        plt.axis([-2.5,2,-0.1,1.1])\n",
        "\n",
        "# This code will download the required data files from GitHub\n",
        "import requests\n",
        "def download_data(source, dest):\n",
        "    base_url = 'https://raw.githubusercontent.com/'\n",
        "    owner = 'SirTurtle'\n",
        "    repo = 'ML-BME-UofA-data'\n",
        "    branch = 'main'\n",
        "    url = '{}/{}/{}/{}/{}'.format(base_url, owner, repo, branch, source)\n",
        "    r = requests.get(url)\n",
        "    f = open(dest, 'wb')\n",
        "    f.write(r.content)\n",
        "    f.close()\n",
        "\n",
        "# Create the temp directory, if it doesn't already exist\n",
        "import os\n",
        "if not os.path.exists('temp'):\n",
        "   os.makedirs('temp')\n",
        "\n",
        "# Download the data\n",
        "download_data('Week-3-Classification-models/data/heart_failure_data.csv', 'temp/heart_failure_data.csv')\n",
        "\n",
        "#Load dataset\n",
        "df = pd.read_csv('temp/heart_failure_data.csv')\n",
        "print('Dataframe columns: ', df.keys())\n",
        "scaler = StandardScaler()\n",
        "data = df.to_numpy()\n",
        "X = scaler.fit_transform(data[:,0].reshape(-1,1)) # Ejection Fraction\n",
        "print('Feature matrix X dimensions: ', X.shape)\n",
        "y = data[:,2] # Heart Failure\n",
        "print('Target vector y dimensions: ', y.shape)\n",
        "plotData(X,y)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Refresher on What We've Learned So Far on linear classifier and Logostic regression.\n",
        "\n",
        "**Linear binary classification**\n",
        "\n",
        "Linear binary classification is a type of classification algorithm that is used to separate data into two classes (hence \"binary\") based on a linear combination of the features of the data (hence \"linear\")\n",
        "\n",
        "**Linear decision function**  \n",
        "\n",
        "The linear decision function is a mathematical equation that helps in distinguishing between two classes based on a set of features or inputs.\n",
        "\n",
        "$$\n",
        "f(\\mathbf{x}) = \\mathbf{w}^T \\mathbf{x} + b\n",
        "$$\n",
        "\n",
        "or\n",
        "\n",
        "$$\n",
        "f(\\mathbf{x}) = w_1 x_1 + w_2 x_2 + \\ldots + w_n x_n + b\n",
        "$$\n",
        "\n",
        "We have the feature vector, $\\mathbf{x}$, and the weight vector, $\\mathbf{w}$, defined as:\n",
        "\n",
        "$$\n",
        "\\mathbf{x} = (1, x_1, x_2, \\ldots, x_N)^T\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\mathbf{w} = (w_0, w_1, w_2, \\ldots, w_N)^T\n",
        "$$\n",
        "\n",
        "**Decision boundary**\n",
        "\n",
        "Decision boundary is typically where the decision function equals zero:\n",
        "\n",
        "$$ f(\\mathbf{x}) = 0 $$.\n",
        "\n",
        "**Classifying the data points**:\n",
        "\n",
        "Depending on the sign of $f(\\mathbf{x})$, the data point $\\mathbf{x}$ is classified into one of the two classes. Typically:\n",
        "   - If $f(\\mathbf{x}) > 0$, we predict the label as 1.\n",
        "   - If $f(\\mathbf{x}) \\leq 0$, we predict the label as 0.\n",
        "\n",
        "**Logistic regression calssifier**\n",
        "\n",
        "An advantage** of the logistic regression classifier is that the **output of the model is a probability.\n",
        "\n",
        "This function squashes the output of decision function into rage [0,1].\n",
        "\n",
        "Sigmoid Function**\n",
        "\n",
        "The output of the decision function is then transformed using the sigmoid function to yield a probability value. The sigmoid function $( \\sigma $) is defined as:\n",
        "\n",
        "\n",
        "$$\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "$$\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-3-Classification-models/imgs/LogisticRegression-fig1.png\" width = \"300\" style=\"float: right;\">\n",
        "\n",
        "\n",
        "So, applying this to our decision function, we get:\n",
        "\n",
        "$$\n",
        "P(y=1|\\mathbf{x}) = \\sigma(f(\\mathbf{x})) = \\sigma(\\mathbf{w}^T \\mathbf{x} + b) = \\frac{1}{1 + e^{-(\\mathbf{w}^T \\mathbf{x} + b)}}\n",
        "$$\n",
        "\n",
        "**Probabilities of Each Class**\n",
        "\n",
        "Using the sigmoid function, we can determine the probability that a given input ($ \\mathbf{x} $) belongs to class 1 ($ y = 1 $) or class 0 ($ y = 0 $):\n",
        "\n",
        "- **Probability of $y = 1 $ given $ \\mathbf{x} $**\n",
        "  \n",
        "  We already derived this above:\n",
        "\n",
        "  $$\n",
        "  P(y=1|\\mathbf{x}) = \\frac{1}{1 + e^{-(\\mathbf{w}^T \\mathbf{x} + b)}}\n",
        "  $$\n",
        "\n",
        "- **Probability of $ y = 0 $ given $ \\mathbf{x} $**\n",
        "  \n",
        "  This can be computed as one minus the probability of $ y = 1 $:\n",
        "\n",
        "  $$\n",
        "  P(y=0|\\mathbf{x}) = 1 - P(y=1|\\mathbf{x}) = 1 - \\frac{1}{1 + e^{-(\\mathbf{w}^T \\mathbf{x} + b)}}\n",
        "  $$\n",
        "\n"
      ],
      "metadata": {
        "id": "wMcqGlZtq2M6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRcc_cfOqzzs"
      },
      "source": [
        "## Exercise 1: Logistic regression model\n",
        "\n",
        "### Fit the model\n",
        "\n",
        "The code below\n",
        "* Selects the ```LogisticRegression``` model\n",
        "* Fits the model using all features and labels\n",
        "* Calculates the accuracy on whole set and using cross-validation\n",
        "\n",
        "Run the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8joJZHXqzzt"
      },
      "outputs": [],
      "source": [
        "# Select the logistic regression model\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X,y)\n",
        "\n",
        "# Calculate the accuracy on the whole set\n",
        "print('Accuracy = ',round(model.score(X,y),2))\n",
        "\n",
        "# Calculate cross-validated accuracy\n",
        "accuracyCV(model,X,y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dF8BI6Dvqzzt"
      },
      "source": [
        "### Task 1: Plot decision function\n",
        "\n",
        "The **decision function** for univariate classifier is defined as $h=w_0+w_1x$. The fitted classifiers in scikit-learn return the values of the decision function using `model.decision_function`.\n",
        "\n",
        "We will now plot the decision function of the fitted Logistic Regression model. Fist we will generate the feature space `x` for scaled EF, which covers interval [-2.5,2] as visible from the plot above.\n",
        "\n",
        "Your task is to\n",
        "* predict the decision function values `df` on the feature space `x`\n",
        "* plot the decision function\n",
        "\n",
        "The code for plotting the data is already there for you. Notices the output values of the decision function and the change of the scale in y-coordinate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HKxbypNqzzt"
      },
      "outputs": [],
      "source": [
        "# Generate feature space\n",
        "x = np.linspace(-2.5,2,100).reshape(-1,1)\n",
        "# Predict decision function\n",
        "df = None\n",
        "# Plot data\n",
        "plotData(X,y,adjust_axis=False)\n",
        "# Plot decision function\n",
        "plt.plot(None,None,'k', label = 'Decision function')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0VZ4R5Cqzzt"
      },
      "source": [
        "### Task 2: Plot decision boundary\n",
        "\n",
        "The **decision boundary**  of a linear classifier is defined by the setting the decision function to zero. In univariate case it is $w_0+w_1x=0$.\n",
        "\n",
        "Your task us to\n",
        "* Find the intercept $w_0$ and slope $w_1$\n",
        "* Calculate the decision boundary as $x=\\frac{-w_0}{w_1}$.\n",
        "\n",
        "*Hint:* Use `model.intercept_` and `model.coef_`. Remember that they are numpy arrays, so you need to select the first element"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LoyCgBbqzzu"
      },
      "outputs": [],
      "source": [
        "# intercept is a 1D array\n",
        "print('Intercept: ', np.around(model.intercept_,2))\n",
        "# coeficients are a 2D array\n",
        "print('Coefficients: ', np.around(model.coef_,2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAplS7miqzzu"
      },
      "outputs": [],
      "source": [
        "# extract weights\n",
        "w0 = None\n",
        "w1 = None\n",
        "print('w0 = ', round(w0,2))\n",
        "print('w1 = ', round(w1,2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFo8S8Xcqzzu"
      },
      "outputs": [],
      "source": [
        "# calculate decision boundary - note it is a scalar\n",
        "decision_boundary = None\n",
        "print('Decision boundary is ', round(decision_boundary,2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnBV3T1rqzzv"
      },
      "outputs": [],
      "source": [
        "# plot the data\n",
        "plotData(X,y,adjust_axis=False)\n",
        "\n",
        "# plot the decision boundary\n",
        "plt.plot([decision_boundary,decision_boundary],[-0.1,1.1], 'k', label = 'Decision boundary')\n",
        "_=plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQ-whAs9qzzv"
      },
      "source": [
        "### Task 3: Plot probabilitic predictions\n",
        "\n",
        "In binary logistic regression the confidence in predicted labels can be evaluated as $p(y=1|x)=\\sigma(h(x))$ and $p(y=0|x)=1-\\sigma(h(x))$. The probabilistic predictions are evaluated using function `model.predict_proba`.\n",
        "\n",
        "Your task is to calculate probabilistic predictions `proba` for the whole feature space and plot them. Check the dimensions of `proba`, you will see that the function returns probabilities for both classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-xSstBzqzzv"
      },
      "outputs": [],
      "source": [
        "# calculate probabilistic predictions\n",
        "proba = None\n",
        "# check the dimension\n",
        "print('Dimensions of proba: ', proba.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jr2bRJIGqzzv"
      },
      "outputs": [],
      "source": [
        "# plot data\n",
        "plotData(X,y)\n",
        "\n",
        "# plot probabilistic predictions\n",
        "plt.plot(x,proba[:,0],'b:', label = 'Healthy Probability')\n",
        "plt.plot(x,proba[:,1],'r-', label = 'Heart Failure Probability')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gq38XtpUqzzv"
      },
      "source": [
        "### Task 4: Diagnosis\n",
        "\n",
        "A patient with suspicion of heart failure has an MRI scan. The ejection fraction is 40%. Decide whether the patient is likely to have a heart failure and calculate confidence in this decision.\n",
        "\n",
        "**Answer:** None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vef9K9HEqzzv"
      },
      "outputs": [],
      "source": [
        "# EF=40%\n",
        "x_new=40\n",
        "\n",
        "# convert the feature to a 2D numpy array\n",
        "# scale it using already fitted scaler\n",
        "x_new = scaler.transform(np.array(x_new).reshape(-1,1))\n",
        "print('Scaled EF: ', np.around(x_new,2))\n",
        "\n",
        "# predict the label\n",
        "y_new = None\n",
        "print('Predicted label: ', y_new)\n",
        "\n",
        "# predict confidence\n",
        "proba_new = None\n",
        "print('Predicted confidence: ', round(proba_new[0,y_new[0].astype(int)],2))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}