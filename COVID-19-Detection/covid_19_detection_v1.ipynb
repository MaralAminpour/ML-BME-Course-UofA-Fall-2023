{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/MaralAminpour/ML-BME-UofA/blob/main/Week-11-COVID-chest-x-ray/deep_learning_covid19_v4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cF4tH3wGGjQF"
   },
   "source": [
    "# COVID-19 Chest X-Ray Image Classification\n",
    "\n",
    "We will use this Kaggle dataset: https://www.kaggle.com/datasets/tawsifurrahman/covid19-radiography-database\n",
    "\n",
    "Our objective will be to train a multiclass model to classify chest x-ray images into 4 classes:\n",
    "- COVID-19\n",
    "- Normal (healthy)\n",
    "- Non-COVID infection\n",
    "- Viral pneumonia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wPFVcSGbIIvK"
   },
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QVbw71w8GjQG"
   },
   "source": [
    "The following commands can be used to install missing packages. You may need not need to use these, or may need different commands, depending on your installation. You won't need to use these on Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lEQ1rKjOGjQG"
   },
   "outputs": [],
   "source": [
    "#!pip install torchvision\n",
    "#!pip install opencv-python\n",
    "\n",
    "#With Anaconda install the packages torchvision and py-opencv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0BzlBerkGjQG"
   },
   "source": [
    "Next will run all the imports we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lpxtcw92kSnb",
    "outputId": "75d21fdc-f520-49c7-e97d-ad9fd3963de9"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import copy\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tqdm.notebook as tqdm\n",
    "from PIL import Image\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "print('Using PyTorch version', torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R-lpfLQtGjQH"
   },
   "source": [
    "Finally, we will set the device variable with a CUDA device if one is available, otherwise we'll use GPU. On Colab you can change the runtime type to T4 GPU if you want to use a GPU device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bNJxypImGjQI",
    "outputId": "e4bf2c6c-8487-4e07-d8d7-37f0490aed2d"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nb3bxLuMILew"
   },
   "source": [
    "# Preprocess dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OLb8WLapGjQI"
   },
   "source": [
    "I've made a duplicate in GitHub of the Kaggle dataset here: https://www.kaggle.com/datasets/tawsifurrahman/covid19-radiography-database\n",
    "\n",
    "This code will let you get all the data into Colab or other environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Taghi0IhGFkB",
    "outputId": "5557c9ed-c018-425e-803a-249099d5db4e"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/SirTurtle/COVID-19_Radiography_Dataset.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "brk3NlRPGjQJ"
   },
   "source": [
    "The following code uses the CV2 library to apply the masks to the images to generate new images with only the lung segments. The rest of the image will be black."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GjA_mHqvGjQJ"
   },
   "outputs": [],
   "source": [
    "dataset_path = 'COVID-19_Radiography_Dataset'\n",
    "dataset_labels = ['COVID', 'Lung_Opacity', 'Normal', 'Viral Pneumonia']\n",
    "segmented_dataset_path = 'Segmented_Dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4T_O6bSEGjQJ"
   },
   "outputs": [],
   "source": [
    "os.makedirs(segmented_dataset_path, exist_ok=True)\n",
    "for label in dataset_labels:\n",
    "    os.makedirs(os.path.join(segmented_dataset_path, label), exist_ok=True)\n",
    "for label in dataset_labels:\n",
    "    image_dir_path = os.path.join(dataset_path, label, 'images')\n",
    "    mask_dir_path = os.path.join(dataset_path, label, 'masks')\n",
    "    image_files = os.listdir(image_dir_path)\n",
    "    for image_file in image_files:\n",
    "        if not image_file.endswith('.png'):\n",
    "            continue\n",
    "        image_file_path = os.path.join(image_dir_path, image_file)\n",
    "        mask_file_path = os.path.join(mask_dir_path, image_file)\n",
    "        image = cv2.imread(image_file_path)\n",
    "        image = cv2.resize(image, (256,256))\n",
    "        mask = cv2.imread(mask_file_path)\n",
    "        masked_image = image\n",
    "        mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
    "        (thresh, mask) = cv2.threshold(mask, 128, 255, cv2.THRESH_BINARY)\n",
    "        masked_image = cv2.bitwise_and(image, image, mask=mask)\n",
    "        masked_image_path = os.path.join(segmented_dataset_path, label, image_file)\n",
    "        cv2.imwrite(masked_image_path, masked_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fVDYY9BnGjQJ"
   },
   "source": [
    "Next we'll check the balance between the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "id": "kYDD3FS6GjQJ",
    "outputId": "bf251636-8faa-47e5-dd49-048c91b671ea"
   },
   "outputs": [],
   "source": [
    "image_count = {}\n",
    "for label in dataset_labels:\n",
    "    image_count[label] = len(os.listdir(os.path.join(segmented_dataset_path, label)))\n",
    "\n",
    "# Plotting the distribution of each class\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(image_count.values(),\n",
    "        labels = image_count.keys(),\n",
    "        shadow = True,\n",
    "        autopct = '%1.1f%%',\n",
    "        startangle = 90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cC9JMC9gGjQJ"
   },
   "source": [
    "Note that the \"Lung_Opacity\" class is non-COVID infection samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iDZcvh1dJWz_"
   },
   "source": [
    "# Image Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aV8YibomGjQJ"
   },
   "source": [
    "We will apply image transformations to our data using Torchvision. This will include our data augmentation, conversion to tensor and normalization.\n",
    "\n",
    "The normalization values are selected to match the normalization applied for our pretrained weights. They are obtained from the documentation here:\n",
    "https://pytorch.org/vision/stable/models/generated/torchvision.models.resnet18.html#torchvision.models.ResNet18_Weights We are also resizing to 224 x 224 (in the pretrained model images are cropped to these dimensions, but I don't think cropping is appropriate for our data).\n",
    "\n",
    "We will follow roughly the data augmentation used by Chowdhury et al.: rotation and translation. For rotation I set a max of 10 degrees (between 5 and 15 is what I saw used in different articles) and for translation 5%. Other transformations I've seen used are RandomHorizontalFlip but I decided not to use this transformation. The data augmentation will be applied only to the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7c8Nnj_TlWG9"
   },
   "outputs": [],
   "source": [
    "means = [0.485, 0.456, 0.406]\n",
    "stds = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(size=(224, 224)),\n",
    "    torchvision.transforms.RandomAffine(degrees=10, translate=(0.05,0.05)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=means, std=stds)\n",
    "])\n",
    "\n",
    "validation_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(size=(224, 224)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=means, std=stds)\n",
    "])\n",
    "\n",
    "test_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(size=(224, 224)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=means, std=stds)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fs3TQ76pGjQJ"
   },
   "source": [
    "# Create training, validation and test sets\n",
    "\n",
    "We will create Torchvision DataLoader objects for each of the training, validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RURnRVzGGjQK",
    "outputId": "e0d7f4cd-5267-48b3-a974-f53f9d655f3d"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "\n",
    "def load_split_train_validation_test(path, validation_size=.1, test_size=.1):\n",
    "    train_data = torchvision.datasets.ImageFolder(path,\n",
    "                    transform=train_transform)\n",
    "    validation_data = torchvision.datasets.ImageFolder(path,\n",
    "                    transform=validation_transform)\n",
    "    test_data = torchvision.datasets.ImageFolder(path,\n",
    "                    transform=test_transform)\n",
    "    num_train = len(train_data)\n",
    "    indices = list(range(num_train))\n",
    "    split_1 = int(np.floor((1 - validation_size - test_size) * num_train))\n",
    "    split_2 = int(np.floor((1 - test_size) * num_train))\n",
    "    np.random.shuffle(indices)\n",
    "    train_idx, validation_idx, test_idx = indices[0:split_1], indices[split_1:split_2], indices[split_2:]\n",
    "    dataset_size = {'train':len(train_idx), 'validation':len(validation_idx), 'test':len(test_idx)}\n",
    "    train_sampler = torch.utils.data.sampler.SubsetRandomSampler(train_idx)\n",
    "    validation_sampler = torch.utils.data.sampler.SubsetRandomSampler(validation_idx)\n",
    "    test_sampler = torch.utils.data.sampler.SubsetRandomSampler(test_idx)\n",
    "    train_loader = torch.utils.data.DataLoader(train_data,\n",
    "                   sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "    validation_loader = torch.utils.data.DataLoader(validation_data,\n",
    "                   sampler=validation_sampler, batch_size=BATCH_SIZE)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data,\n",
    "                   sampler=test_sampler, batch_size=1)\n",
    "    return train_loader, validation_loader, test_loader, dataset_size\n",
    "\n",
    "train_loader, validation_loader, test_loader, dataset_size = \\\n",
    "    load_split_train_validation_test(segmented_dataset_path, .1, .1)\n",
    "print(dataset_size)\n",
    "data_loaders = {'train':train_loader, 'validation':validation_loader, 'test':test_loader}\n",
    "data_sizes = {x: len(data_loaders[x].sampler) for x in ['train', 'validation', 'test']}\n",
    "class_names = train_loader.dataset.classes\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cdsCrK6rJlZR"
   },
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Et_qomOGjQK"
   },
   "source": [
    "Let's visualize our dataset now. We'll create a show_images function that will visualize a random batch of our dataset. This function will allow us to both visualize the images and to display the true and predicted labels. Since we don't have predicted labels yet we'll just use the true labels for both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113
    },
    "id": "cqPGyFFNmxeC",
    "outputId": "fa6e6765-882d-4db8-d754-5e9f1a8e6a16"
   },
   "outputs": [],
   "source": [
    "def show_images(images, labels, preds):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    for i, image in enumerate(images):\n",
    "        plt.subplot(1, BATCH_SIZE, i + 1, xticks=[], yticks=[])\n",
    "        image = image.to(torch.device('cpu'))\n",
    "        labels = labels.to(torch.device('cpu'))\n",
    "        preds = labels.to(torch.device('cpu'))\n",
    "        image = image.numpy().transpose((1, 2, 0))\n",
    "        mean = np.array(means)\n",
    "        std = np.array(stds)\n",
    "        image = image * std + mean\n",
    "        image = np.clip(image, 0., 1.)\n",
    "        plt.imshow(image)\n",
    "        col = 'green'\n",
    "        if preds[i] != labels[i]:\n",
    "            col = 'red'\n",
    "        plt.xlabel(f'{class_names[int(labels[i].numpy())]}')\n",
    "        plt.ylabel(f'{class_names[int(preds[i].numpy())]}', color=col)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "images, labels = next(iter(train_loader))\n",
    "show_images(images, labels, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113
    },
    "id": "WvV-tEimJ5ou",
    "outputId": "fad53bbb-ef54-4bad-9494-816619a2012c"
   },
   "outputs": [],
   "source": [
    "images, labels = next(iter(validation_loader))\n",
    "show_images(images, labels, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g7PiROQwLUK3"
   },
   "source": [
    "# Creating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CfAs9qS2GjQK"
   },
   "source": [
    "Now let's create our initial model. We'll be using resnet18. This is the version of . We will also load the pre-trained weights on the ImageNet1K dataset.\n",
    "\n",
    "We will only modify the final layer for our 4 class problem. We won't be freezing any of the parameters.\n",
    "\n",
    "If we want to use a different model, e.g. densenet121 or ChexNet, we would need to edit this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A5peMLcqKNgP",
    "outputId": "42b40d14-7732-4018-bbca-6011569a85da"
   },
   "outputs": [],
   "source": [
    "resnet18 = torchvision.models.resnet18(weights='ResNet18_Weights.IMAGENET1K_V1')\n",
    "print(resnet18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s-gY2L7QLA3s"
   },
   "outputs": [],
   "source": [
    "resnet18.fc = torch.nn.Linear(in_features=512, out_features=len(class_names))\n",
    "resnet18 = resnet18.to(device)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(resnet18.parameters(), lr=1e-5)\n",
    "_ = resnet18.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u9PY8Uv6GjQK",
    "outputId": "3ff796ca-694d-4839-989f-da41398cb8df"
   },
   "outputs": [],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in resnet18.parameters() if p.requires_grad)\n",
    "print(\"Number of trainable parameters: \\n{}\".format(pytorch_total_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pBPwaSB9GjQL"
   },
   "source": [
    "Let's also show the predictions of the untrained model. We'll use function to monitor our model during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YAnZicSULClx"
   },
   "outputs": [],
   "source": [
    "def show_preds(model, images, labels):\n",
    "    model.eval()\n",
    "    outputs = model(images)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    show_images(images, labels, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6m-RirqTLV9Y"
   },
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "blvMiiHVGjQL"
   },
   "source": [
    "We will train our model for 1 epoch, just so this example will run quickly.\n",
    "\n",
    "We will use our validation set to implement an \"early stopping\" procedure. In this case, we will save the model if the loss on the validation set is improved compared with the previous epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "do2eP7SILN62"
   },
   "outputs": [],
   "source": [
    "def train(model, epochs):\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = np.inf\n",
    "\n",
    "    print('Starting training..')\n",
    "\n",
    "    for e in range(0, epochs):\n",
    "\n",
    "        print(f'Starting epoch {e + 1}/{epochs}')\n",
    "\n",
    "        val_loss = 0.\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for train_step, (images, labels) in enumerate(train_loader):\n",
    "\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if train_step % 100 == 0:\n",
    "                print('Running batch {}'.format(train_step))\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        for val_step, (images, labels) in enumerate(validation_loader):\n",
    "\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            val_loss += loss.item() * images.size(0) # Multiplying by batch size\n",
    "\n",
    "            val_loss /= (val_step + 1)\n",
    "\n",
    "            if val_step % 100 == 0:\n",
    "                show_preds(model, images, labels)\n",
    "\n",
    "        epoch_loss = val_loss / data_sizes['validation'] # Dividing by total validation set size\n",
    "\n",
    "        if epoch_loss < best_loss:\n",
    "            print('Validation loss decreased from {:.3f} to {:.3f}'.format(best_loss, epoch_loss))\n",
    "            print('...Saving weights')\n",
    "            best_loss = epoch_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    print('Training complete..')\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "lD5WNehqL6Ab",
    "outputId": "9e00f50a-1911-4c9f-dc8c-d9edab03a5cd"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "resnet18 = train(resnet18, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ooJnNvqMMOQL"
   },
   "source": [
    "# Final Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "paU1g2m4GjQP"
   },
   "source": [
    "# Evaluating the Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CKJrNz6fGjQP"
   },
   "source": [
    "Next, we print the classification report and display the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 641,
     "referenced_widgets": [
      "34c861f157b04736b95e70f442874962",
      "188f205f72364eca9046e9489d2e941a",
      "7c92f103d4bb4f4c9dab38632beb558f",
      "cf544c18a5b84d4699bacf88103d10ca",
      "1a90e6228c8345d7bec3de414ea3de20",
      "11c588d923114d5eb87177eaca3fef76",
      "761cfa930aee4513b9987f10118a0efc",
      "ea2bb7962c144cc1a5808f3ca8040eaf",
      "c974c193620c495da828bd5e268e9ce1",
      "7708ef0dcdf44ec1acee8a979200ca04",
      "a68b76c42da547a4b40862061d13ec53"
     ]
    },
    "id": "1XZiyPigGjQP",
    "outputId": "d26426d4-22b9-47ae-fb85-bbaf23001b0e"
   },
   "outputs": [],
   "source": [
    "# Calculate the predicted and true labels\n",
    "\n",
    "y_pred_list = []\n",
    "y_true_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in tqdm.tqdm(test_loader, leave=False):\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        y_test_pred = resnet18(x_batch)\n",
    "        y_test_pred = torch.log_softmax(y_test_pred, dim=1)\n",
    "        _, y_pred_tag = torch.max(y_test_pred, dim = 1)\n",
    "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "        y_true_list.append(y_batch.cpu().numpy())\n",
    "\n",
    "# Calculate and print the classification report\n",
    "\n",
    "print(classification_report(y_true_list, y_pred_list))\n",
    "\n",
    "# Display the confusion matrix\n",
    "\n",
    "heatmap = sns.heatmap(confusion_matrix(y_true_list, y_pred_list), annot=True, fmt='g')\n",
    "heatmap.yaxis.set_ticklabels(class_names)\n",
    "heatmap.xaxis.set_ticklabels(class_names)\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__XXfJd7b28L"
   },
   "source": [
    "# Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sobgClSDcFx-"
   },
   "outputs": [],
   "source": [
    "torch.save(resnet18.state_dict(), 'covid_classifier_resnet18.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "10S3BZZ1cLrC"
   },
   "source": [
    "# Inference on a Single Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F-x3A0rTcOtJ"
   },
   "outputs": [],
   "source": [
    "# Load the model and set in eval\n",
    "resnet18 = torchvision.models.resnet18(weights='ResNet18_Weights.IMAGENET1K_V1')\n",
    "resnet18.fc = torch.nn.Linear(in_features=512, out_features=4)\n",
    "\n",
    "resnet18.load_state_dict(torch.load('covid_classifier_resnet18.pt'))\n",
    "resnet18.eval()\n",
    "\n",
    "def predict_image_class(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    image = test_transform(image)\n",
    "    image = image.unsqueeze(0)\n",
    "    image = image.to(torch.device('cpu'))\n",
    "    output = resnet18(image)[0]\n",
    "    probabilities = torch.nn.Softmax(dim=0)(output)\n",
    "    probabilities = probabilities.cpu().detach().numpy()\n",
    "    predicted_class_index = np.argmax(probabilities)\n",
    "    predicted_class_name = class_names[predicted_class_index]\n",
    "    return probabilities, predicted_class_index, predicted_class_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DgWF-TiGcTum",
    "outputId": "4082e068-28fd-4db7-bb70-ea6fc8e7de84"
   },
   "outputs": [],
   "source": [
    "image_path = 'Segmented_Dataset/COVID/COVID-1.png'\n",
    "probabilities, predicted_class_index, predicted_class_name = predict_image_class(image_path)\n",
    "print('Probabilities:', probabilities)\n",
    "print('Predicted class index:', predicted_class_index)\n",
    "print('Predicted class name:', predicted_class_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vGczE8gSGjQQ",
    "outputId": "718d6869-bc32-4601-b998-a4ff59f43b6a"
   },
   "outputs": [],
   "source": [
    "image_path = 'Segmented_Dataset/Lung_Opacity/Lung_Opacity-1.png'\n",
    "#image_path = 'covid_data_small/test/viral/Viral_Pneumonia-201.png'\n",
    "\n",
    "probabilities, predicted_class_index, predicted_class_name = predict_image_class(image_path)\n",
    "print('Probabilities:', probabilities)\n",
    "print('Predicted class index:', predicted_class_index)\n",
    "print('Predicted class name:', predicted_class_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qGdFGVOoGjQQ",
    "outputId": "fdae6955-4143-4caa-d841-75293fd2120e"
   },
   "outputs": [],
   "source": [
    "image_path = 'Segmented_Dataset/Normal/Normal-1.png'\n",
    "#image_path = 'covid_data_small/test/normal/Normal-201.png'\n",
    "\n",
    "probabilities, predicted_class_index, predicted_class_name = predict_image_class(image_path)\n",
    "print('Probabilities:', probabilities)\n",
    "print('Predicted class index:', predicted_class_index)\n",
    "print('Predicted class name:', predicted_class_name)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "11c588d923114d5eb87177eaca3fef76": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "188f205f72364eca9046e9489d2e941a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_11c588d923114d5eb87177eaca3fef76",
      "placeholder": "​",
      "style": "IPY_MODEL_761cfa930aee4513b9987f10118a0efc",
      "value": "100%"
     }
    },
    "1a90e6228c8345d7bec3de414ea3de20": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "34c861f157b04736b95e70f442874962": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_188f205f72364eca9046e9489d2e941a",
       "IPY_MODEL_7c92f103d4bb4f4c9dab38632beb558f",
       "IPY_MODEL_cf544c18a5b84d4699bacf88103d10ca"
      ],
      "layout": "IPY_MODEL_1a90e6228c8345d7bec3de414ea3de20"
     }
    },
    "761cfa930aee4513b9987f10118a0efc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7708ef0dcdf44ec1acee8a979200ca04": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7c92f103d4bb4f4c9dab38632beb558f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ea2bb7962c144cc1a5808f3ca8040eaf",
      "max": 2117,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c974c193620c495da828bd5e268e9ce1",
      "value": 2117
     }
    },
    "a68b76c42da547a4b40862061d13ec53": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c974c193620c495da828bd5e268e9ce1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cf544c18a5b84d4699bacf88103d10ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7708ef0dcdf44ec1acee8a979200ca04",
      "placeholder": "​",
      "style": "IPY_MODEL_a68b76c42da547a4b40862061d13ec53",
      "value": " 2116/2117 [00:17&lt;00:00, 102.34it/s]"
     }
    },
    "ea2bb7962c144cc1a5808f3ca8040eaf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
