{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOLKCGzb0lh2fitP6Td9AW8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaralAminpour/IVM_supplementary_materials/blob/main/Clustering_introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part I: Clustering algorithms\n",
        "\n",
        "- K-means\n",
        "\n",
        "- Gaussian Mixture model"
      ],
      "metadata": {
        "id": "zMAHTUMDHO0H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Application: Breast Cancer Diagnosis**\n",
        "\n",
        "\n",
        "Introducing the Wisconsin breast cancer dataset, a valuable tool in the field of medical research and diagnostics.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-5-Clustering%20and%20Features/imgs/Picture1.gif' width=200px >\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-5-Clustering%20and%20Features/imgs/Picture2.png' width=200px >\n",
        "\n",
        "\n",
        "\n",
        "Source: (Street et al.: Nuclear feature extraction for breast tumor diagnosis. IS&T/SPIE 1993)\n",
        "\n",
        "\n",
        "Here's a glimpse of what's inside: Cells, both from benign and malignant tissues, were carefully extracted using a biopsy technique known as a **\"fine needle aspirate.\"** Once extracted, they were then captured and converted into **digital photographs** with the help of a **microscope**.\n",
        "\n",
        "But it doesn’t stop there! We used an intresting method named **\"snakes\"** to interactively **detect the boundaries of these cells.** With these boundaries set, we were able to extract a variety of cell features. We've got information on **30 different features**, such as:\n",
        "\n",
        "- **Radius**: This refers to the **average distance from the cell's center to its perimeter**.\n",
        "\n",
        "- **Texture**: It's essentially the **standard deviation of the cell's intensities**.\n",
        "\n",
        "- **Area**: It’s **the number of pixels enclosed** within the cell boundary.\n",
        "\n",
        "For each biopsy specimen, we've accumulated statistics related to these features. This includes the **average value**, **standard error**, and even the **highest value observed**.\n",
        "\n",
        "Here’s the crucial bit: Every specimen comes with a diagnosis, indicating whether it's benign (non-cancerous) or malignant (cancerous).\n",
        "\n",
        "Now, while our dataset focuses on the traditional method of **hand-crafted feature extraction from images**, it's worth noting that modern algorithms are evolving. Many now autonomously learn relevant features from images through tools like **convolutional neural networks**.\n",
        "\n",
        "But for this exploration, our primary focus will be on **uncovering the hidden structures within the feature space**. Let's dive in!"
      ],
      "metadata": {
        "id": "5w_sFf07jyyj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Application: Breast Cancer Dataset\n",
        "\n",
        "We'll begin by selecting two primary features: the mean radius and the mean texture of the cells. Using these features as our basis, we can efficiently fit a classifier when the labels are already known to predict cancer diagnosis.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-5-Clustering%20and%20Features/imgs/Picture3.png' width=400px >\n",
        "\n",
        "**Is there a natural structure in this dataset that we could discover without knowing the labels?**\n",
        "\n",
        "If we didn't have the labels, could we still detect a natural structure within this dataset based on the inherent characteristics of the cells? Are malignant cell properties distinct enough to be recognized without prior knowledge of the diagnosis?\n",
        "\n",
        "Clustering algorithms could be the key to uncovering this information.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-5-Clustering%20and%20Features/imgs/Picture4.png' width=400px >\n",
        "\n"
      ],
      "metadata": {
        "id": "KMC_Hbecgwp9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unsupervised learning\n",
        "\n",
        "Unsupervised learning is a type of machine learning where the algorithm is provided with input data that doesn't have labeled responses. The system tries to learn the patterns and the structure from the data without any supervised feedback.\n",
        "\n",
        "In simpler terms, imagine you have a jigsaw puzzle without the finished picture on the box. You have to figure out how to piece it together based solely on the shapes and images on each piece. That's a bit like unsupervised learning.\n",
        "\n",
        "Some common types of unsupervised learning methods include:\n",
        "\n",
        "1. **Clustering:** It's like grouping. The aim is to segregate groups with similar traits and assign them into clusters. Examples include K-means clustering where we try to group data into 'K' number of clusters.\n",
        "\n",
        "2. **Association:** Here, the algorithm identifies rules that describe large portions of the data, like people that buy X also tend to buy Y (think of the \"customers who bought this item also bought\" feature on shopping sites).\n",
        "\n",
        "Unsupervised learning contrasts with supervised learning, where the algorithm is trained on a dataset where the correct outputs are known and the algorithm makes predictions based on this prior knowledge.\n",
        "\n",
        "**Comic time**\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-5-Clustering%20and%20Features/imgs/cartoon-unsupervised-machine-learning-700-1.jpg' width=400px >\n"
      ],
      "metadata": {
        "id": "w4yizCk3nMX7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clustering\n",
        "\n",
        "Imagine you have a big box of mixed-up marbles, and you want to group them based on their similarities. That's kind of what clustering does in the world of machine learning.\n",
        "\n",
        "Now, clustering is a part of \"unsupervised learning\". It's like trying to sort these marbles **without someone telling you what criteria to use**. Maybe you'd choose color, size, or material. You're unsupervised and figuring it out on your own.\n",
        "\n",
        "In the specific method we're talking about, called **\"K-means clustering\"**, the goal is pretty straightforward. Imagine each group of marbles as having a **\"middle point\" or \"mean\"**. We want to group them such that the marbles are **as close as possible** to their respective **\"middle point\"**. The closer the marbles in a group are to this central point, the better our group is!\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-5-Clustering%20and%20Features/imgs/picture5.png' width=700px >\n",
        "\n",
        "\n",
        "There's a fancy way to measure how good our groups are using a thing called 'within-class variance'. Without diving too deep, it's a way to look at **how spread out marbles are in each group**. For our marble sorting, we want groups where marbles are really close to each other and the \"middle point\" of their group. So, **lower variance means our groups are tighter and better!**\n",
        "\n",
        "Here's a formula (don't worry too much about it) that captures this idea:\n",
        "\n",
        "$$\\sigma^2 = \\sum_{k=1}^{K} \\frac{n_k}{N} \\sigma_k^2$$\n",
        "\n",
        "This formula represents the **weighted average of the variances of each group** (or cluster) in the K-means clustering method.\n",
        "\n",
        "- $n$: number of examples in class k\n",
        "\n",
        "- $\\sigma$: variance if class k\n",
        "\n",
        "- $N$: total number of samples\n",
        "\n",
        "In simpler words, it's just a way to measure the average spread of marbles in all our groups. The goal is to get this value as small as possible!\n"
      ],
      "metadata": {
        "id": "O3vumVqClz7k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## K-means Clustering: Minimizing Intra-Class Variance\n",
        "\n",
        "(So how can we minimise the intra-class variance?)\n",
        "\n",
        "**Understanding K-means**\n",
        "\n",
        "When we delve into K-means clustering, our main goal is to group our data points into distinct clusters where every point in a cluster is more similar to its mates in the same cluster than to those in other clusters. How do we measure this similarity, you ask? Well, it's about minimizing the variance or the differences within each cluster.\n",
        "\n",
        "Here's an analogy to make it clearer: Picture yourself organizing books on a shelf. You'd naturally want books of the same genre or author to sit together. Now, if we consider the \"distance\" between each book as the differences in their content or style, our aim with K-means is akin to minimizing these distances, but with data points rather than books.\n",
        "\n",
        "**Step-by-Step Process**\n",
        "\n",
        "1. **Initialization**: Start by randomly selecting a few points that'll serve as our initial cluster centers.\n",
        "2. **Assigning Clusters**: For every data point, determine its nearest cluster center (akin to finding the most similar genre for a book) and assign it to that cluster.\n",
        "3. **Update the Cluster Centers**: After all data points have been assigned to clusters, calculate the average position of points in a cluster. This average point becomes the new cluster center.\n",
        "4. **Iterate**: Go back to the second step and continue until the cluster centers stabilize.\n",
        "\n",
        "A neat observation: The variance within each cluster can be seen as the average \"distance\" every data point in that cluster has from its center. Naturally, we want this \"distance\" to be as small as possible for tightly defined clusters.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-5-Clustering%20and%20Features/imgs/kmeans.mp4\" width = \"200\" style=\"float: right;\">\n",
        "\n",
        "\n",
        "**The Math Behind It**\n",
        "\n",
        "The formula you provided calculates this \"distance\" or variance:\n",
        "\n",
        "$$ \\sigma^2 = \\sum_{k=1}^{K} \\frac{n_k}{N} \\sigma_k^2 = \\sum_{k=1}^{K} \\left( \\frac{n_k}{N} \\frac{1}{n_k} \\sum_{i \\in S_k} |x_i - \\mu_k|^2 \\right) = \\frac{1}{N} \\sum_{i=1}^{N} |x_i - \\mu_{z_i}|^2 $$\n",
        "\n",
        "Here:\n",
        "- $𝒙_𝑖$ represents the feature of a specific data point.\n",
        "- $𝝁_(𝑧_𝑖)$ is the average feature of the cluster to which the data point belongs.\n",
        "- The symbol |...| represents the Euclidean distance, which is just a way of measuring the straight-line distance between two points.\n",
        "\n",
        "In essence, this formula adds up all the distances of the data points to their respective cluster centers and then averages them.\n",
        "\n",
        "\n",
        "**Elaboration on variance formula**\n",
        "\n",
        "- Total Variance, $ \\sigma^2 $: This is the overall intra-class variance.\n",
        "\n",
        "- The left part of the formula calculates the variance for each cluster, weighing them, and then summing up.\n",
        "\n",
        "- $ n_k $ is the count of data points in the k-th cluster, $ N $ is the total number of data points, and $ \\sigma_k^2 $ represents the variance of the k-th cluster.\n",
        "\n",
        "- $ \\mu_k $ is the center of the k-th cluster.\n",
        "\n",
        "- The sum $ \\sum_{i \\in S_k} |x_i - \\mu_k|^2 $ computes the sum of squared distances of all data points in the k-th cluster from its center.\n",
        "\n",
        "- The expression on the far right, $ \\frac{1}{N} \\sum_{i=1}^{N} |x_i - \\mu_{z_i}|^2 $, calculates the average squared distance every data point has from its respective cluster center.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Md6qTP0mnaJi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Breast cancer dataset: mean radius and texture of the cells\n",
        "\n",
        "Let's take a look at how k-means clusters the breast cancer dataset based on the mean radius and texture of the cells. Even without training labels, the data still forms distinct clusters. The performance is impressive, with a k-means accuracy of 0.86, nearly matching the classification accuracy of 0.89.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-5-Clustering%20and%20Features/imgs/picture6.png\" width = \"700\" style=\"float: right;\">\n"
      ],
      "metadata": {
        "id": "VOi4Zp0Iq71G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## K-means Clustering and Its Limitations\n",
        "\n",
        "K-means clustering is a popular method for grouping data into distinct categories. It works by minimizing the distance of data points from the center of their assigned cluster. But it has a limitation: it always looks for round-shaped clusters. This means if our data naturally forms elongated or oddly-shaped groups, K-means might not capture them accurately.\n",
        "\n",
        "Imagine a group of points that forms a long, narrow ellipse. K-means would probably see this as multiple small round clusters rather than one long one.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-5-Clustering%20and%20Features/imgs/picture7.png\" width = \"700\" style=\"float: right;\">\n",
        "\n",
        "\n",
        "\n",
        "**Gaussian Mixture Model (GMM) - A Different Approach:**\n",
        "To tackle the above limitation, we can use the Gaussian Mixture Model (GMM). This model is excellent for cases where clusters aren't necessarily round. It can recognize and capture elongated or other complex shapes.\n",
        "\n",
        "In our example, when we look at two specific features - the mean area and the worst area - the Gaussian Mixture Model does a much better job than K-means. It recognizes the natural shape of the data and groups them more accurately. This leads to a high accuracy rate of 0.93.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-5-Clustering%20and%20Features/imgs/picture8.png\" width = \"700\" style=\"float: right;\">\n",
        "\n",
        "\n",
        "The secret behind GMM's performance is its ability to see clusters as shapes formed by Gaussian distributions. This means it can detect and adapt to clusters that are round, elongated, or even more intricate.\n",
        "\n",
        "\n",
        "So, if you're dealing with data that might not naturally form into neat circles, considering the Gaussian Mixture Model might be a good move!"
      ],
      "metadata": {
        "id": "kMHH2tImsO4Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assumptions\n",
        "\n",
        "**Assumption**: Clusters are shaped by Gaussian distributions.\n",
        "\n",
        "**Outcome**: This can lead to stretched-out clusters with curved boundaries.\n",
        "Its impressive performance comes from the fact that it shapes clusters based on Gaussian distributions, which can result in extended clusters with non-linear edges.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-5-Clustering%20and%20Features/imgs/picture9.png\" width = \"700\" style=\"float: right;\">\n"
      ],
      "metadata": {
        "id": "95hygLi6teJV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding the Gaussian Mixture Model (GMM) through MRI Brain Segmentation\n",
        "\n",
        "Gaussian Mixture Model (GMM) is a statistical model that represents the presence of multiple Gaussian distributions within a dataset. It assumes that the data is generated from several Gaussian distributions, each with its own parameters. Let's break it down using the example of segmenting brain MRI based on voxel intensities.\n",
        "<img src=\"https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-5-Clustering%20and%20Features/imgs/picture10.png\" width = \"600\" style=\"float: right;\">\n",
        "\n",
        "\n",
        "1. **The Data: Brain MRI**\n",
        "   - An MRI scan of the brain contains voxel intensities. Each voxel (a 3D pixel) has an intensity value that represents a specific tissue type.\n",
        "   - If you were to plot the distribution of these intensities, you might notice distinct peaks or clusters.\n",
        "\n",
        "2. **Brainmasking and Observing Peaks:**\n",
        "   - Brainmasking is a preprocessing step where only the brain region is considered, and other parts are set to zero. This helps in removing non-brain elements.\n",
        "   - Once this is done, if you look at the histogram of intensities, you might notice three clear peaks. These peaks typically correspond to:\n",
        "     - White matter\n",
        "     - Grey matter\n",
        "     - Cerebro-spinal fluid (CSF)\n",
        "\n",
        "3. **How GMM Comes Into Play:**\n",
        "   - GMM will consider these peaks in the histogram as arising from different Gaussian distributions. It will try to fit multiple Gaussian curves to these peaks.\n",
        "   - The main goal of GMM is to identify parameters (like mean and standard deviation) for each Gaussian distribution (each tissue type in our case).\n",
        "   - Once GMM has these parameters, it can tell us the probability of a voxel belonging to white matter, grey matter, or CSF based on its intensity.\n",
        "\n",
        "4. **Processing the MRI Data:**\n",
        "   - For each voxel in the MRI data, GMM will evaluate its intensity and assign it to one of the Gaussian distributions (or tissue types) based on the highest probability.\n",
        "   - Remember the parts outside the brain that were padded with zeros after brainmasking? GMM will essentially ignore these because they don't belong to any of the identified Gaussian distributions.\n",
        "\n",
        "5. **Outcome:**\n",
        "   - After processing, GMM will provide segmented regions of the brain MRI, clearly demarcating areas of white matter, grey matter, and CSF.\n",
        "\n",
        "In summary, the Gaussian Mixture Model, through its ability to model multiple Gaussian distributions, can efficiently segment complex data like brain MRI images, identifying and categorizing different tissue types based on voxel intensities."
      ],
      "metadata": {
        "id": "7C_8quNPPlX-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gaussian distribution\n",
        "\n",
        "We will represent the intensity distribution for each cluster using a Gaussian distribution. This can be viewed as the likelihood of observing intensity $x_i$ from a sample in class k. This Gaussian distribution is characterized by two parameters: the mean $\\mu$ and the variance $\\sigma^2$.\n",
        "\n",
        "Intensity distribution for each cluster $k$ is Gaussian with $\\mu_k$, $\\sigma_k$.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-5-Clustering%20and%20Features/imgs/picture20.png\" width = \"700\" style=\"float: right;\">\n",
        "\n",
        "\n",
        "The observed intensity distribution of the entire image, represented by the normalized histogram, can be perceived as the likelihood of observing intensity x.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-5-Clustering%20and%20Features/imgs/Picture11.png\" width = \"400\" style=\"float: right;\">\n",
        "\n",
        "\n",
        "The normalized intensity histogram can be depicted as a combination of Gaussians, defined by parameters\n",
        "\n",
        "$$\\Phi=(\\mu_k, \\sigma_k, c_k)_{k=1,\\ldots,K}.$$\n",
        "\n",
        "We represent this likelihood distribution as a blend of Gaussian distributions, each multiplied by their respective mixing coefficients, and then summed across all classes.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-5-Clustering%20and%20Features/imgs/picture21.png\" width = \"400\" style=\"float: right;\">\n",
        "\n",
        "The intensity distribution for each cluster k is Gaussian, described by $\\mu_k$ and $\\sigma_k$.\n"
      ],
      "metadata": {
        "id": "9_jLQRWbhE5d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gaussian Distributions in simple words\n",
        "\n",
        "Let's take a step back and have a friendly chat about the basic \"Gaussian distribution\" before diving into the \"Gaussian Mixture.\" You might already be familiar with it, but a little recap never hurts!\n",
        "\n",
        "Imagine you have a bunch of different colors, and you want to **group them into certain categories based on how similar they are**. In our case, we're grouping them based on their **intensity**, which is like how light or dark a color is.\n",
        "\n",
        "**1. Using Gaussian Distributions:**\n",
        "\n",
        "When we talk about representing the intensity distribution for each cluster (or category) using a Gaussian distribution, think of it like this: Imagine a **bell curve**. The top or peak of the curve represents the **most common** intensity for that group, and as we move away from the peak, the intensities become **less common**.\n",
        "\n",
        "This bell curve is defined by two things:\n",
        "\n",
        "- Its **center** (called the mean, represented by $\\mu$).\n",
        "\n",
        "- How **spread out** it is (called the variance, represented by $\\sigma^2$).\n",
        "\n",
        "**2. Likelihood of Observing an Intensity:**\n",
        "\n",
        "Now, when we see a certain **intensity, $x_i$**, in an image, we can use our bell curve to check how likely it is that this **intensity belongs to a group $k$**. The formula for this is:\n",
        "\n",
        "$$P(x_i|y_i=k,\\mu_k, \\sigma_k ) = \\frac{1}{\\sqrt{2\\pi \\sigma}} e^{\\frac{-(x_i-\\mu_k)^2}{2\\sigma_k^2}}$$\n",
        "\n",
        "This formula might look a bit intimidating, but it's just a mathematical way to describe our bell curve.\n",
        "\n",
        "**3. Mixture of Gaussians:**\n",
        "\n",
        "But what if our image has multiple groups of colors? This is where the idea of a mixture of Gaussians comes in. Instead of just one bell curve, we'll have several, **each representing a different group of intensities**. Each bell curve gets a **weight**, called the **mixing coefficient ($c_k$)**, based on how dominant that group is in the image. We combine all these bell curves to get the overall distribution of intensities in the image:\n",
        "\n",
        "$$P(x|\\Phi) = \\sum_{k} c_k G_{\\sigma_k} (x−\\mu_k)$$\n",
        "\n",
        "**4. Parameters for Each Gaussian:**\n",
        "\n",
        "Lastly, for every bell curve (or Gaussian) we use, it's described by its center ($\\mu_k$), spread ($\\sigma_k$), and weight ($c_k$). We collectively represent all these parameters for all the bell curves as\n",
        "\n",
        "$$\\Phi = (\\mu_k, \\sigma_k, c_k)_{k=1,\\ldots,K}$$.\n",
        "\n",
        "\n",
        "So, in simple terms, we're using a combination of bell curves to represent the distribution of intensities in an image. Each bell curve stands for a group of similar intensities, and the entire set of bell curves helps us understand the overall color distribution in the image."
      ],
      "metadata": {
        "id": "r5RSdMarhKoH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Comic time**\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-5-Clustering%20and%20Features/imgs/pearly_gates-heaven-the_afterlife-st_peter-judgement-education-teaching-CX304149_low.jpg\" width = \"400\" style=\"float: right;\">\n"
      ],
      "metadata": {
        "id": "MFr6iS9PvvOp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gaussian Mixture model\n",
        "\n",
        "The Gaussian Mixture Model, often shortened to GMM, is like a recipe with three main ingredients for each group or class. These ingredients are\n",
        "\n",
        "- the **mean** (it's like the average value),\n",
        "\n",
        "- the **standard deviation** (tells us how spread out the values are), and\n",
        "\n",
        "- the **mixing proportion** (think of it as the importance or weight of each group).\n",
        "\n",
        "**How do we fit the parameters of the Gaussian Mixture Model**\n",
        "\n",
        "$$\\Phi = (\\mu_k, \\sigma_k, c_k)_{k=1,\\ldots,K}?$$\n",
        "\n",
        "Imagine we have a brain image, and we're looking at the **different intensities** in it. What we want to do with the GMM is find the **best mix** – the best set of means, standard deviations, and mixing proportions – that **describes these intensities**. This is a bit like trying to find the perfect recipe that brings out all the flavors in a dish.\n",
        "\n",
        "To do this, we aim to **maximize** something called the **log-likelihood** of the image given our GMM parameters, denoted as $\\phi$. In simpler terms, we're trying to find the best fit for our model to the data. Now, here's a neat thing: if we assume that each tiny unit (or voxel) in the image is independent, we can sum up the individual contributions of each voxel to get the overall goodness of fit. And that's where our **mix of Gaussian distributions** comes into play.\n",
        "\n",
        "Our goal is to adjust our ingredients (the parameters of our GMM) so that we get the best possible fit.\n",
        "\n",
        "Find Φ to maximise log-likelihood log L of the image x=(x_1,…,x_N)\n",
        "\n",
        "\n",
        "Mathematically, we can express this as:\n",
        "\n",
        "$$\n",
        "\\log L(\\Phi) = \\log P(\\mathbf{x}|\\Phi) = \\sum_{i=1}^{N} \\log P(x_i |\\Phi)\n",
        "$$\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-5-Clustering%20and%20Features/imgs/picture12.png\" width = \"300\" >\n",
        "\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-5-Clustering%20and%20Features/imgs/picture13.png\" width = \"300\" >\n",
        "\n",
        "\n",
        "Where $L(\\Phi)$ represents the likelihood of our image given our model parameters.\n",
        "\n",
        "To find the best fit, we can use some calculus magic! We'll set the derivatives (or slopes) of our log-likelihood with respect to each of our GMM parameters to zero. These equations are:\n",
        "$$\n",
        "\\frac{\\partial \\log L(\\Phi)}{\\partial \\mu_k} = 0\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial \\log L(\\Phi)}{\\partial \\sigma_k} = 0\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial \\log L(\\Phi)}{\\partial c_k} = 0\n",
        "$$\n",
        "\n",
        "Lastly, an essential part of our GMM is making sure **the sum of all our mixing proportions equals 1**, ensuring our model represents a complete probability distribution. This is captured as:\n",
        "$$\n",
        "\\sum_{k} c_k = 1\n",
        "$$\n",
        "\n",
        "That's how we fit a Gaussian Mixture Model to our image data. It's all about finding the right mix to best describe our image intensities."
      ],
      "metadata": {
        "id": "HXFDcRETTeoU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Probabilistic segmentation 𝑝_𝑖𝑘=𝑃(𝑦_𝑖=𝑘|𝑥_𝑖,Φ)\n"
      ],
      "metadata": {
        "id": "6cINsi7HLgEZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Candy Analogy for Gaussian Mixture model\n",
        "\n",
        "Imagine you have a colorful bag of candies. Each candy represents a data point (or intensity in our case), and they come in different flavors. Now, you want to group these candies based on their flavors, but you're not entirely sure how many flavors there are or what each flavor tastes like. That's where the Gaussian Mixture Model (GMM) steps in. It helps us find these hidden \"flavors\" (or clusters) in our data.\n",
        "\n",
        "To make things even cooler, GMM doesn't just say, \"Hey, this candy belongs to this flavor!\" Instead, it tells us the probability or chance that a candy belongs to each flavor. So, for a given candy, it might say there's an 80% chance it's apple-flavored and a 20% chance it's orange-flavored. This is what we call **probabilistic segmentation**.\n",
        "\n",
        "The entire game is about finding the best way to describe our candies using these groups or clusters. This is achieved using certain parameters, collectively called **Ф (Phi)**. Think of these parameters as the unique recipe for each flavor. The main ingredients of this recipe are:\n",
        "- The average taste of candies in a group (**μ**).\n",
        "- How varied the taste is within the group (**σ**).\n",
        "- The overall presence of that flavor in the entire bag (**c**).\n",
        "\n",
        "Our mission is to find the best possible recipe for each flavor. But here's the thing: we don't just do it in one go. We keep tasting and adjusting until we're pretty sure we've got it right. This process of taste-test-adjust is what we call an **algorithm**. In this case, our algorithm is:\n",
        "1. Make a guess about the flavors and their recipes.\n",
        "2. Based on that guess, see how likely each candy is to belong to each flavor.\n",
        "3. Refine our guess based on the candies' responses.\n",
        "4. Repeat until our guess doesn't change much anymore (or in fancy terms, until the **log-likelihood** converges).\n",
        "\n",
        "Mathematically, our tastings and adjustments are expressed by these formulas:\n",
        "- For the chance that a candy belongs to a flavor:\n",
        "$$\n",
        "p_{ik} = \\frac{c_k G_{\\sigma_k} (x_i - \\mu_k)}{\\sum_k c_k G_{\\sigma_k} (x_i - \\mu_k)}\n",
        "$$\n",
        "- Refining our guess for the average taste (**mu**), taste spread (**sigma**), and flavor's presence (**c**):\n",
        "$$\n",
        "\\mu_k = \\frac{\\sum_i p_{ik} x_i}{\\sum_i p_{ik}}, \\quad \\sigma_k^2 = \\frac{\\sum_i p_{ik} (x_i - \\mu_k)^2}{\\sum_i p_{ik}}, \\quad c_k = \\frac{\\sum_i p_{ik}}{n}\n",
        "$$\n",
        "- Calculating the chance a candy belongs to a flavor using all our gathered information (via **Bayes formula**):\n",
        "$$\n",
        "P(y_i = k | x_i, \\Phi) = \\frac{p(y_i = k | \\Phi) P(x_i | y_i = k, \\Phi)}{\\sum_k p(y_i = k | \\Phi) P(x_i | y_i = k, \\Phi)}\n",
        "$$\n",
        "\n",
        "In essence, we're dancing between adjusting our guesses for the flavor recipes and checking how our candies feel about those guesses, over and over, until everyone's mostly happy!\n"
      ],
      "metadata": {
        "id": "2K-u7580Twk7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "So, you've got this vibrant collection of data points, like having a bag of assorted candies. These data points, similar to candies, have their unique characteristics, like different flavors. Our challenge? Grouping these points based on their similarities. Enter Gaussian Mixture Model (GMM). It helps us identify these underlying groups in our data.\n",
        "\n",
        "But GMM adds a twist! Instead of just categorizing a data point into a group, it gives us the likelihood of it belonging to each group. For example, it might say a particular point has an 80% chance of being in Group A and 20% in Group B. This cool feature is known as **probabilistic segmentation**.\n",
        "\n",
        "The core idea is about best describing our data using these groups. We do this with certain parameters, known as $Ф$ (Phi). These parameters help define each group. They include:\n",
        "- The average characteristic of points in a group ($mu$).\n",
        "- The variability within the group ($sigma$).\n",
        "- The proportion of that group in the entire dataset ($c$).\n",
        "\n",
        "Our goal is to pinpoint these parameters. But we don't do this randomly. It's a systematic process where we estimate, check, and refine. This iterative process is our **algorithm**. The steps are:\n",
        "1. Make an initial estimation of the groups and their characteristics.\n",
        "2. Based on that estimation, determine the likelihood of each data point belonging to each group.\n",
        "3. Tweak our initial guess based on the feedback.\n",
        "4. Keep refining until our estimates are consistent and reliable.\n",
        "\n",
        "On the math side of things, here's how we express this:\n",
        "- The likelihood a point belongs to a group:\n",
        "$$\n",
        "p_{ik} = \\frac{c_k G_{\\sigma_k} (x_i - \\mu_k)}{\\sum_k c_k G_{\\sigma_k} (x_i - \\mu_k)}\n",
        "$$\n",
        "- Refining our estimate for average ($mu$), variability ($sigma$), and group proportion ($c$):\n",
        "$$\n",
        "\\mu_k = \\frac{\\sum_i p_{ik} x_i}{\\sum_i p_{ik}}, \\quad \\sigma_k^2 = \\frac{\\sum_i p_{ik} (x_i - \\mu_k)^2}{\\sum_i p_{ik}}, \\quad c_k = \\frac{\\sum_i p_{ik}}{n}\n",
        "$$\n",
        "- Getting the likelihood of a point belonging to a group, considering all the parameters:\n",
        "$$\n",
        "P(y_i = k | x_i, \\Phi) = \\frac{p(y_i = k | \\Phi) P(x_i | y_i = k, \\Phi)}{\\sum_k p(y_i = k | \\Phi) P(x_i | y_i = k, \\Phi)}\n",
        "$$\n",
        "\n",
        "So there you have it! While it might feel like a dance between estimating and refining, by the end, it all comes together, giving us a clearer picture of our data's story. Dive into GMM and enjoy the journey!"
      ],
      "metadata": {
        "id": "x77s4Zz_Wk5c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The formula for calculating the ejection fraction (EF) is given by:\n",
        "$$\n",
        "EF = \\left( \\frac{LVEDV - LVESV}{LVEDV} \\right) \\times 100\n",
        "$$\n",
        "\n",
        "Here's a breakdown of the terms:\n",
        "\n",
        "- **EF (Ejection Fraction)**: This represents the percentage of blood that's pumped out of the left ventricle (a main pumping chamber of the heart) during each heartbeat.\n",
        "\n",
        "- **LVEDV (Left Ventricular End-Diastolic Volume)**: This is the amount of blood in the left ventricle just before it contracts to pump the blood out. Essentially, it's the volume of the left ventricle when it's fullest.\n",
        "\n",
        "- **LVESV (Left Ventricular End-Systolic Volume)**: This is the amount of blood remaining in the left ventricle after it has contracted. It's the volume of the left ventricle when it's least full.\n",
        "\n",
        "The formula calculates the difference between the volume of blood in the left ventricle before and after it contracts. This difference is then divided by the full volume (before contraction) to get a fraction. Multiplying by 100 converts this fraction into a percentage, giving us the ejection fraction. This percentage helps in understanding how effectively the heart is pumping blood out during each beat."
      ],
      "metadata": {
        "id": "eMggHf8Vj-ep"
      }
    }
  ]
}