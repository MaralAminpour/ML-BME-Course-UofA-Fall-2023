{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOYawRdU8Xr96PZjgwvDAvx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaralAminpour/IVM_supplementary_materials/blob/main/feature_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part IV: Feature Extraction"
      ],
      "metadata": {
        "id": "ZDIGxOzLDmMy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## what is feature extraction?\n",
        "\n",
        "\n",
        "Feature extraction is the art of simplifying our data. Imagine you have a vast canvas with a sprawling and complex painting. Feature extraction is like being able to reproduce that painting on a smaller canvas, while still capturing its essence and beauty. When we dive deep into large datasets, especially those from signals or images, there's so much going on! There are tons of details, but not all of them are crucial. Feature extraction helps us filter out the noise and focus on the salient features, those significant brush strokes that truly define the artwork.\n",
        "\n",
        "Why is this important? Well, when we reduce the complexity of our data, we're setting ourselves up for success in the world of machine learning. A simplified, yet meaningful dataset is less prone to overfitting, where our model might get too caught up in the tiny details and miss the big picture. Furthermore, models trained on such data are not only more accurate but also learn faster.\n",
        "\n",
        "So, the next time you come across a large dataset, think of it as that vast canvas. Through feature extraction, you're not only making the canvas more manageable but ensuring that every brush stroke on it truly counts. By emphasizing the right details and sidelining the redundant ones, we create a perfect setting for our machine learning models to thrive.\n"
      ],
      "metadata": {
        "id": "W30aR226B555"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Extraction Examples\n",
        "\n",
        "Picture this: You're watching a super detailed movie of your heart in action. This isn't just any regular film; it's captured using a special camera known as a 3D+T MRI. This camera allows us to see every tiny detail of your heart from all angles and across time.\n",
        "\n",
        "Now, these pictures can be a bit overwhelming. Every single detail of your heart is captured in these images. But, what if we just want to understand how your heart pumps blood? That's where \"feature extraction\" waltzes in like a detective, looking for clues about your heart's performance. It's like skimming a book to understand the main plot without getting bogged down by all the side stories.\n",
        "\n",
        "Now, to help us understand these images, we use a smart tool called a **convolutional neural network**. Think of it as a set of virtual glasses that can highlight specific parts of the heart in these pictures.\n",
        "\n",
        "From these highlighted sections, we measure how much blood the left ventricle (an important chamber in the heart) holds and pumps out. It's a bit like observing how much water fills and empties from a water balloon.\n",
        "\n",
        "Two moments capture our attention: when the balloon is fully blown (end diastole) and when it's most squeezed out (end systole). With these two measurements, we can calculate something called the \"ejection fraction\" (EF). It's like finding out how much water the balloon pushes out in one go!\n",
        "\n",
        "Here's a simple formula:\n",
        "$$\n",
        "EF = \\left( \\frac{LVEDV - LVESV}{LVEDV} \\right) \\times 100\n",
        "$$\n",
        "\n",
        "Here, we're just looking at the difference between the balloon's maximum and minimum water levels as a percentage. Don't worry about the letters too much; they're just fancy names for the balloon's maximum and minimum water levels.\n",
        "\n",
        "So, to wrap it up: We have this high-tech movie of the heart, we use special glasses to focus on the main events, and from that, we gauge how well our heart is working. Simple as that!\n"
      ],
      "metadata": {
        "id": "nY7j-X5pB-JD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The formula for calculating the ejection fraction (EF) is given by:\n",
        "$$\n",
        "EF = \\left( \\frac{LVEDV - LVESV}{LVEDV} \\right) \\times 100\n",
        "$$\n",
        "\n",
        "Here's a breakdown of the terms:\n",
        "\n",
        "- **EF (Ejection Fraction)**: This represents the percentage of blood that's pumped out of the left ventricle (a main pumping chamber of the heart) during each heartbeat.\n",
        "\n",
        "- **LVEDV (Left Ventricular End-Diastolic Volume)**: This is the amount of blood in the left ventricle just before it contracts to pump the blood out. Essentially, it's the volume of the left ventricle when it's fullest.\n",
        "\n",
        "- **LVESV (Left Ventricular End-Systolic Volume)**: This is the amount of blood remaining in the left ventricle after it has contracted. It's the volume of the left ventricle when it's least full.\n",
        "\n",
        "The formula calculates the difference between the volume of blood in the left ventricle before and after it contracts. This difference is then divided by the full volume (before contraction) to get a fraction. Multiplying by 100 converts this fraction into a percentage, giving us the ejection fraction. This percentage helps in understanding how effectively the heart is pumping blood out during each beat."
      ],
      "metadata": {
        "id": "gucr6Fr1CORv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### QRS Interval from ECG:\n",
        "\n",
        "To derive the QRS interval from an ECG signal, we start with processing the raw ECG. The first step involves filtering the signal to discard low-frequency drifts and any external noise. This gives us a cleaner waveform to work with. Next, we determine the peaks which appear as local highs and lows (maxima and minima) within the filtered signal. By setting certain thresholds, like a minimum height and distance between the peaks, we can accurately pinpoint the Q, R, and S peaks. After these peaks are detected, the next step is to compute the average duration of the QRS interval from the data gathered over the course of the ECG recording.\n",
        "\n",
        "+\n",
        "\n",
        "Let's chat about the QRS interval from an ECG, kind of like figuring out the rhythm of a catchy song! Imagine you're listening to a tune, but there's a bit of static. First, you'd want to get rid of that noise. That's what we do with filtering; it helps us hear (or in this case, see) the main beats clearly.\n",
        "\n",
        "Next up, we look for the high and low points in our 'song'. These are our peaks. But wait! Not every peak is the right beat we're looking for. So, we set some ground rules to pick out the main beats, which we call the Q, R, and S peaks.\n",
        "\n",
        "Lastly, with our main beats identified, we figure out the average duration between these beats. That's our QRS interval. It's like finding out the average time between the beats of our favorite song.\n",
        "\n",
        "And there you have it! From a jumbled tune to the clear rhythm of the heart's song!"
      ],
      "metadata": {
        "id": "kSYHZ6CJCYie"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Feature Categories:**\n",
        "\n",
        "Letâ€™s now have a look at various types of features that we can use to train our machine learning models. Some features are directly observed or measured, and no feature extraction is needed. These are for example age, gender, height, or blood glucose levels.\n",
        "\n",
        "1. **Direct Observations:**\n",
        "   - Examples include age, gender, height, blood glucose levels, and others. These are straightforward measurements that don't need any additional processing.\n",
        "\n",
        "2. **Morphological Features:**\n",
        "   - These focus on the shape and structure of objects. Common ones are length, area, and volume. Think about measurements like the size of brain structures or the duration of QRS waves.\n",
        "\n",
        "3. **Texture Descriptions:**\n",
        "   - These dive into the statistical details of images or parts of images. Key terms here are mean, variance, kurtosis, and entropy. For instance, studying the texture of breast tissue samples can help detect cancer.\n",
        "\n",
        "4. **Transform-based Attributes:**\n",
        "   - This involves changing signals or images into different formats or domains. One example is moving to the frequency domain, which can help pinpoint abnormalities in things like ECG readings.\n",
        "\n",
        "5. **Local Feature Descriptions:**\n",
        "   - These emphasize specific points in images, like edges, blobs, or corners. They are typically highlighted using filtering techniques.\n",
        "\n",
        "In summary, there's a wide variety of features that assist in training machine learning models. Some are direct and easy to see, while others require a deeper look or transformation to understand fully."
      ],
      "metadata": {
        "id": "U4hyM6QjCeW_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature extraction example\n",
        "\n",
        "\"We will be exploring the PatchCamelyon dataset, which includes histological images of breast tissues. These images showcase both healthy and cancerous cells. Our goal is to determine if texture descriptors, specifically GLCM statistical properties, or localized feature descriptors like DAISY can be indicators of cancer.\n",
        "\n",
        "**Breast Cancer Classification Using Histological Images**\n",
        "- **Dataset:** PatchCamelyon\n",
        "- **Feature Extraction Techniques:**\n",
        "  - Texture Descriptors (GLCM)\n",
        "  - Localised Feature Descriptors (DAISY)\n",
        "\n",
        "In the upcoming tutorial, you'll have the opportunity to delve into this yourself.\n",
        "\n",
        "**Jupyter Notebook:** 8.4 Feature Extraction\""
      ],
      "metadata": {
        "id": "n8_OJApTCvWv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Texture Description\n",
        "\n",
        "The Gray level co-occurrence matrix, or GLCM for short, is a way to understand the texture of an image. Think of it like a table that shows how often pairs of pixels with specific brightness values are found together in an image.\n",
        "\n",
        "To make sense of the numbers in this table, we change them into percentages or probabilities. This makes it a 2D probability distribution, represented as \\( p(i,j) \\).\n",
        "\n",
        "Now, from this table, we can figure out some cool things about the texture of our image:\n",
        "\n",
        "1. **Contrast**: Measures the amount of local variations present. It's calculated using the formula:\n",
        "$$ \\sum_{i,j=0}^{l-1} (i-j)^2 p(i,j) $$\n",
        "\n",
        "2. **Dissimilarity**: Gives us an idea about how different pairs of pixels are. The formula is:\n",
        "$$ \\sum_{i,j=0}^{l-1} |i-j| p(i,j) $$\n",
        "\n",
        "3. **Homogeneity**: Tells us how close the elements of the matrix are to the matrix diagonal. In simpler words, how uniform our texture is. It's found using:\n",
        "$$ \\sum_{i,j=0}^{l-1} \\frac{1}{1+(i-j)^2} p(i,j) $$\n",
        "\n",
        "4. **Energy**: It's like a measure of uniformity or orderliness of the pixels. The higher the energy, the more order or regularity there is in the image. Formula:\n",
        "$$ \\sqrt{\\sum_{i,j=0}^{l-1} p(i,j)^2} $$\n",
        "\n",
        "5. **Correlation**: Shows how a pixel is related to its neighbor. The math behind it is a bit complex, but it's done using:\n",
        "$$ \\sum_{i,j=0}^{l-1} \\frac{(i-\\mu_i)(j-\\mu_j)}{\\sigma_I \\sigma_j} p(i,j) $$\n",
        "\n",
        "In simple words, once we've got our matrix set up and converted to percentages, these formulas help us describe the texture of our image in different ways!"
      ],
      "metadata": {
        "id": "BgW-0-9pC0TE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The DAISY descriptor\n",
        "\n",
        "Think of it as a special tool that doesn't just look for standout spots in an image but captures details all over it. It's super keen on understanding how colors change and transition in pictures by studying gradients from various angles.\n",
        "\n",
        "So, how does it work? First, we figure out how colors shift in different directions using some neat filtering tricks. After that, we add a gentle blur with varying degrees of fuzziness and pick certain spots to gather our information.\n",
        "\n",
        "Now, imagine a picture of the DAISY descriptor layout. There's a central red dot where the DAISY is located in the image. Then, surrounding it, you'll see red and blue dots. These are our special spots where we gather all the gradient details. And see those circles around? They symbolize the amount of blur we're usingâ€”the bigger the circle, the fuzzier the blur. Plus, those lines coming out from the dots? They're showing us the gradient directions.\n",
        "\n",
        "When we're done, we line up all the cool features we've captured from DAISY into a neat list, ready for some action! We'll even see how DAISY helps in classifying medical samples for breast cancer and stack it up against another method called GLCM.\n",
        "\n",
        "A quick recap:\n",
        "DAISY:\n",
        "- Dots = where we're checking details\n",
        "- Circles = how much we're blurring things\n"
      ],
      "metadata": {
        "id": "OOP0itRxDA36"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature extraction in the era of deep learning\n",
        "\n",
        "**Feature Engineering:**\n",
        "This is like giving our computer a little manual on how to look at data. We're telling it, \"Hey, check out these particular bits and pieces, they're super important!\" In our chat today, I'll show you some cool techniques we've got up our sleeve for this.\n",
        "\n",
        "**Feature Learning:**\n",
        "Here's where things get futuristic! With deep neural networks, we basically let our computer play detective. Instead of us pointing out the clues, it finds them on its own. It's especially awesome for looking at things like pictures or sounds where there's a ton of info to sift through.\n",
        "\n",
        "**Why Still Love Feature Engineering?**\n",
        "Even with all the fancy auto-detective work, there's value in the old-school approach:\n",
        "\n",
        "- We get to be the boss! We decide what features the computer should focus on.\n",
        "\n",
        "- It's way easier to explain. If someone asks, \"Why did the computer decide this?\", we've got clear answers.\n",
        "\n",
        "- It's super handy when we don't have a mountain of data, or when we really want to understand what's driving our computer's decisions.\n",
        "\n",
        "In a nutshell, while letting the computer learn on its own is super powerful, there's still a special place in our hearts (and in many projects) for good old feature engineering!"
      ],
      "metadata": {
        "id": "rfSLvvD8DFyk"
      }
    }
  ]
}