{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMGcs9S9u8Ehw5gDCAFdf1g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaralAminpour/IVM_supplementary_materials/blob/main/feature_selection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part III: Feature Selection\n",
        "\n",
        "Feature selection is an essential step in the machine learning pipeline. It refers to the process of selecting **a subset of the most relevant features (or variables) from the original set**, enhancing the performance of the model, speeding up the training process, and preventing overfitting."
      ],
      "metadata": {
        "id": "2tORMVGnUiAv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why we select certain features over others?\n",
        "\n",
        "Ever wonder why we select certain features over others? Well, there are some pretty cool reasons:\n",
        "\n",
        "1. **To Avoid Overfitting**: Think of it as **decluttering**. By removing extra stuff (or redundant data), our model doesn't get sidetracked by unnecessary **noise**.\n",
        "2. **Boost Accuracy**: It's like focusing on the main story without the side plots. By cutting out misleading data, our model can be more **on-point and accurate**.\n",
        "3. **Speed Things Up**: Everyone loves a quick result, right? With fewer features, our model can sprint through the training process, making everything snappier.\n"
      ],
      "metadata": {
        "id": "RSsqIqlqU7ux"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Application: Prediction of age at scan\n",
        "\n",
        "Let's focus on predicting age based on brain scans. Remember when we talked about predicting age from those 86 brain structure volumes? Using just a straightforward multivariate linear regression might make our model a bit too eager and overfit the training data. The clue? A big difference in how the model does on the full training set versus when tested with cross-validation.\n",
        "\n",
        "Now, throughout our journey, we've come across several cool techniques to prevent this overfitting. Do any of them ring a bell?\n",
        "\n",
        "- Keeping our model's enthusiasm in check with **regularisation** (like Ridge or Lasso).\n",
        "- Reducing the number of features with methods like **PCA, ICA, or Laplacian Eigenmap**.\n",
        "- Bringing together multiple models in **ensemble learning** (like our buddy, the Random Forest).\n",
        "- And of course, being picky with our data using **feature selection**!\n",
        "\n",
        "Quick recap: We aim to predict the age at scan using volumes of **86 brain structures** in preterm babies. Just using multivariate linear regression? **Oops, we overfit!**\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-5-Clustering%20and%20Features/imgs/select1.png' width=500px >\n",
        "\n",
        "So, what's in our toolkit to prevent overfitting? Regularisation, dimensionality reduction, ensemble learning, and feature selection. Got it? Onward!"
      ],
      "metadata": {
        "id": "salPD6QqsPMj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simulated features\n",
        "\n",
        " Imagine we've got five of these features, each sprinkled with varying amounts of noise and non-linearity. Picture it like this:\n",
        "\n",
        "- The x-axis is showing the feature value.\n",
        "- The y-axis? That’s our target value.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-5-Clustering%20and%20Features/imgs/select2.png' width=700px >\n",
        "\n",
        "Now, our goal is to pick the best and most informative features. To help visualize this, we've simulated five features:\n",
        "\n",
        "1. The first feature has a straight-line relationship with the targets, but it's kinda like static on a TV – a bit noisy.\n",
        "2. The second one? Still a straight-line relationship, but clearer than a sunny day – much less noise.\n",
        "3. Our third feature's relationship with the target is more like a wavy line, a bit curvy but still pretty clear.\n",
        "4. The fourth one is like a roller coaster, super curvy and unpredictable. It’s not a straight shot to our target, but it’s clear without much noise.\n",
        "5. And the fifth? Well, that's just like radio static – all noise and no clear connection to our targets."
      ],
      "metadata": {
        "id": "KzJfcGXvtdh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature importances\n",
        "\n",
        "Feature importances helps us understand which features really make a difference when making predictions.\n",
        "\n",
        "**Univariate Feature Importances**:\n",
        "\n",
        "- Think of this as looking at **one feature at a time**.\n",
        "\n",
        "- We're trying to see **how good** each feature is at **predicting the target values/labels.**\n",
        "\n",
        "- **correlation or mutual information**: It’s kinda like finding out which ingredients make a dish taste amazing! We might use things like correlation or mutual information to get a feel.\n",
        "\n",
        "**Model-Based Feature Importances**:\n",
        "\n",
        "- Imagine building a LEGO castle. Each brick represents a feature, and some bricks are more crucial than others.\n",
        "\n",
        "- If we’re talking linear models (like regressors or classifiers), we create a fancy formula that looks like this:\n",
        "\n",
        "  $$y-hat = w_0 + w_1 x_1 + ... + w_n x_n$$\n",
        "\n",
        "  Here, the weights (those 'w' values) tell us the importance of each feature.\n",
        "\n",
        "- **Quick tip:** Make sure to scale your features before fitting; it’s like making sure all the LEGO bricks are the same size.\n",
        "\n",
        "- If you’re using scikit-learn, you can peek at these weights with `model.coef_`.\n",
        "\n",
        "**Tree-Based Methods**:\n",
        "\n",
        "- Trees are cool! They **split data based on features** and see which ones **clear up confusion the best.**\n",
        "\n",
        "- If you’ve got a bunch of trees (like in a forest), you **average out the results from all trees.**\n",
        "\n",
        "- Again, if you're using scikit-learn, check out `model.feature_importances_` to see the star players.\n"
      ],
      "metadata": {
        "id": "wCQIts2cts6O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature importance in Decision Trees\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-5-Clustering%20and%20Features/imgs/select3.png' width=500px >\n",
        "\n",
        "Decision trees are a standout in the machine learning landscape due to their **intrinsic interpretability**. They provide a visual representation where each decision made by the tree can be **seen and understood**. Each path from the tree's root to a leaf signifies a **specific rule**. For instance, an internal node in the tree making a decision based on the feature \"Age < 25\" is straightforward to comprehend.\n",
        "\n",
        "**Feature importance** is another area where decision trees shine. While the tree itself is **transparent**, its underlying mechanism offers valuable insights that can be used to **explain even more complex models**, like **random forests** or **gradient boosting machines**. In decision trees, the **importance** of a feature is often computed based on the **total reduction of a criterion**, such as the **Gini impurity or entropy**, brought about by that feature. A higher value indicates that the feature plays a more pivotal role in making decisions within the tree.\n",
        "\n",
        "On the other hand, in linear regression models, c**oefficients give us a direct measure of a feature's importance in a linear context**. Each coefficient indicates the change in the output for a unit change in that feature, assuming other features remain constant. However, the **simplicity of these coefficients is also their limitation**. They provide insights into linear relationships but **can't capture non-linear interactions between features**.\n",
        "\n",
        "**That's where decision trees fill the gap**. They, and by extension, **ensemble methods like random forests**, are adept at capturing non-linear relationships. The feature importance metrics from trees shed light on which features are most critical in decision-making, even in intricate, non-linear scenarios. By **juxtaposing the coefficients from linear models with the feature importance derived from decision trees**, one can obtain a nuanced understanding of the dataset. Linear models provide insights into the linear importance of each feature, while decision trees offer a broader perspective, encapsulating both linear and non-linear interactions.\n",
        "\n",
        "In summary, decision trees are not only easy to interpret on their own, but their ability to rank features based on importance makes them a robust tool for explaining a variety of machine learning models, linear or non-linear."
      ],
      "metadata": {
        "id": "_LR79hFUD4Fn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Univariate feature selection: Pearson’s correlation coefficient\n",
        "\n",
        "1. **What's the gist?** We want to pick out the most relevant features for our data analysis. One way to do this is by looking at the Pearson’s correlation coefficient.\n",
        "2. **How do we use it?** Think of this coefficient as a measure of how related two sets of data are. If a feature has a coefficient close to +1 or -1, it means it's strongly correlated with our target outcome.\n",
        "3. **But wait, there's noise!** Sometimes, features might not show a clear linear relationship because of noise or non-linear patterns. For instance, a very wiggly feature or one with lots of random spikes might not have a strong correlation.\n",
        "4. **So how do we pick the best features?** With univariate feature selection, we rank the features based on their importance and pick the top ones. Tools like `SelectKBest` from `scikit-learn` can help with this.\n",
        "5. **A note on scikit-learn:** While it doesn't calculate the Pearson’s coefficient directly, it has a nifty function called `f_regression`. This gives us an F-value, which essentially tells us how likely it is that a feature's correlation with the target is just by chance. The good news? This F-value is closely tied to the Pearson’s coefficient.\n",
        "6. **Using SelectKBest:** This tool needs two things from us - a way to score features (like our `f_regression` function) and the number of top features we want.\n",
        "7. **What did we find?** For our data, the top three features (0, 1, and 2) have a straightforward, linear relationship with our target values.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-5-Clustering%20and%20Features/imgs/pearson_select.png' width=700px >\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-5-Clustering%20and%20Features/imgs/peardon2.png' width=500px >\n",
        "\n",
        "Let's focus on univariate feature selection. This method allows us to assess the importance of individual features based on Pearson's correlation coefficient with the target values.\n",
        "\n",
        "Notably, features that exhibit a linear or near-linear relationship with the target will have a **high correlation** value. However, this correlation is diminished by factors like non-linearity and noise. This is evident in features like the highly **non-linear 'feature 3'** and the **noisy 'feature 4'**, both of which show almost **negligible correlation**, making them **unlikely choices for selection**.\n",
        "\n",
        "In the univariate selection process, features are ranked based on their significance, and the best-performing ones are chosen. As an example, we can use the `SelectKBest` module from scikit-learn to **select the top 3 features**. Although scikit-learn doesn't provide a direct method to compute Pearson’s correlation coefficient, it does offer the `f_regression` object, which yields an F-value. **This F-value evaluates the likelihood that the regression coefficient differs from zero**, essentially acting as a hypothesis test. Intriguingly, the F-value is intrinsically linked to Pearson’s correlation coefficient. When utilizing `SelectKBest`, it requires two main inputs: a scoring function (in this context, `f_regression`) and the number of features we wish to select.\n",
        "\n",
        "Upon examination, we find that the leading three features, labeled as 0, 1, and 2, all exhibit a linear correlation with the target values."
      ],
      "metadata": {
        "id": "p3Ovd1f2uus5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Univariate feature selection: Mutual Information\n",
        "\n",
        "Ready to find out which features are the best buddies with your target?\n",
        "\n",
        "1. **What's Mutual Information?** Think of it like a friendship meter. It tells us how much two variables, in our case, feature values and target values, have in common. The more they \"know\" about each other, the higher the mutual information!\n",
        "2. **How to pick the best buddies?** We're on the lookout for features that share a lot of secrets (information) with our target. But sometimes, the chatter (noise) can get in the way and lessen their bond.\n",
        "3. **Unique and Wiggly Relations? No Problem!** The cool thing about mutual information is that it's unfazed by whether the relationship is straight or all over the place. In fact, a super wiggly feature can sometimes share more info with the target than a straight-line feature.\n",
        "4. **How to Measure Friendship in Code?** `Scikit-learn` is our matchmaker here! It has a tool called `mutual_info_regression` that measures how tight-knit our features and targets are. If we were to pick the top three BFFs using `SelectKBest`, it'd likely be features 1, 2, and 3!\n",
        "5. **But wait, what about Classification?** Ah, for that, `scikit-learn` has different measuring tapes. The top ones are:\n",
        "   - `chi2` (chi squared)\n",
        "   - `f_classif`\n",
        "   - `mutual_info_classif`\n"
      ],
      "metadata": {
        "id": "rmWHp1E6vAtO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we apply mutual information, the scenario shifts. Mutual information gauges the shared information between two variables, in this context, between feature values and target values. It becomes evident that while noise diminishes importance, factors like non-linearity or uniqueness don't have the same effect.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-5-Clustering%20and%20Features/imgs/mutual_info.png' width=700px >\n",
        "\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-5-Clustering%20and%20Features/imgs/mutual2.png' width=500px >\n",
        "\n",
        "\n",
        "In fact, the highly non-linear and non-unique 'feature 3' exhibits a stronger mutual information with the targets compared to the nearly linear 'feature 2'.\n",
        "\n",
        "Scikit-learn provides the `mutual_info_regression` function to compute the mutual information of each feature in relation to the targets. Utilizing `SelectKBest` to cherry-pick the top three features, we'd select features 1, 2, and 3.\n",
        "\n",
        "It's worth noting that the examples discussed are geared towards regression. For classification tasks, scikit-learn presents scoring functions like chi-squared, `f_classif`, and `mutual_info_classif`."
      ],
      "metadata": {
        "id": "uGXcl7dfQPjd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model based feature selection: Lasso\n",
        "\n",
        "**What's the Lasso Way?**\n",
        "\n",
        "1. Lasso is like a talent scout. It zeroes in on the most impactful features by giving them the highest \"coefficients\" or weights.\n",
        "\n",
        "2. There's some magic behind the scenes! When Lasso uses the L1 norm penalty, it makes many feature weights zero, resulting in a simpler model. This is called \"sparsity.\"\n",
        "\n",
        "**Tuning with LassoCV:**\n",
        "\n",
        "Lasso has this cool feature called \"LassoCV.\" It's like a radio that automatically tunes to the best station! In our case, it finds the ideal setting for the hyperparameter 𝜆. And for our little example here, it settled on 0.003.\n",
        "\n",
        "**Picking the Stars with SelectFromModel:**\n",
        "\n",
        "Sklearn has this nifty tool called `SelectFromModel`. Think of it as a director casting the leading roles in a movie. It selects features based on how impactful (or high) their coefficients are. And yep, for our movie, features 1 and 2 got the leading roles! They're pretty straightforward and don't come with a lot of drama (noise)."
      ],
      "metadata": {
        "id": "zRfqTjzGvtOj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shifting our focus to model-based feature selection, we already know that the incorporation of the **L1 norm** during training induces model **sparsity**, which in turn **facilitates direct feature selection**. Given that we are dealing with a regression task, we'll employ the Lasso model to ascertain feature importance. In this demonstration, we're using the `LassoCV` object since it autonomously tunes the **alpha hyperparameter**, which, for this instance, settled at 0.003.\n",
        "\n",
        "For feature selection in this context, we turn to the scikit-learn `SelectFromModel` object. This requires a machine learning model possessing the `coef_` attribute for input. Additionally, we have the liberty to set a coefficient threshold for feature selection. As observed, features 1 and 2 were chosen, both of which exhibit either linear traits or near-linear attributes and are minimally affected by noise.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-5-Clustering%20and%20Features/imgs/lasso.png' width=700px >\n",
        "\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-5-Clustering%20and%20Features/imgs/lasso2.png' width=500px >\n"
      ],
      "metadata": {
        "id": "YQi1fNpvRuLY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model based feature selection: Random Forest\n",
        "\n",
        "So, with Random Forest guiding us, we're equipped to uncover those truly special features that can make our predictions shine!\n",
        "\n",
        "Let's venture into the world of \"Random Forest\" for feature selection!\n",
        "\n",
        "**The Random Forest Way:**\n",
        "\n",
        "1. Random Forest, like a wise old sage, picks features based on their \"importance.\" How do they get this wisdom? It's by calculating how much each feature clears up the uncertainty (or decreases the impurity) in a decision tree.\n",
        "2. Picture a bunch of trees, each with their own opinions. Random Forest's \"feature importance\" is the average clarity (or decrease in impurity) each feature brings across all these trees.\n",
        "\n",
        "**Selecting the Shining Stars:**\n",
        "\n",
        "Using `SelectFromModel`, we can handpick those standout features. Just like before, but this time, it's based on `feature_importances_`. And for this act, features 1, 2, and 3 stole the show!\n",
        "\n",
        "**A Special Note on Feature 3:**\n",
        "\n",
        "Isn't it fascinating? Even though Feature 3 dances to its own non-linear tune and doesn't have a unique bond with the target, Random Forest still recognizes its worth! It's like appreciating a unique dancer in a troupe. Teaming up with Feature 1, they create a magical performance.\n"
      ],
      "metadata": {
        "id": "Qj9HblnqwaE_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-5-Clustering%20and%20Features/imgs/random.png' width=700px >\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-5-Clustering%20and%20Features/imgs/random_forest2.png' width=500px >\n",
        "\n",
        "We can use the Random Forest model to help choose important features. The Random Forest model gave a high score to feature 3, even though it's not straight-forward and doesn't have a direct link to the target. This shows that the Random Forest model can handle tricky features well. Even if a feature isn't directly linked, it can still be useful when used with others, like feature 1.\n",
        "\n",
        "Using `SelectFromModel`, we pick the features based on their importance scores. This time, we pick features 1, 2, and 3.\n"
      ],
      "metadata": {
        "id": "LvRu34m_TUZ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recursive Feature Elimination\n",
        "\n",
        "Peeling the onion layer by layer, that's what \"Recursive Feature Elimination\" (RFE) is all about!\n",
        "\n",
        "**Deep Dive into RFE:**\n",
        "\n",
        "1. RFE doesn't just pick features; it *ranks* them. How? By starting with all features and taking them away, one by one, from least important to most.\n",
        "2. Here's the fun part: This isn't a one-time deal. With each feature removed, the model retrains and recalculates the importances. Think of it as a reality show, where contestants are voted off one by one, based on their performance in each episode.\n",
        "\n",
        "**Steps in RFE's Dance:**\n",
        "\n",
        "1. First, we fit a ranking model. For our case, we're using Ridge regression.\n",
        "2. After that, we find the least important feature and bid it goodbye.\n",
        "3. Rinse and repeat! We keep training the model with the remaining features and keep eliminating the weakest links.\n",
        "\n",
        "By the end, we get a ranking of all features from the star performers (last ones standing) to the early departures (first ones out).\n",
        "\n",
        "**Special Tools for RFE:**\n",
        "\n",
        "If you're a fan of automation, you'd love `RFECV` from sklearn. It's RFE with a sprinkle of cross-validation magic. Instead of manually picking the number of features, `RFECV` finds the optimal number that maximizes model performance.\n",
        "\n",
        "For our Ridge regression escapade, features 1 and 2 came out on top. Bravo!"
      ],
      "metadata": {
        "id": "0LTv90rlxJ85"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another way to determine important features is by using recursive feature elimination. In this method, we use a machine learning model to rank the features based on their importance. The least important feature is then removed and the model is tested again. This process continues, with one feature being removed at a time, until we have a ranking of features from most to least important. By doing this, we can decide on the top features we want to keep. If we use tools like RFECV from sklearn, it can even automatically choose the best number of features for us. When we tried this with the Ridge regression model, it selected features 1 and 2 as the most important.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-5-Clustering%20and%20Features/imgs/recursive.png' width=700px >\n",
        "\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-5-Clustering%20and%20Features/imgs/recursive2.png' width=500px >\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MV9ClgquU_qV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature selection results - summary\n",
        "\n",
        "Time for a feature selection showdown summary!\n",
        "\n",
        "**The Stars of the Show:**\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-5-Clustering%20and%20Features/imgs/1.png' width=200px >\n",
        "\n",
        "\n",
        "- Features 1 and 2 are the headliners, chosen by all the methods. They're as straight as an arrow, which makes them easy to work with, and they keep the noise level down.\n",
        "\n",
        "**Feature 3, the Underdog:**\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-5-Clustering%20and%20Features/imgs/2.png' width=100px >\n",
        "\n",
        "- Feature 3 may not have a unique relationship with the target, but it's a versatile player. Mutual information and Random Forest recognized its value. It brings a dose of non-linearity to the party and keeps the noise to a minimum.\n",
        "- In fact, it even outshone Feature 2 in the eyes of Random Forest and Mutual Information!\n",
        "\n",
        "**The Noise Maker, Feature 4:**\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-5-Clustering%20and%20Features/imgs/3.png' width=100px >\n",
        "\n",
        "\n",
        "- Last but not least, Feature 4 got the boot every time. It's like that one noisy neighbor that no one wants to invite to the party. Just full of noise, no real connection to the target.\n",
        "\n",
        "So, there you have it! Our feature selection methods have spoken, and the winners are Features 1, 2, and 3. They bring the harmony, while Feature 4 got left out in the noise."
      ],
      "metadata": {
        "id": "aZeq2_6xx3Sy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Application: Prediction of age at scan\n",
        "\n",
        "Let's circle back to our brainy task of predicting age at scan for preterm neonates using volumes of 86 brain structures.\n",
        "\n",
        "**The Overfitting Conundrum:**\n",
        "\n",
        "You see, when we initially tried to tackle this with multivariate linear regression, it was like trying to fit a square peg into a round hole - overfitting was lurking around the corner.\n",
        "\n",
        "**Enter Feature Selection, Our Hero!**\n",
        "\n",
        "But hold on, can feature selection actually save the day and prevent overfitting?\n",
        "\n",
        "**Drumroll, Please... Conclusion Time!**\n",
        "\n",
        "The answer is a resounding YES! Feature selection turned out to be our trusty sidekick in this adventure. It helped us avoid overfitting and led to some impressive results.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-5-Clustering%20and%20Features/imgs/application.png' width=700px >\n",
        "\n",
        "\n",
        "We tested three feature selection methods:\n",
        "\n",
        "1. Univariate feature selection using correlation (picking 2 features).\n",
        "2. Model-based feature selection using Lasso (choosing 6 features).\n",
        "3. Recursive Feature Elimination (RFE) with a linear regression model (opting for 4 features).\n",
        "\n",
        "And the winner of this feature selection showdown is... Lasso! It performed the best, closely followed by RFE. These methods worked their magic and gave us results akin to using Ridge Regression, a pretty effective model.\n",
        "\n",
        "So there you have it! Feature selection not only prevents overfitting but also boosts our performance. It's like finding the perfect-sized puzzle piece for our prediction task."
      ],
      "metadata": {
        "id": "cA-Y_5T-yS8M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interpretation of feature importances\n",
        "\n",
        "**Understanding Feature Importances:**\n",
        "\n",
        "In the realm of machine learning, feature importances provide us with valuable insights into which features carry the most weight when it comes to making predictions. These insights can be enlightening and help us better understand the problem we're trying to solve.\n",
        "\n",
        "**The Challenge of Correlated Features:**\n",
        "\n",
        "However, there's a challenge we often face. Imagine you have a group of features that are closely related or correlated. It's like having multiple teammates who are equally skilled. Sometimes, when we use feature selection methods, they might end up choosing just one of these correlated features to represent the entire group. This can be a problem because some of those highly predictive features might not get the recognition they deserve, leading to lower importances for them.\n",
        "\n",
        "**Comparing Feature Selection Methods:**\n",
        "\n",
        "Let's take a look at an example where we used three different feature selection methods. Each of these methods selected its own set of top three features. This variety in selection can make it tricky to interpret which features are truly the most important.\n",
        "\n",
        "So, in a nutshell, feature importances are like guiding lights in the world of data, showing us which features matter most. But when features are closely connected, we might end up highlighting just one, and that can sometimes overshadow other valuable features. It's all part of the data exploration journey!"
      ],
      "metadata": {
        "id": "Ld01MWZiy4YZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature interpretation\n",
        "\n",
        "The instability we encounter here makes it tricky to confidently interpret the significance of individual features. However, if our primary aim is to reduce dimensionality rather than dissect individual features, this instability might not be a big concern.\n",
        "\n",
        "There are some clever solutions to address this issue:\n",
        "\n",
        "1. **Stability Selection:** This involves running the feature selection method on various subsets of the data and then calculating statistics to determine how often each feature is selected.\n",
        "\n",
        "2. **Feature Merging:** Another approach is to identify pairs or clusters of highly correlated features and make a decision. You can either drop the less predictive ones or merge them into a single representative feature.\n",
        "\n",
        "These strategies help us navigate the instability and make the most out of our feature selection process."
      ],
      "metadata": {
        "id": "aOrOVbJ4zlV3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comic time\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-5-Clustering%20and%20Features/imgs/comic2.jpg' width=500px >\n"
      ],
      "metadata": {
        "id": "-1h2KpMtz4yU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Manifold learning\n",
        "\n",
        "Manifold learning is a technique used in machine learning and data analysis to uncover the underlying structure or geometry of complex, high-dimensional data. In simpler terms, it helps us make sense of data that doesn't adhere to a straightforward, linear pattern.\n",
        "\n",
        "When we say \"manifold,\" we're referring to a mathematical concept that represents a lower-dimensional, continuous surface within a higher-dimensional space. Manifold learning algorithms aim to find this lower-dimensional representation of the data, where the relationships between data points become more apparent and understandable.\n",
        "\n",
        "The primary goal of manifold learning is dimensionality reduction while preserving essential information. It's particularly useful when dealing with data that exhibits non-linear relationships or when you want to visualize high-dimensional data in a more manageable form.\n",
        "\n",
        "Some commonly used manifold learning techniques, in addition to Laplacian Eigenmap mentioned earlier, include Isomap, t-SNE (t-Distributed Stochastic Neighbor Embedding), and LLE (Locally Linear Embedding). These methods help uncover the hidden structure in data, making it easier to analyze and interpret complex datasets."
      ],
      "metadata": {
        "id": "EIS9RR1j0VkR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Laplacian Eigenmap\n",
        "\n",
        "In the context of manifold learning, Laplacian Eigenmap is a dimensionality reduction technique used to uncover the underlying structure of complex, non-linear datasets. It's a method that helps transform high-dimensional data into a lower-dimensional space while preserving the essential relationships between data points.\n",
        "\n",
        "Here's a more detailed explanation:\n",
        "\n",
        "1. **Graph Representation:** Laplacian Eigenmap starts by representing the data as a graph, where data points are nodes, and edges are drawn between points that are similar to each other in some way. This similarity can be based on distance, correlation, or other measures.\n",
        "\n",
        "2. **Graph Laplacian:** Next, it computes a mathematical construct known as the \"Graph Laplacian.\" This construct captures how connected or isolated each data point is within the graph. It's a matrix that encodes the relationships between data points.\n",
        "\n",
        "3. **Eigenvalues and Eigenvectors:** Laplacian Eigenmap then calculates the eigenvalues and corresponding eigenvectors of this Graph Laplacian. These eigenvalues and eigenvectors provide insight into the hidden structure of the data.\n",
        "\n",
        "4. **Dimension Reduction:** The eigenvalues are ordered from smallest to largest, and the corresponding eigenvectors are used to create a new, lower-dimensional embedded space. This new space represents the data in a way that highlights its non-linear structure.\n",
        "\n",
        "5. **Clustering or Visualization:** Finally, this lower-dimensional space can be used for various purposes, such as clustering similar data points together or visualizing the data in a more interpretable form.\n",
        "\n",
        "Laplacian Eigenmap is particularly effective when dealing with data that doesn't follow a linear pattern, as it aims to capture the non-linear relationships between data points. It's a valuable tool in manifold learning for understanding and making sense of complex, high-dimensional datasets."
      ],
      "metadata": {
        "id": "U-dv6hU10ude"
      }
    }
  ]
}