{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOL5lgZsyLhlHUjERBe+Z2A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaralAminpour/IVM_supplementary_materials/blob/main/feature_selection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part III: Feature Selection"
      ],
      "metadata": {
        "id": "2tORMVGnUiAv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why we select certain features over others?\n",
        "\n",
        "Ever wonder why we select certain features over others? Well, there are some pretty cool reasons:\n",
        "\n",
        "1. **To Avoid Overfitting**: Think of it as decluttering. By removing extra stuff (or redundant data), our model doesn't get sidetracked by unnecessary noise.\n",
        "2. **Boost Accuracy**: It's like focusing on the main story without the side plots. By cutting out misleading data, our model can be more on-point and accurate.\n",
        "3. **Speed Things Up**: Everyone loves a quick result, right? With fewer features, our model can sprint through the training process, making everything snappier.\n"
      ],
      "metadata": {
        "id": "RSsqIqlqU7ux"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Application: Prediction of age at scan\n",
        "\n",
        "Let's focus on predicting age based on brain scans. Remember when we talked about predicting age from those 86 brain structure volumes? Using just a straightforward multivariate linear regression might make our model a bit too eager and overfit the training data. The clue? A big difference in how the model does on the full training set versus when tested with cross-validation.\n",
        "\n",
        "Now, throughout our journey, we've come across several cool techniques to prevent this overfitting. Do any of them ring a bell?\n",
        "\n",
        "- Keeping our model's enthusiasm in check with **regularisation** (like Ridge or Lasso).\n",
        "- Reducing the number of features with methods like **PCA, ICA, or Laplacian Eigenmap**.\n",
        "- Bringing together multiple models in **ensemble learning** (like our buddy, the Random Forest).\n",
        "- And of course, being picky with our data using **feature selection**!\n",
        "\n",
        "Quick recap: We aim to predict the age at scan using volumes of 86 brain structures in preterm babies. Just using multivariate linear regression? Oops, we overfit!\n",
        "\n",
        "So, what's in our toolkit to prevent overfitting? Regularisation, dimensionality reduction, ensemble learning, and feature selection. Got it? Onward!"
      ],
      "metadata": {
        "id": "salPD6QqsPMj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## simulated features\n",
        "\n",
        " Imagine we've got five of these features, each sprinkled with varying amounts of noise and non-linearity. Picture it like this:\n",
        "\n",
        "- The x-axis is showing the feature value.\n",
        "- The y-axis? That’s our target value.\n",
        "\n",
        "Now, our goal is to pick the best and most informative features. To help visualize this, we've simulated five features:\n",
        "\n",
        "1. The first feature has a straight-line relationship with the targets, but it's kinda like static on a TV – a bit noisy.\n",
        "2. The second one? Still a straight-line relationship, but clearer than a sunny day – much less noise.\n",
        "3. Our third feature's relationship with the target is more like a wavy line, a bit curvy but still pretty clear.\n",
        "4. The fourth one is like a roller coaster, super curvy and unpredictable. It’s not a straight shot to our target, but it’s clear without much noise.\n",
        "5. And the fifth? Well, that's just like radio static – all noise and no clear connection to our targets."
      ],
      "metadata": {
        "id": "KzJfcGXvtdh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature importances\n",
        "\n",
        "Feature importances helps us understand which features really make a difference when making predictions.\n",
        "\n",
        "**Univariate Feature Importances**:\n",
        "- Think of this as looking at one feature at a time.\n",
        "- We're trying to see how good each feature is at predicting the target values.\n",
        "- It’s kinda like finding out which ingredients make a dish taste amazing! We might use things like correlation or mutual information to get a feel.\n",
        "\n",
        "**Model-Based Feature Importances**:\n",
        "- Imagine building a LEGO castle. Each brick represents a feature, and some bricks are more crucial than others.\n",
        "- If we’re talking linear models (like regressors or classifiers), we create a fancy formula that looks like this:\n",
        "  $$y-hat = w_0 + w_1 x_1 + ... + w_n x_n$$\n",
        "  Here, the weights (those 'w' values) tell us the importance of each feature.\n",
        "- Quick tip: Make sure to scale your features before fitting; it’s like making sure all the LEGO bricks are the same size.\n",
        "- If you’re using scikit-learn, you can peek at these weights with `model.coef_`.\n",
        "\n",
        "**Tree-Based Methods**:\n",
        "- Trees are cool! They split data based on features and see which ones clear up confusion the best.\n",
        "- If you’ve got a bunch of trees (like in a forest), you average out the results from all trees.\n",
        "- Again, if you're using scikit-learn, check out `model.feature_importances_` to see the star players.\n"
      ],
      "metadata": {
        "id": "wCQIts2cts6O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Univariate feature selection\n",
        "\n",
        "1. **What's the gist?** We want to pick out the most relevant features for our data analysis. One way to do this is by looking at the Pearson’s correlation coefficient.\n",
        "2. **How do we use it?** Think of this coefficient as a measure of how related two sets of data are. If a feature has a coefficient close to +1 or -1, it means it's strongly correlated with our target outcome.\n",
        "3. **But wait, there's noise!** Sometimes, features might not show a clear linear relationship because of noise or non-linear patterns. For instance, a very wiggly feature or one with lots of random spikes might not have a strong correlation.\n",
        "4. **So how do we pick the best features?** With univariate feature selection, we rank the features based on their importance and pick the top ones. Tools like `SelectKBest` from `scikit-learn` can help with this.\n",
        "5. **A note on scikit-learn:** While it doesn't calculate the Pearson’s coefficient directly, it has a nifty function called `f_regression`. This gives us an F-value, which essentially tells us how likely it is that a feature's correlation with the target is just by chance. The good news? This F-value is closely tied to the Pearson’s coefficient.\n",
        "6. **Using SelectKBest:** This tool needs two things from us - a way to score features (like our `f_regression` function) and the number of top features we want.\n",
        "7. **What did we find?** For our data, the top three features (0, 1, and 2) have a straightforward, linear relationship with our target values."
      ],
      "metadata": {
        "id": "p3Ovd1f2uus5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Univariate feature selection2\n",
        "\n",
        "Ready to find out which features are the best buddies with your target?\n",
        "\n",
        "1. **What's Mutual Information?** Think of it like a friendship meter. It tells us how much two variables, in our case, feature values and target values, have in common. The more they \"know\" about each other, the higher the mutual information!\n",
        "2. **How to pick the best buddies?** We're on the lookout for features that share a lot of secrets (information) with our target. But sometimes, the chatter (noise) can get in the way and lessen their bond.\n",
        "3. **Unique and Wiggly Relations? No Problem!** The cool thing about mutual information is that it's unfazed by whether the relationship is straight or all over the place. In fact, a super wiggly feature can sometimes share more info with the target than a straight-line feature.\n",
        "4. **How to Measure Friendship in Code?** `Scikit-learn` is our matchmaker here! It has a tool called `mutual_info_regression` that measures how tight-knit our features and targets are. If we were to pick the top three BFFs using `SelectKBest`, it'd likely be features 1, 2, and 3!\n",
        "5. **But wait, what about Classification?** Ah, for that, `scikit-learn` has different measuring tapes. The top ones are:\n",
        "   - `chi2` (chi squared)\n",
        "   - `f_classif`\n",
        "   - `mutual_info_classif`\n"
      ],
      "metadata": {
        "id": "rmWHp1E6vAtO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model based feature selection\n",
        "\n",
        "**What's the Lasso Way?**\n",
        "\n",
        "1. Lasso is like a talent scout. It zeroes in on the most impactful features by giving them the highest \"coefficients\" or weights.\n",
        "2. There's some magic behind the scenes! When Lasso uses the L1 norm penalty, it makes many feature weights zero, resulting in a simpler model. This is called \"sparsity.\"\n",
        "\n",
        "**Tuning with LassoCV:**\n",
        "\n",
        "Lasso has this cool feature called \"LassoCV.\" It's like a radio that automatically tunes to the best station! In our case, it finds the ideal setting for the hyperparameter 𝜆. And for our little example here, it settled on 0.003.\n",
        "\n",
        "**Picking the Stars with SelectFromModel:**\n",
        "\n",
        "Sklearn has this nifty tool called `SelectFromModel`. Think of it as a director casting the leading roles in a movie. It selects features based on how impactful (or high) their coefficients are. And yep, for our movie, features 1 and 2 got the leading roles! They're pretty straightforward and don't come with a lot of drama (noise)."
      ],
      "metadata": {
        "id": "zRfqTjzGvtOj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model based feature selection\n",
        "\n",
        "So, with Random Forest guiding us, we're equipped to uncover those truly special features that can make our predictions shine!\n",
        "\n",
        "Let's venture into the world of \"Random Forest\" for feature selection!\n",
        "\n",
        "**The Random Forest Way:**\n",
        "\n",
        "1. Random Forest, like a wise old sage, picks features based on their \"importance.\" How do they get this wisdom? It's by calculating how much each feature clears up the uncertainty (or decreases the impurity) in a decision tree.\n",
        "2. Picture a bunch of trees, each with their own opinions. Random Forest's \"feature importance\" is the average clarity (or decrease in impurity) each feature brings across all these trees.\n",
        "\n",
        "**Selecting the Shining Stars:**\n",
        "\n",
        "Using `SelectFromModel`, we can handpick those standout features. Just like before, but this time, it's based on `feature_importances_`. And for this act, features 1, 2, and 3 stole the show!\n",
        "\n",
        "**A Special Note on Feature 3:**\n",
        "\n",
        "Isn't it fascinating? Even though Feature 3 dances to its own non-linear tune and doesn't have a unique bond with the target, Random Forest still recognizes its worth! It's like appreciating a unique dancer in a troupe. Teaming up with Feature 1, they create a magical performance.\n"
      ],
      "metadata": {
        "id": "Qj9HblnqwaE_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recursive Feature Elimination\n",
        "\n",
        "Peeling the onion layer by layer, that's what \"Recursive Feature Elimination\" (RFE) is all about!\n",
        "\n",
        "**Deep Dive into RFE:**\n",
        "\n",
        "1. RFE doesn't just pick features; it *ranks* them. How? By starting with all features and taking them away, one by one, from least important to most.\n",
        "2. Here's the fun part: This isn't a one-time deal. With each feature removed, the model retrains and recalculates the importances. Think of it as a reality show, where contestants are voted off one by one, based on their performance in each episode.\n",
        "\n",
        "**Steps in RFE's Dance:**\n",
        "\n",
        "1. First, we fit a ranking model. For our case, we're using Ridge regression.\n",
        "2. After that, we find the least important feature and bid it goodbye.\n",
        "3. Rinse and repeat! We keep training the model with the remaining features and keep eliminating the weakest links.\n",
        "\n",
        "By the end, we get a ranking of all features from the star performers (last ones standing) to the early departures (first ones out).\n",
        "\n",
        "**Special Tools for RFE:**\n",
        "\n",
        "If you're a fan of automation, you'd love `RFECV` from sklearn. It's RFE with a sprinkle of cross-validation magic. Instead of manually picking the number of features, `RFECV` finds the optimal number that maximizes model performance.\n",
        "\n",
        "For our Ridge regression escapade, features 1 and 2 came out on top. Bravo!"
      ],
      "metadata": {
        "id": "0LTv90rlxJ85"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature selection results - summary\n",
        "\n",
        "Time for a feature selection showdown summary!\n",
        "\n",
        "**The Stars of the Show:**\n",
        "\n",
        "- Features 1 and 2 are the headliners, chosen by all the methods. They're as straight as an arrow, which makes them easy to work with, and they keep the noise level down.\n",
        "\n",
        "**Feature 3, the Underdog:**\n",
        "\n",
        "- Feature 3 may not have a unique relationship with the target, but it's a versatile player. Mutual information and Random Forest recognized its value. It brings a dose of non-linearity to the party and keeps the noise to a minimum.\n",
        "- In fact, it even outshone Feature 2 in the eyes of Random Forest and Mutual Information!\n",
        "\n",
        "**The Noise Maker, Feature 4:**\n",
        "\n",
        "- Last but not least, Feature 4 got the boot every time. It's like that one noisy neighbor that no one wants to invite to the party. Just full of noise, no real connection to the target.\n",
        "\n",
        "So, there you have it! Our feature selection methods have spoken, and the winners are Features 1, 2, and 3. They bring the harmony, while Feature 4 got left out in the noise."
      ],
      "metadata": {
        "id": "aZeq2_6xx3Sy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Application: Prediction of age at scan\n",
        "\n",
        "Let's circle back to our brainy task of predicting age at scan for preterm neonates using volumes of 86 brain structures.\n",
        "\n",
        "**The Overfitting Conundrum:**\n",
        "\n",
        "You see, when we initially tried to tackle this with multivariate linear regression, it was like trying to fit a square peg into a round hole - overfitting was lurking around the corner.\n",
        "\n",
        "**Enter Feature Selection, Our Hero!**\n",
        "\n",
        "But hold on, can feature selection actually save the day and prevent overfitting?\n",
        "\n",
        "**Drumroll, Please... Conclusion Time!**\n",
        "\n",
        "The answer is a resounding YES! Feature selection turned out to be our trusty sidekick in this adventure. It helped us avoid overfitting and led to some impressive results.\n",
        "\n",
        "We tested three feature selection methods:\n",
        "\n",
        "1. Univariate feature selection using correlation (picking 2 features).\n",
        "2. Model-based feature selection using Lasso (choosing 6 features).\n",
        "3. Recursive Feature Elimination (RFE) with a linear regression model (opting for 4 features).\n",
        "\n",
        "And the winner of this feature selection showdown is... Lasso! It performed the best, closely followed by RFE. These methods worked their magic and gave us results akin to using Ridge Regression, a pretty effective model.\n",
        "\n",
        "So there you have it! Feature selection not only prevents overfitting but also boosts our performance. It's like finding the perfect-sized puzzle piece for our prediction task."
      ],
      "metadata": {
        "id": "cA-Y_5T-yS8M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interpretation of feature importances\n",
        "\n",
        "**Understanding Feature Importances:**\n",
        "\n",
        "In the realm of machine learning, feature importances provide us with valuable insights into which features carry the most weight when it comes to making predictions. These insights can be enlightening and help us better understand the problem we're trying to solve.\n",
        "\n",
        "**The Challenge of Correlated Features:**\n",
        "\n",
        "However, there's a challenge we often face. Imagine you have a group of features that are closely related or correlated. It's like having multiple teammates who are equally skilled. Sometimes, when we use feature selection methods, they might end up choosing just one of these correlated features to represent the entire group. This can be a problem because some of those highly predictive features might not get the recognition they deserve, leading to lower importances for them.\n",
        "\n",
        "**Comparing Feature Selection Methods:**\n",
        "\n",
        "Let's take a look at an example where we used three different feature selection methods. Each of these methods selected its own set of top three features. This variety in selection can make it tricky to interpret which features are truly the most important.\n",
        "\n",
        "So, in a nutshell, feature importances are like guiding lights in the world of data, showing us which features matter most. But when features are closely connected, we might end up highlighting just one, and that can sometimes overshadow other valuable features. It's all part of the data exploration journey!"
      ],
      "metadata": {
        "id": "Ld01MWZiy4YZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature interpretation\n",
        "\n",
        "The instability we encounter here makes it tricky to confidently interpret the significance of individual features. However, if our primary aim is to reduce dimensionality rather than dissect individual features, this instability might not be a big concern.\n",
        "\n",
        "There are some clever solutions to address this issue:\n",
        "\n",
        "1. **Stability Selection:** This involves running the feature selection method on various subsets of the data and then calculating statistics to determine how often each feature is selected.\n",
        "\n",
        "2. **Feature Merging:** Another approach is to identify pairs or clusters of highly correlated features and make a decision. You can either drop the less predictive ones or merge them into a single representative feature.\n",
        "\n",
        "These strategies help us navigate the instability and make the most out of our feature selection process."
      ],
      "metadata": {
        "id": "aOrOVbJ4zlV3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comic time\n",
        "\n"
      ],
      "metadata": {
        "id": "-1h2KpMtz4yU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Manifold learning\n",
        "\n",
        "Manifold learning is a technique used in machine learning and data analysis to uncover the underlying structure or geometry of complex, high-dimensional data. In simpler terms, it helps us make sense of data that doesn't adhere to a straightforward, linear pattern.\n",
        "\n",
        "When we say \"manifold,\" we're referring to a mathematical concept that represents a lower-dimensional, continuous surface within a higher-dimensional space. Manifold learning algorithms aim to find this lower-dimensional representation of the data, where the relationships between data points become more apparent and understandable.\n",
        "\n",
        "The primary goal of manifold learning is dimensionality reduction while preserving essential information. It's particularly useful when dealing with data that exhibits non-linear relationships or when you want to visualize high-dimensional data in a more manageable form.\n",
        "\n",
        "Some commonly used manifold learning techniques, in addition to Laplacian Eigenmap mentioned earlier, include Isomap, t-SNE (t-Distributed Stochastic Neighbor Embedding), and LLE (Locally Linear Embedding). These methods help uncover the hidden structure in data, making it easier to analyze and interpret complex datasets."
      ],
      "metadata": {
        "id": "EIS9RR1j0VkR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Laplacian Eigenmap\n",
        "\n",
        "In the context of manifold learning, Laplacian Eigenmap is a dimensionality reduction technique used to uncover the underlying structure of complex, non-linear datasets. It's a method that helps transform high-dimensional data into a lower-dimensional space while preserving the essential relationships between data points.\n",
        "\n",
        "Here's a more detailed explanation:\n",
        "\n",
        "1. **Graph Representation:** Laplacian Eigenmap starts by representing the data as a graph, where data points are nodes, and edges are drawn between points that are similar to each other in some way. This similarity can be based on distance, correlation, or other measures.\n",
        "\n",
        "2. **Graph Laplacian:** Next, it computes a mathematical construct known as the \"Graph Laplacian.\" This construct captures how connected or isolated each data point is within the graph. It's a matrix that encodes the relationships between data points.\n",
        "\n",
        "3. **Eigenvalues and Eigenvectors:** Laplacian Eigenmap then calculates the eigenvalues and corresponding eigenvectors of this Graph Laplacian. These eigenvalues and eigenvectors provide insight into the hidden structure of the data.\n",
        "\n",
        "4. **Dimension Reduction:** The eigenvalues are ordered from smallest to largest, and the corresponding eigenvectors are used to create a new, lower-dimensional embedded space. This new space represents the data in a way that highlights its non-linear structure.\n",
        "\n",
        "5. **Clustering or Visualization:** Finally, this lower-dimensional space can be used for various purposes, such as clustering similar data points together or visualizing the data in a more interpretable form.\n",
        "\n",
        "Laplacian Eigenmap is particularly effective when dealing with data that doesn't follow a linear pattern, as it aims to capture the non-linear relationships between data points. It's a valuable tool in manifold learning for understanding and making sense of complex, high-dimensional datasets."
      ],
      "metadata": {
        "id": "U-dv6hU10ude"
      }
    }
  ]
}