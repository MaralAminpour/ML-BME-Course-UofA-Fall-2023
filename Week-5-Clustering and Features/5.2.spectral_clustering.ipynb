{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPncjPuCoJDG1IkB/g+aNI7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaralAminpour/IVM_supplementary_materials/blob/main/spectral_clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part II: Spectral Clustering"
      ],
      "metadata": {
        "id": "TzMmNmp-EDo2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Moons dataset\n",
        "\n",
        "\"moons\" dataset is often used as a toy dataset in machine learning for classification tasks. This dataset contains two interleaving half circles (or moons), making it particularly suitable for experimenting with non-linear classification techniques.\n",
        "\n",
        "The moons dataset can be generated using libraries such as `scikit-learn`.\n"
      ],
      "metadata": {
        "id": "H5quacBCqNT2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code will produce a scatter plot of the moons dataset with two different colors, each representing one of the two classes in the dataset.\n",
        "Here's how you can generate and visualize it using Python:"
      ],
      "metadata": {
        "id": "qan5kmXZKRTt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_moons\n",
        "\n",
        "# Generate moons dataset\n",
        "X, y = make_moons(n_samples=100, noise=0.1, random_state=0)\n",
        "\n",
        "# Plotting the dataset\n",
        "plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='red', marker='o', edgecolor='black')\n",
        "plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='blue', marker='^', edgecolor='black')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WXUhPxbBJ3wk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Have you ever wondered how we could cluster moon-shaped data? Would popular methods like K-means or Gaussian Mixture do the trick? Let's find out.\n",
        "\n",
        "So here's the thing: the moons dataset is already packed inside the scikit-learn library. Cool, right? Picture it as two curvy moon-shaped clusters just hanging out together."
      ],
      "metadata": {
        "id": "IanSZilDKYFE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How spectral clustering dealt with moon-shaped clusters\n",
        "\n",
        "But the real question is: Can our good old pals K-means and Gaussian Mixture pinpoint these clusters? Well, it turns out that these clusters' shapes make it a tad tricky for K-means and Gaussian Mixture to work their magic, even though these moons don't overlap.\n",
        "\n",
        "**Non-linear Manifold Embedding**: It starts with a cool move, performing a non-linear manifold embedding. Think of it as giving our data a fresh perspective. One popular method here is the Laplacian Eigenmap combined with... you guessed it, K-means clustering.\n",
        "\n",
        "\n",
        "To understand the process of non-linear manifold embedding using the Laplacian Eigenmap, let's break it down step-by-step:\n",
        "\n",
        "**Step 1: Create a symmetric k-NN graph:** Start by constructing a symmetric k-Nearest Neighbour graph. In our example with the moon dataset, we used 5 nearest neighbours ($k=5$). This configuration produced two separate graphs that accurately represent the two clusters present in the dataset.\n",
        "\n",
        "**Step 2: Define the affinity matrix $A$:** The affinity matrix $A$ indicates the similarity between pairs of samples. If there's an edge between two samples in the graph, they're deemed similar; otherwise, they're not.\n",
        "\n",
        "**Step 3: Compute the Graph Laplacian $L$:** The Graph Laplacian is calculated as $L=D-A$, where $D$ is the degree matrix.\n",
        "\n",
        "**Step 4: Eigen-decomposition of $L$:** Determine the eigenvalues and eigenvectors of the Graph Laplacian $L$.\n",
        "\n",
        "**Step 5: Order the eigenvalues:** List the eigenvalues in ascending order, from the smallest to the largest.\n",
        "\n",
        "**Step 6: Define the new embedded space:** The new space is determined by the eigenvectors corresponding to a set number of the smallest eigenvalues. Note: the very first eigenvalue, which is always zero, is excluded.\n",
        "\n",
        "**Step 7: Execute k-means clustering:** With the newly embedded space defined, perform k-means clustering on it.\n",
        "\n",
        "In summary, spectral clustering, through the aforementioned steps, effectively manages moon-shaped clusters. The procedure provides a clear representation of clusters even in non-linear data distributions.\n"
      ],
      "metadata": {
        "id": "4DeujGKLI7oe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Graph Laplacian\n",
        "\n",
        "When we talk about the Graph Laplacian, represented by $L$, there's a foundational principle to keep in mind. Every distinct section (or \"connected component\") of our graph contributes one zero eigenvalue to the Graph Laplacian. What does this mean? Think of an eigenvalue as a kind of marker or label. The fact that the smallest of these labels is always zero indicates that there's always at least one connected piece in our graph. However, the first eigenvector—associated with this smallest label—doesn't offer us new information for our analysis, so we usually leave it out when creating an embedded representation of our data.\n",
        "\n",
        "Now, let's relate this to our 'moons' dataset as an example. Our graph, created from this dataset, had two separate sections, or \"connected components.\" Because of this, our Graph Laplacian, $L$, presented two of these zero eigenvalue labels. While both labels are critical for understanding the structure, we typically set one aside when creating our embedded representation.\n",
        "\n",
        "**Here's the interesting part:** the very first dimension of this embedded representation carries only two distinct values. These values serve as signposts, pointing out the two clusters in our dataset. So, in a situation where our graph has multiple unconnected sections, our dataset, when transformed into this embedded space, gives us discrete markers that help identify and differentiate these sections.\n",
        "\n",
        "If we were to visually map out the data using the first three dimensions of this embedded representation:\n",
        "\n",
        "- The first dimension, as we discussed, clearly differentiates the two clusters.\n",
        "\n",
        "- The second dimension zooms in on the red cluster, showing its shape and spread.\n",
        "\n",
        "- The third dimension does the same, but for the blue cluster.\n",
        "\n",
        "**To sum it up:**\n",
        "- The Graph Laplacian, $L$, will have a zero eigenvalue for each distinct section of the graph.\n",
        "\n",
        "- Though the first eigenvalue will always be zero (thanks to the always-present connected section), its corresponding eigenvector doesn't add to our understanding and so is skipped in our embedded representation.\n",
        "  \n",
        "**In the case of our moons:**\n",
        "\n",
        "- Two distinct graph sections led to two zero eigenvalues, but we only actively used one in our analysis.\n",
        "\n",
        "- The result? The primary embedded dimension neatly captures and differentiates the two clusters in our dataset."
      ],
      "metadata": {
        "id": "NwmWJpeONlDz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also illustrate this in the original Euclidean space by color-coding the data points based on each embedded dimension. The first embedded feature accurately separates the two clusters. The second embedded feature shows variation within the bottom moon while remaining constant for the top moon. Conversely, the third embedded feature fluctuates within the top moon but remains unchanged for the bottom moon."
      ],
      "metadata": {
        "id": "mUh6m5XUPNdE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### More on Laplacian Matrix\n",
        "\n",
        "The Graph Laplacian is a fundamental matrix representation used in graph theory and network analysis. It captures the structure and properties of a graph and is widely used in various applications, from spectral clustering in machine learning to network analysis in complex systems.\n",
        "\n",
        "Let's break it down:\n",
        "\n",
        "**Graph Basics:**\n",
        "A graph consists of nodes (or vertices) and edges connecting these nodes. Each node can have one or multiple connections with other nodes through edges.\n",
        "\n",
        "**Adjacency Matrix ($A$):**\n",
        "This is a matrix representation of a graph. If there's an edge between node i and node j, then $A_{ij} = 1$ (or some weight if the graph is weighted). Otherwise, $A_{ij} = 0$.\n",
        "\n",
        "**Degree Matrix ($D$):**\n",
        "This is a diagonal matrix where each diagonal entry $D_{ii}$ is the sum of the weights of the edges connected to node i (or simply the number of edges if the graph is unweighted).\n",
        "\n",
        "**Graph Laplacian ($L$):**\n",
        "Defined as:\n",
        "$$L = D - A$$\n",
        "The Graph Laplacian is essentially the difference between the Degree Matrix and the Adjacency Matrix.\n",
        "\n",
        "**Properties and Significance of Graph Laplacian:**\n",
        "1. For an undirected graph, the Laplacian matrix is symmetric.\n",
        "2. The smallest eigenvalue of $L$ is 0, and its corresponding eigenvector is a constant one-vector.\n",
        "3. The number of times the eigenvalue 0 appears indicates the number of connected components in the graph. For instance, if there are two separate, unconnected parts of a graph, the Laplacian will have the eigenvalue 0 twice.\n",
        "4. The Graph Laplacian is used in spectral clustering to identify clusters in data based on graph structure. The eigenvectors corresponding to the smallest eigenvalues are used to embed data into a new space where clusters can be more easily identified.\n",
        "5. The Laplacian captures the flow on a graph, making it useful for various algorithms and processes that involve diffusion or spreading on a network.\n",
        "\n"
      ],
      "metadata": {
        "id": "06pvqo3nOpOF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, we've seen how it works with samples that have straightforward distances, like points on a graph. But what about stuff that isn’t as straightforward? Imagine trying to cluster whole images from baby brain MRIs! Sounds challenging, right?\n",
        "\n",
        "**Picture this:** We've got 68 adorable little ones, both term and preterm, all scanned when they reached term age. Now, if we want to get into the spectral clustering magic, we need to figure out this thing called the affinity matrix 'A'. Here's a fun twist: we line up all those brain images nice and neat, like ducks in a row, in the same reference space. Then, we gauge how similar they are using this cool method called normalised cross-correlation (NCC). And because all the images are of the same type, our NCC values are always on the positive side, maxing out at a perfect 1. It's like everyone holding hands in a fully connected circle of friends!\n",
        "\n",
        "We jazz things up a bit using the normalised Graph Laplacian since, you know, not all nodes are created equal. For our grand finale, we opt for a 3D view and, after some pondering over the data's layout, decide to group them into three fabulous clusters.\n",
        "\n",
        "So, that’s our exciting venture into the world of baby brain MRI clustering! Neat, right?"
      ],
      "metadata": {
        "id": "jNy7gIlEqRja"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cluster brain MRI of term and preterm babies scanned at 40 weeks GA\n",
        "\n",
        "Imagine this: We've got a bunch of brain MRI images from both term and preterm babies, all scanned at a cozy 40 weeks GA. Now, to get a clearer view, think of averaging these images, kinda like blending different smoothie ingredients to get one delicious flavor.\n",
        "\n",
        "Here's the intriguing part:\n",
        "- **Clusters 1 & 2**: Dominated by term babies.\n",
        "- **Cluster 3**: Mainly preterm babies.\n",
        "\n",
        "Wondering why? Well, our preterm babies tend to have a bit more CSF (Cerebrospinal Fluid). And there's a twist! One of the term babies, who landed in Cluster 3, also had a bit more CSF, even though they weren't born preterm. It's like finding an unexpected ingredient in your smoothie!\n",
        "\n",
        "Now, if we splash some color on our clusters based on their birth gestational age, a pattern emerges. The third cluster lights up for preterm babies, while clusters 1 and 2? They're more of a mixed bag without a clear tie to the age at birth.\n",
        "\n",
        "To get a closer look, let's whip up an average of the images in each cluster. It's like viewing a movie's highlight reel! And the star of our show? The difference in CSF levels! Clusters 1 & 2 show less CSF, while cluster 3 has it in abundance, a common trait in our preterm babies.\n",
        "\n",
        "But remember our term baby outlier in cluster 3? On a closer look, this little champ has more CSF, making them more akin to the preterm babies, even though they arrived right on schedule."
      ],
      "metadata": {
        "id": "sZMOZ_FXq1HN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why we select certain features over others?\n",
        "\n",
        "Ever wonder why we select certain features over others? Well, there are some pretty cool reasons:\n",
        "\n",
        "1. **To Avoid Overfitting**: Think of it as decluttering. By removing extra stuff (or redundant data), our model doesn't get sidetracked by unnecessary noise.\n",
        "2. **Boost Accuracy**: It's like focusing on the main story without the side plots. By cutting out misleading data, our model can be more on-point and accurate.\n",
        "3. **Speed Things Up**: Everyone loves a quick result, right? With fewer features, our model can sprint through the training process, making everything snappier.\n"
      ],
      "metadata": {
        "id": "8M6HLTVvsMfz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Application: Prediction of age at scan\n",
        "\n",
        "Let's focus on predicting age based on brain scans. Remember when we talked about predicting age from those 86 brain structure volumes? Using just a straightforward multivariate linear regression might make our model a bit too eager and overfit the training data. The clue? A big difference in how the model does on the full training set versus when tested with cross-validation.\n",
        "\n",
        "Now, throughout our journey, we've come across several cool techniques to prevent this overfitting. Do any of them ring a bell?\n",
        "\n",
        "- Keeping our model's enthusiasm in check with **regularisation** (like Ridge or Lasso).\n",
        "- Reducing the number of features with methods like **PCA, ICA, or Laplacian Eigenmap**.\n",
        "- Bringing together multiple models in **ensemble learning** (like our buddy, the Random Forest).\n",
        "- And of course, being picky with our data using **feature selection**!\n",
        "\n",
        "Quick recap: We aim to predict the age at scan using volumes of 86 brain structures in preterm babies. Just using multivariate linear regression? Oops, we overfit!\n",
        "\n",
        "So, what's in our toolkit to prevent overfitting? Regularisation, dimensionality reduction, ensemble learning, and feature selection. Got it? Onward!"
      ],
      "metadata": {
        "id": "salPD6QqsPMj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## simulated features\n",
        "\n",
        " Imagine we've got five of these features, each sprinkled with varying amounts of noise and non-linearity. Picture it like this:\n",
        "\n",
        "- The x-axis is showing the feature value.\n",
        "- The y-axis? That’s our target value.\n",
        "\n",
        "Now, our goal is to pick the best and most informative features. To help visualize this, we've simulated five features:\n",
        "\n",
        "1. The first feature has a straight-line relationship with the targets, but it's kinda like static on a TV – a bit noisy.\n",
        "2. The second one? Still a straight-line relationship, but clearer than a sunny day – much less noise.\n",
        "3. Our third feature's relationship with the target is more like a wavy line, a bit curvy but still pretty clear.\n",
        "4. The fourth one is like a roller coaster, super curvy and unpredictable. It’s not a straight shot to our target, but it’s clear without much noise.\n",
        "5. And the fifth? Well, that's just like radio static – all noise and no clear connection to our targets."
      ],
      "metadata": {
        "id": "KzJfcGXvtdh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature importances\n",
        "\n",
        "Feature importances helps us understand which features really make a difference when making predictions.\n",
        "\n",
        "**Univariate Feature Importances**:\n",
        "- Think of this as looking at one feature at a time.\n",
        "- We're trying to see how good each feature is at predicting the target values.\n",
        "- It’s kinda like finding out which ingredients make a dish taste amazing! We might use things like correlation or mutual information to get a feel.\n",
        "\n",
        "**Model-Based Feature Importances**:\n",
        "- Imagine building a LEGO castle. Each brick represents a feature, and some bricks are more crucial than others.\n",
        "- If we’re talking linear models (like regressors or classifiers), we create a fancy formula that looks like this:\n",
        "  $$y-hat = w_0 + w_1 x_1 + ... + w_n x_n$$\n",
        "  Here, the weights (those 'w' values) tell us the importance of each feature.\n",
        "- Quick tip: Make sure to scale your features before fitting; it’s like making sure all the LEGO bricks are the same size.\n",
        "- If you’re using scikit-learn, you can peek at these weights with `model.coef_`.\n",
        "\n",
        "**Tree-Based Methods**:\n",
        "- Trees are cool! They split data based on features and see which ones clear up confusion the best.\n",
        "- If you’ve got a bunch of trees (like in a forest), you average out the results from all trees.\n",
        "- Again, if you're using scikit-learn, check out `model.feature_importances_` to see the star players.\n"
      ],
      "metadata": {
        "id": "wCQIts2cts6O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Univariate feature selection\n",
        "\n",
        "1. **What's the gist?** We want to pick out the most relevant features for our data analysis. One way to do this is by looking at the Pearson’s correlation coefficient.\n",
        "2. **How do we use it?** Think of this coefficient as a measure of how related two sets of data are. If a feature has a coefficient close to +1 or -1, it means it's strongly correlated with our target outcome.\n",
        "3. **But wait, there's noise!** Sometimes, features might not show a clear linear relationship because of noise or non-linear patterns. For instance, a very wiggly feature or one with lots of random spikes might not have a strong correlation.\n",
        "4. **So how do we pick the best features?** With univariate feature selection, we rank the features based on their importance and pick the top ones. Tools like `SelectKBest` from `scikit-learn` can help with this.\n",
        "5. **A note on scikit-learn:** While it doesn't calculate the Pearson’s coefficient directly, it has a nifty function called `f_regression`. This gives us an F-value, which essentially tells us how likely it is that a feature's correlation with the target is just by chance. The good news? This F-value is closely tied to the Pearson’s coefficient.\n",
        "6. **Using SelectKBest:** This tool needs two things from us - a way to score features (like our `f_regression` function) and the number of top features we want.\n",
        "7. **What did we find?** For our data, the top three features (0, 1, and 2) have a straightforward, linear relationship with our target values."
      ],
      "metadata": {
        "id": "p3Ovd1f2uus5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Univariate feature selection2\n",
        "\n",
        "Ready to find out which features are the best buddies with your target?\n",
        "\n",
        "1. **What's Mutual Information?** Think of it like a friendship meter. It tells us how much two variables, in our case, feature values and target values, have in common. The more they \"know\" about each other, the higher the mutual information!\n",
        "2. **How to pick the best buddies?** We're on the lookout for features that share a lot of secrets (information) with our target. But sometimes, the chatter (noise) can get in the way and lessen their bond.\n",
        "3. **Unique and Wiggly Relations? No Problem!** The cool thing about mutual information is that it's unfazed by whether the relationship is straight or all over the place. In fact, a super wiggly feature can sometimes share more info with the target than a straight-line feature.\n",
        "4. **How to Measure Friendship in Code?** `Scikit-learn` is our matchmaker here! It has a tool called `mutual_info_regression` that measures how tight-knit our features and targets are. If we were to pick the top three BFFs using `SelectKBest`, it'd likely be features 1, 2, and 3!\n",
        "5. **But wait, what about Classification?** Ah, for that, `scikit-learn` has different measuring tapes. The top ones are:\n",
        "   - `chi2` (chi squared)\n",
        "   - `f_classif`\n",
        "   - `mutual_info_classif`\n"
      ],
      "metadata": {
        "id": "rmWHp1E6vAtO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model based feature selection\n",
        "\n",
        "**What's the Lasso Way?**\n",
        "\n",
        "1. Lasso is like a talent scout. It zeroes in on the most impactful features by giving them the highest \"coefficients\" or weights.\n",
        "2. There's some magic behind the scenes! When Lasso uses the L1 norm penalty, it makes many feature weights zero, resulting in a simpler model. This is called \"sparsity.\"\n",
        "\n",
        "**Tuning with LassoCV:**\n",
        "\n",
        "Lasso has this cool feature called \"LassoCV.\" It's like a radio that automatically tunes to the best station! In our case, it finds the ideal setting for the hyperparameter 𝜆. And for our little example here, it settled on 0.003.\n",
        "\n",
        "**Picking the Stars with SelectFromModel:**\n",
        "\n",
        "Sklearn has this nifty tool called `SelectFromModel`. Think of it as a director casting the leading roles in a movie. It selects features based on how impactful (or high) their coefficients are. And yep, for our movie, features 1 and 2 got the leading roles! They're pretty straightforward and don't come with a lot of drama (noise)."
      ],
      "metadata": {
        "id": "zRfqTjzGvtOj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model based feature selection\n",
        "\n",
        "So, with Random Forest guiding us, we're equipped to uncover those truly special features that can make our predictions shine!\n",
        "\n",
        "Let's venture into the world of \"Random Forest\" for feature selection!\n",
        "\n",
        "**The Random Forest Way:**\n",
        "\n",
        "1. Random Forest, like a wise old sage, picks features based on their \"importance.\" How do they get this wisdom? It's by calculating how much each feature clears up the uncertainty (or decreases the impurity) in a decision tree.\n",
        "2. Picture a bunch of trees, each with their own opinions. Random Forest's \"feature importance\" is the average clarity (or decrease in impurity) each feature brings across all these trees.\n",
        "\n",
        "**Selecting the Shining Stars:**\n",
        "\n",
        "Using `SelectFromModel`, we can handpick those standout features. Just like before, but this time, it's based on `feature_importances_`. And for this act, features 1, 2, and 3 stole the show!\n",
        "\n",
        "**A Special Note on Feature 3:**\n",
        "\n",
        "Isn't it fascinating? Even though Feature 3 dances to its own non-linear tune and doesn't have a unique bond with the target, Random Forest still recognizes its worth! It's like appreciating a unique dancer in a troupe. Teaming up with Feature 1, they create a magical performance.\n"
      ],
      "metadata": {
        "id": "Qj9HblnqwaE_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recursive Feature Elimination\n",
        "\n",
        "Peeling the onion layer by layer, that's what \"Recursive Feature Elimination\" (RFE) is all about!\n",
        "\n",
        "**Deep Dive into RFE:**\n",
        "\n",
        "1. RFE doesn't just pick features; it *ranks* them. How? By starting with all features and taking them away, one by one, from least important to most.\n",
        "2. Here's the fun part: This isn't a one-time deal. With each feature removed, the model retrains and recalculates the importances. Think of it as a reality show, where contestants are voted off one by one, based on their performance in each episode.\n",
        "\n",
        "**Steps in RFE's Dance:**\n",
        "\n",
        "1. First, we fit a ranking model. For our case, we're using Ridge regression.\n",
        "2. After that, we find the least important feature and bid it goodbye.\n",
        "3. Rinse and repeat! We keep training the model with the remaining features and keep eliminating the weakest links.\n",
        "\n",
        "By the end, we get a ranking of all features from the star performers (last ones standing) to the early departures (first ones out).\n",
        "\n",
        "**Special Tools for RFE:**\n",
        "\n",
        "If you're a fan of automation, you'd love `RFECV` from sklearn. It's RFE with a sprinkle of cross-validation magic. Instead of manually picking the number of features, `RFECV` finds the optimal number that maximizes model performance.\n",
        "\n",
        "For our Ridge regression escapade, features 1 and 2 came out on top. Bravo!"
      ],
      "metadata": {
        "id": "0LTv90rlxJ85"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature selection results - summary\n",
        "\n",
        "Time for a feature selection showdown summary!\n",
        "\n",
        "**The Stars of the Show:**\n",
        "\n",
        "- Features 1 and 2 are the headliners, chosen by all the methods. They're as straight as an arrow, which makes them easy to work with, and they keep the noise level down.\n",
        "\n",
        "**Feature 3, the Underdog:**\n",
        "\n",
        "- Feature 3 may not have a unique relationship with the target, but it's a versatile player. Mutual information and Random Forest recognized its value. It brings a dose of non-linearity to the party and keeps the noise to a minimum.\n",
        "- In fact, it even outshone Feature 2 in the eyes of Random Forest and Mutual Information!\n",
        "\n",
        "**The Noise Maker, Feature 4:**\n",
        "\n",
        "- Last but not least, Feature 4 got the boot every time. It's like that one noisy neighbor that no one wants to invite to the party. Just full of noise, no real connection to the target.\n",
        "\n",
        "So, there you have it! Our feature selection methods have spoken, and the winners are Features 1, 2, and 3. They bring the harmony, while Feature 4 got left out in the noise."
      ],
      "metadata": {
        "id": "aZeq2_6xx3Sy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Application: Prediction of age at scan\n",
        "\n",
        "Let's circle back to our brainy task of predicting age at scan for preterm neonates using volumes of 86 brain structures.\n",
        "\n",
        "**The Overfitting Conundrum:**\n",
        "\n",
        "You see, when we initially tried to tackle this with multivariate linear regression, it was like trying to fit a square peg into a round hole - overfitting was lurking around the corner.\n",
        "\n",
        "**Enter Feature Selection, Our Hero!**\n",
        "\n",
        "But hold on, can feature selection actually save the day and prevent overfitting?\n",
        "\n",
        "**Drumroll, Please... Conclusion Time!**\n",
        "\n",
        "The answer is a resounding YES! Feature selection turned out to be our trusty sidekick in this adventure. It helped us avoid overfitting and led to some impressive results.\n",
        "\n",
        "We tested three feature selection methods:\n",
        "\n",
        "1. Univariate feature selection using correlation (picking 2 features).\n",
        "2. Model-based feature selection using Lasso (choosing 6 features).\n",
        "3. Recursive Feature Elimination (RFE) with a linear regression model (opting for 4 features).\n",
        "\n",
        "And the winner of this feature selection showdown is... Lasso! It performed the best, closely followed by RFE. These methods worked their magic and gave us results akin to using Ridge Regression, a pretty effective model.\n",
        "\n",
        "So there you have it! Feature selection not only prevents overfitting but also boosts our performance. It's like finding the perfect-sized puzzle piece for our prediction task."
      ],
      "metadata": {
        "id": "cA-Y_5T-yS8M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interpretation of feature importances\n",
        "\n",
        "**Understanding Feature Importances:**\n",
        "\n",
        "In the realm of machine learning, feature importances provide us with valuable insights into which features carry the most weight when it comes to making predictions. These insights can be enlightening and help us better understand the problem we're trying to solve.\n",
        "\n",
        "**The Challenge of Correlated Features:**\n",
        "\n",
        "However, there's a challenge we often face. Imagine you have a group of features that are closely related or correlated. It's like having multiple teammates who are equally skilled. Sometimes, when we use feature selection methods, they might end up choosing just one of these correlated features to represent the entire group. This can be a problem because some of those highly predictive features might not get the recognition they deserve, leading to lower importances for them.\n",
        "\n",
        "**Comparing Feature Selection Methods:**\n",
        "\n",
        "Let's take a look at an example where we used three different feature selection methods. Each of these methods selected its own set of top three features. This variety in selection can make it tricky to interpret which features are truly the most important.\n",
        "\n",
        "So, in a nutshell, feature importances are like guiding lights in the world of data, showing us which features matter most. But when features are closely connected, we might end up highlighting just one, and that can sometimes overshadow other valuable features. It's all part of the data exploration journey!"
      ],
      "metadata": {
        "id": "Ld01MWZiy4YZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature interpretation\n",
        "\n",
        "The instability we encounter here makes it tricky to confidently interpret the significance of individual features. However, if our primary aim is to reduce dimensionality rather than dissect individual features, this instability might not be a big concern.\n",
        "\n",
        "There are some clever solutions to address this issue:\n",
        "\n",
        "1. **Stability Selection:** This involves running the feature selection method on various subsets of the data and then calculating statistics to determine how often each feature is selected.\n",
        "\n",
        "2. **Feature Merging:** Another approach is to identify pairs or clusters of highly correlated features and make a decision. You can either drop the less predictive ones or merge them into a single representative feature.\n",
        "\n",
        "These strategies help us navigate the instability and make the most out of our feature selection process."
      ],
      "metadata": {
        "id": "aOrOVbJ4zlV3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comic time\n",
        "\n"
      ],
      "metadata": {
        "id": "-1h2KpMtz4yU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Manifold learning\n",
        "\n",
        "Manifold learning is a technique used in machine learning and data analysis to uncover the underlying structure or geometry of complex, high-dimensional data. In simpler terms, it helps us make sense of data that doesn't adhere to a straightforward, linear pattern.\n",
        "\n",
        "When we say \"manifold,\" we're referring to a mathematical concept that represents a lower-dimensional, continuous surface within a higher-dimensional space. Manifold learning algorithms aim to find this lower-dimensional representation of the data, where the relationships between data points become more apparent and understandable.\n",
        "\n",
        "The primary goal of manifold learning is dimensionality reduction while preserving essential information. It's particularly useful when dealing with data that exhibits non-linear relationships or when you want to visualize high-dimensional data in a more manageable form.\n",
        "\n",
        "Some commonly used manifold learning techniques, in addition to Laplacian Eigenmap mentioned earlier, include Isomap, t-SNE (t-Distributed Stochastic Neighbor Embedding), and LLE (Locally Linear Embedding). These methods help uncover the hidden structure in data, making it easier to analyze and interpret complex datasets."
      ],
      "metadata": {
        "id": "EIS9RR1j0VkR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Laplacian Eigenmap\n",
        "\n",
        "In the context of manifold learning, Laplacian Eigenmap is a dimensionality reduction technique used to uncover the underlying structure of complex, non-linear datasets. It's a method that helps transform high-dimensional data into a lower-dimensional space while preserving the essential relationships between data points.\n",
        "\n",
        "Here's a more detailed explanation:\n",
        "\n",
        "1. **Graph Representation:** Laplacian Eigenmap starts by representing the data as a graph, where data points are nodes, and edges are drawn between points that are similar to each other in some way. This similarity can be based on distance, correlation, or other measures.\n",
        "\n",
        "2. **Graph Laplacian:** Next, it computes a mathematical construct known as the \"Graph Laplacian.\" This construct captures how connected or isolated each data point is within the graph. It's a matrix that encodes the relationships between data points.\n",
        "\n",
        "3. **Eigenvalues and Eigenvectors:** Laplacian Eigenmap then calculates the eigenvalues and corresponding eigenvectors of this Graph Laplacian. These eigenvalues and eigenvectors provide insight into the hidden structure of the data.\n",
        "\n",
        "4. **Dimension Reduction:** The eigenvalues are ordered from smallest to largest, and the corresponding eigenvectors are used to create a new, lower-dimensional embedded space. This new space represents the data in a way that highlights its non-linear structure.\n",
        "\n",
        "5. **Clustering or Visualization:** Finally, this lower-dimensional space can be used for various purposes, such as clustering similar data points together or visualizing the data in a more interpretable form.\n",
        "\n",
        "Laplacian Eigenmap is particularly effective when dealing with data that doesn't follow a linear pattern, as it aims to capture the non-linear relationships between data points. It's a valuable tool in manifold learning for understanding and making sense of complex, high-dimensional datasets."
      ],
      "metadata": {
        "id": "U-dv6hU10ude"
      }
    }
  ]
}