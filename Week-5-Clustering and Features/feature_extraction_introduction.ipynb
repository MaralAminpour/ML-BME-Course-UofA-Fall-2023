{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaralAminpour/ML-BME-Course-UofA-Fall-2023/blob/main/Week-5-Clustering%20and%20Features/feature_extraction_introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part IV: Feature Extraction"
      ],
      "metadata": {
        "id": "ZDIGxOzLDmMy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## what is feature extraction?\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-5-Clustering%20and%20Features/imgs/ex1.gif' width=200px >\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-5-Clustering%20and%20Features/imgs/ex2.png' width=400px >\n",
        "\n",
        "Feature extraction is the art of **simplifying our data**. Imagine you have a vast canvas with a sprawling and complex painting. Feature extraction is like being able to **reproduce that painting on a smaller canvas, while still capturing its essence and beauty**. When we explore large datasets, especially those from **signals or images**, **there's so much going on!** There are tons of details, but not all of them are crucial. Feature extraction helps us **filter out the noise and focus on the salient features**, those significant brush strokes that truly define the artwork.\n",
        "\n",
        "**Why is this important?** Well, when we **reduce the complexity of our data**, we're setting ourselves up for success in the world of machine learning. A simplified, yet meaningful dataset is **less prone to overfitting**, where our model might get too caught up in the tiny details and miss the big picture. Furthermore, models trained on such data are not only **more accurate but also learn faster.**\n",
        "\n",
        "So, the next time you come across a large dataset, think of it as that vast canvas. Through feature extraction, you're not only making the canvas more manageable **but ensuring that every brush stroke on it truly counts**. By emphasizing the right details and sidelining the redundant ones, we create a perfect setting for our machine learning models to thrive.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "W30aR226B555"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature extraction examples: Brain structure volumes\n",
        "\n",
        "Let’s now have a look at a few examples of feature extraction. You will find, that feature extraction methods are often **non-trivial and application specific**. This makes sense, because extracting **salient features** requires **extensive domain knowledge**. I will now explain the main ideas how we created the features for machine learning examples in this module.\n",
        "\n",
        "One example is trying to find the **size of different parts of the brain** using MRI images from babies born too early.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-5-Clustering%20and%20Features/imgs/extract2.png' width=600px >\n",
        "\n",
        "To extract volumes of brain structures from brain MRI of preterm babies, we first need to perform segmentation of these structures. To do this, we first have to **mark or segment these parts** on the image. After marking them, we measure **how many small picture elements (voxels)** are in that marked area. By **multiplying** this count with the **size of each voxel**, we find out the size of these brain parts.\n",
        "\n",
        "The segmentation is performed by combining two technique: clustering using Gaussian Mixture Models, to separate different brain tissue types, including WM, GM and CSF.  \n",
        "\n",
        "To mark these parts, we use two methods. The first method **groups similar brain tissues** using what's called **Gaussian Mixture Models**. This helps separate different parts like the **white matter (WM), grey matter (GM), and the fluid (CSF)**. Additionally we propagate labels from a number of segmented atlases using deformable registration, to help us define anatomical regions. The second method uses previously marked images as a guide to help mark the regions in our current image.\n",
        "\n",
        "### Segmentation:\n",
        "\n",
        "Segmentation is a critical step in many medical image processing tasks. It involves **partitioning an image into multiple segments, where each segment corresponds to a particular object or part of an object**.\n",
        "\n",
        "In the context of brain imaging, segmentation is used to differentiate between **various brain structures and tissue types.**\n",
        "\n",
        "### 1. Clustering using Gaussian Mixture Models (GMM):\n",
        "\n",
        "**Gaussian Mixture Models (GMM)**:\n",
        "\n",
        "- A GMM is a probabilistic model that **assumes that the data is generated from a mixture of several Gaussian distributions.**\n",
        "\n",
        "- Each Gaussian distribution represents a cluster.\n",
        "\n",
        "- In the context of brain image segmentation, the different clusters could represent **different tissue types.**\n",
        "\n",
        "**Application to Brain Tissue Types**:\n",
        "\n",
        "- The brain consists of various tissue types like White Matter (WM), Gray Matter (GM), and **Cerebrospinal Fluid (CSF)**.\n",
        "\n",
        "- By using GMM,\n",
        "\n",
        " - the aim is to **separate these tissue types based on their intensity values in the image**.\n",
        "\n",
        " - Each tissue type would have a distinct intensity distribution, and the\n",
        "\n",
        " - **GMM can model these distributions to identify and cluster similar voxels (3D pixels) together**.\n",
        "\n",
        "### 2. Propagation of Labels from Segmented Atlases:\n",
        "\n",
        "**Segmented Atlases**:\n",
        "\n",
        "- These are previously **segmented images that have labels for various anatomical regions**.\n",
        "\n",
        "- Atlases serve as a **reference** for segmenting new images.\n",
        "\n",
        "**Deformable Registration**:\n",
        "- Registration is the **process of aligning two or more images**.\n",
        "\n",
        "- Deformable registration allows for **non-linear alignment**, meaning it can account for variations in anatomy between individuals.\n",
        "\n",
        "- In this process, the atlas is **\"deformed\" or \"warped\"** to match the anatomy of the target image.\n",
        "\n",
        "**Label Propagation**:\n",
        "\n",
        "- Once the atlas is aligned with the **target image** through deformable registration, the labels from the atlas (**which indicate various anatomical regions**) can be **transferred or \"propagated\"** to the target image.\n",
        "\n",
        "- This helps in defining specific anatomical regions in the target image based on the reference atlas.\n",
        "\n",
        "In conclusion, the segmentation technique described uses a combination of **probabilistic clustering through GMM to differentiate tissue types and label propagation from atlases to identify anatomical regions**. The combination ensures both the differentiation of tissue types and the precise demarcation of anatomical structures.\n",
        "\n",
        "**NOTE**: In machine learning (ML), the term \"**salient feature**\" refers to a feature that **stands out** or is **most noticeable or important** in representing a particular pattern within the data. Salient features play a significant role in determining the outcome or the characteristics of the data. **Recognizing and leveraging such features can greatly enhance the performance of ML models.**"
      ],
      "metadata": {
        "id": "eYiR-pb-UTo6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature extraction examples: Cardiac function\n",
        "\n",
        "Extraction of quantitative markers of cardiac function depends on segmentation. For this purpose, in this study (Puyol-Antón et al.) utilized a convolutional neural network to segment various parts of the heart in 3D+T cardiac MRI using numerous annotated samples. We will discuss convolutional neural networks later in the course.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-5-Clustering%20and%20Features/imgs/extract3.png' width=600px >\n",
        "\n",
        "\n",
        "From these segmentations, the volume of the left ventricle throughout the cardiac cycle is computed. The time-points with the maximum and minimum volumes, end diastole and end systole, are identified.\n",
        "\n",
        "The ejection fraction is then calculated using the equation:\n",
        "\n",
        "$$ EF = \\frac{(LVEDV-LVESV)}{LVEDV} \\times 100 $$\n",
        "\n",
        "Summary:\n",
        "\n",
        "- Cardiac function assessment through segmentation of 3D+T MRI images using specialized deep learning techniques.\n",
        "\n",
        "- Parameters such as EF and GLS are derived from these segmentations.\n",
        "\n",
        "- Details of left ventricle volumes include:\n",
        "  - End diastolic (LVEDV)\n",
        "  - End systolic (LVESV)\n",
        "  \n",
        "For further reading, refer to Puyol-Antón et al.'s \"Regional multi-view learning for cardiac motion analysis: Application to identification of dilated cardiomyopathy patients”. Published in IEEE TMBE, 2018.\n",
        "\n",
        "**NOTE: Annonated samples**\n",
        "\n",
        "Annotated samples refer to data items (in this context, 3D+T cardiac MRI images) that have been **labeled or marked by experts or annotators** to indicate specific features or structures of interest. In the case of cardiac MRI, annotations might involve delineating different parts of the heart, such as the chambers, valves, or myocardium. These annotations provide ground truth information, which the convolutional neural network (CNN) can use during training to learn how to correctly segment these structures on new, unseen images. The more accurate and diverse the annotated samples are, the better the CNN will be at segmenting similar structures in new images."
      ],
      "metadata": {
        "id": "_-XOWzDRJcfL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Formula for calculating ejection fraction (EF)\n",
        "\n",
        "Picture this: You're watching a super detailed movie of your heart in action. This isn't just any regular film; it's captured using a special camera known as a **3D+T MRI**. This camera allows us to see every tiny detail of your heart from **all angles and across time**.\n",
        "\n",
        "Now, these pictures can be a bit overwhelming. Every single detail of your heart is captured in these images. But, what if we just want to understand **how your heart pumps blood**? That's where \"feature extraction\" waltzes in like a detective, looking for clues about your heart's performance. It's like skimming a book to understand the main plot without getting bogged down by all the side stories.\n",
        "\n",
        "Two moments capture our attention: when the balloon is fully **blown** (**end diastole**) and when it's most **squeezed out** (**end systole**). With these two measurements, we can calculate something called the **\"ejection fraction\" (EF)**. It's like finding out how much water the balloon pushes out in one go!\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-5-Clustering%20and%20Features/imgs/ejection.png' width=400px >\n",
        "\n",
        "[source](https://www.nursingcenter.com/ncblog/august-2021/how-to-calculate-ejection-fraction)\n",
        "\n",
        "The formula for calculating the ejection fraction (EF) is given by:\n",
        "\n",
        "$$\n",
        "EF = \\left( \\frac{LVEDV - LVESV}{LVEDV} \\right) \\times 100\n",
        "$$\n",
        "\n",
        "Here, we're just looking at the **difference between the balloon's maximum and minimum water levels as a percentage**. Don't worry about the letters too much; they're just fancy names for the balloon's maximum and minimum water levels.\n",
        "\n",
        "Here's a breakdown of the terms:\n",
        "\n",
        "- **EF (Ejection Fraction)**: This represents the percentage of blood that's pumped out of the left ventricle (a main pumping chamber of the heart) during each heartbeat.\n",
        "\n",
        "- **LVEDV (Left Ventricular End-Diastolic Volume)**: This is the amount of blood in the left ventricle just before it contracts to pump the blood out. Essentially, it's the volume of the left ventricle when it's fullest.\n",
        "\n",
        "- **LVESV (Left Ventricular End-Systolic Volume)**: This is the amount of blood remaining in the left ventricle after it has contracted. It's the volume of the left ventricle when it's least full.\n",
        "\n",
        "The formula calculates the difference between the volume of blood in the left ventricle before and after it contracts. This difference is then divided by the full volume (before contraction) to get a fraction. Multiplying by 100 converts this fraction into a percentage, giving us the ejection fraction. This percentage helps in understanding how effectively the heart is pumping blood out during each beat."
      ],
      "metadata": {
        "id": "gucr6Fr1CORv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature extraction examples: QRS interval from ECG\n",
        "\n",
        "Our third way to measure heart function is called the QRS interval. Instead of using MRI, we get this from an ECG.\n",
        "\n",
        "To derive the QRS interval from an ECG signal, we start with processing the **raw ECG**.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-5-Clustering%20and%20Features/imgs/extract44.png' width=700px >\n",
        "\n",
        "\n",
        "The first step involves **filtering the signal** to discard low-frequency drifts and any external noise. This gives us a cleaner waveform to work with.\n",
        "\n",
        "Next, we determine the peaks which appear as l**ocal highs and lows (maxima and minima)** within the filtered signal.\n",
        "\n",
        "To make sure we get the right points, we set limits on how close or how intense these points can be. By setting **certain thresholds**, like a **minimum height and distance between the peaks**, we can accurately pinpoint the Q, R, and S peaks.\n",
        "\n",
        "After these peaks are detected, the next step is to compute the **average duration of the QRS interval** from the data gathered over the course of the ECG recording.\n",
        "\n"
      ],
      "metadata": {
        "id": "kSYHZ6CJCYie"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Types of features: Feature Categories**\n",
        "\n",
        "Let’s now have a look at various types of features that we can use to train our machine learning models. Some features are directly observed or measured, and no feature extraction is needed. These are for example age, gender, height, or blood glucose levels.\n",
        "\n",
        "1. **Direct Observations:**\n",
        "   - Examples include age, gender, height, blood glucose levels, and others. These are straightforward measurements that don't need any additional processing.\n",
        "\n",
        "2. **Morphological Features:**\n",
        "   - These focus on the shape and structure of objects. Common ones are length, area, and volume. Think about measurements like the size of brain structures or the duration of QRS waves.\n",
        "\n",
        "3. **Texture Descriptions:**\n",
        "   - These dive into the statistical details of images or parts of images. Key terms here are mean, variance, kurtosis, and entropy. For instance, studying the texture of breast tissue samples can help detect cancer.\n",
        "\n",
        "4. **Transform-based Attributes:**\n",
        "   - This involves changing signals or images into different formats or domains. One example is moving to the frequency domain, which can help pinpoint abnormalities in things like ECG readings.\n",
        "\n",
        "5. **Local Feature Descriptions:**\n",
        "   - These emphasize specific points in images, like edges, blobs, or corners. They are typically highlighted using filtering techniques.\n",
        "\n",
        "In summary, there's a wide variety of features that assist in training machine learning models. Some are direct and easy to see, while others require a deeper look or transformation to understand fully."
      ],
      "metadata": {
        "id": "U4hyM6QjCeW_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature extraction example: Breast Cancer Classification Using Histological Images\n",
        "\n",
        "We will be exploring the PatchCamelyon dataset, which includes histological images of breast tissues. These images showcase both healthy and cancerous cells. Our goal is to determine if texture descriptors, specifically GLCM statistical properties, or localized feature descriptors like DAISY can be indicators of cancer.\n",
        "\n",
        "**Breast Cancer Classification Using Histological Images**\n",
        "\n",
        "- **Dataset:** PatchCamelyon\n",
        "\n",
        "- **Feature Extraction Techniques:**\n",
        "\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-5-Clustering%20and%20Features/imgs/extract6.png' width=500px >\n",
        "\n",
        "\n",
        "  - Texture Descriptors (GLCM)\n",
        "\n",
        "  - Localised Feature Descriptors (DAISY)\n",
        "\n",
        "In the upcoming tutorial, you'll have the opportunity to do this yourself.\n",
        "\n",
        "**Jupyter Notebook:** 8.4 Feature Extraction\"\n",
        "\n",
        "**NOTE: Histological images** are visual representations of tissue sections that have been prepared and stained to be observed under a microscope. The study of the microscopic anatomy of tissues is called histology. The main purpose of histological imaging is to diagnose diseases, study the anatomy and structure of tissues, and for various research purposes."
      ],
      "metadata": {
        "id": "n8_OJApTCvWv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Texture descriptors: Gray level co-occurrence matrix (GLCM)\n",
        "\n",
        "Gray level co-occurrence matrix expresses how pairs of discretised intensities, or grey levels, of neighbouring pixels or voxels, are distributed along one of the image directions. In 2D, we calculate the matrix in x and y directions separately.\n",
        "\n",
        "A gray level co-occurrence matrix is like a table that shows **how often certain pairs of colors or shades are next to each other in an image**. In a 2D picture, we look at pairs in left-right (x) and up-down (y) directions.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-5-Clustering%20and%20Features/imgs/extract7.png' width=500px >\n",
        "\n",
        "Imagine you have a chart for the left-right direction. The chart shows we have one pair where the first color is 1 and the next color is 2. So, in our table, where we have row 1 and column 2, we put the number 1.\n",
        "\n",
        "Now, for the up-down direction, we have another chart. Here, we see the pair (1,2) twice. So, in the same table, for row 1 and column 2, we put the number 2.\n",
        "\n",
        "**NOTE: Discretized intensities** refer to **the process of converting a continuous range of pixel or voxel values (intensities) in an image into a limited number of discrete levels or bins.** This is often done to simplify the image data or to reduce the amount of data for analysis.\n",
        "\n",
        "For example, consider an image where pixel intensities can vary continuously between 0 and 255. If this image is discretized into 8 levels, then the intensity range might be divided into 8 equally spaced bins, such as:\n",
        "- 0-31\n",
        "- 32-63\n",
        "- 64-95\n",
        "- ...\n",
        "- 224-255\n",
        "\n",
        "In this discretized version of the image, any pixel with an intensity between 0 and 31 would be assigned a new intensity value (perhaps the midpoint, 15), any pixel with an intensity between 32 and 63 would be assigned another value (perhaps 47), and so on.\n",
        "\n",
        "Discretization is commonly used in methods like the Gray Level Co-occurrence Matrix (GLCM) to reduce the size of the matrix and to make computations more manageable. It's important to choose an appropriate number of discrete levels based on the image characteristics and the specific analysis task.\n",
        "\n",
        "**The term \"gray\" in \"Gray Level Co-occurrence Matrix\" (GLCM)** refers to grayscale images. Grayscale images are those that have only shades of gray and no color. In such images, each pixel's value represents a shade of gray, ranging from black to white.\n",
        "\n",
        "Unlike color images, which typically have three channels (red, green, blue) to represent color information, grayscale images have only one channel. The intensity value of each pixel in this channel can range from, for example, 0 (representing black) to 255 (representing white) in an 8-bit image.\n",
        "\n",
        "GLCM is often used in the context of grayscale images to capture texture information by examining the spatial relationship of pixel intensities. **The word \"gray\" emphasizes that the method is primarily focused on variations in intensity rather than color.**"
      ],
      "metadata": {
        "id": "ZO-V4JM9RBXo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Texture Description: Texture Descriptors\n",
        "\n",
        "Once were recorded the frequencies in the matrix, we normalise it by the total number of discretised grey level intensity pairs. This way, we will convert the matrix to 2D probabilistic distribution. We can then calculate statistical properties of this distribution as our texture descriptors. In particular, we calculate contrast, dissimilarity, homogeneity, energy and correlation.\n",
        "\n",
        "The Gray level co-occurrence matrix, or GLCM for short, is a way to understand the texture of an image. Think of it like a table that shows how often pairs of pixels with specific brightness values are found together in an image.\n",
        "\n",
        "To make sense of the numbers in this table, we change them into percentages or probabilities. This makes it a 2D probability distribution, represented as \\( p(i,j) \\).\n",
        "\n",
        "Now, from this table, we can figure out some cool things about the texture of our image:\n",
        "\n",
        "1. **Contrast**: Measures the amount of local variations present. It's calculated using the formula:\n",
        "$$ \\sum_{i,j=0}^{l-1} (i-j)^2 p(i,j) $$\n",
        "\n",
        "2. **Dissimilarity**: Gives us an idea about how different pairs of pixels are. The formula is:\n",
        "$$ \\sum_{i,j=0}^{l-1} |i-j| p(i,j) $$\n",
        "\n",
        "3. **Homogeneity**: Tells us how close the elements of the matrix are to the matrix diagonal. In simpler words, how uniform our texture is. It's found using:\n",
        "$$ \\sum_{i,j=0}^{l-1} \\frac{1}{1+(i-j)^2} p(i,j) $$\n",
        "\n",
        "4. **Energy**: It's like a measure of uniformity or orderliness of the pixels. The higher the energy, the more order or regularity there is in the image. Formula:\n",
        "$$ \\sqrt{\\sum_{i,j=0}^{l-1} p(i,j)^2} $$\n",
        "\n",
        "5. **Correlation**: Shows how a pixel is related to its neighbor. The math behind it is a bit complex, but it's done using:\n",
        "$$ \\sum_{i,j=0}^{l-1} \\frac{(i-\\mu_i)(j-\\mu_j)}{\\sigma_I \\sigma_j} p(i,j) $$\n",
        "\n",
        "In simple words, once we've got our matrix set up and converted to percentages, these formulas help us describe the texture of our image in different ways!"
      ],
      "metadata": {
        "id": "BgW-0-9pC0TE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Texture description: Classification of breast cancer from histological images\n",
        "\n",
        "Here we're comparing pictures of regular tissue and cancer tissue. It's clear there's a difference in GLCM patterns between them. In our tutorial, we'll dig deeper to find out which stats work best to spot the cancerous parts.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-5-Clustering%20and%20Features/imgs/extract8.png' width=700px >\n",
        "\n",
        "About the PatchCamelyon dataset:\n",
        "- We get the GLCM from shaded patches.\n",
        "- We then look at the contrast, difference, evenness, energy, and relationship of each patch to figure out if it has cancer tissue in it.\""
      ],
      "metadata": {
        "id": "xQuDdhpDWjon"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Localised feature descriptors: The DAISY descriptor\n",
        "\n",
        "Its main goal of DAISY descriptor is to gather detailed features from an image, rather than just pinpointing standout points. It's all about understanding how colors or shades change in different directions. To do this, we first create maps of these changes using specific techniques. Afterward, we slightly blur these maps using different-sized filters and then pick information from set spots on them.\n",
        "\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-5-Clustering%20and%20Features/imgs/extract9.png' width=400px >\n",
        "\n",
        "\n",
        "Imagine an illustrative picture of how the DAISY descriptor works. There's a central red spot marking the DAISY's position on the image. There are other red and blue spots indicating where we're getting our information. Circles on this picture signify the blurring filters we use; bigger circles mean more blurring. Lines on the image show the direction of the color changes.\n",
        "\n",
        "Once we've got all this info, we organize it into a neat list or vector for later use. In our upcoming tutorial, we'll be using the DAISY descriptor to help identify types of breast cancer in samples and see how it stacks up against GLCM.\n",
        "\n",
        "Calculate gradients maps in different orientations\n",
        "Blur gradient maps with Gaussian filters of different sizes\n",
        "Sample blurred gradient maps at selected locations\n",
        "Arrange the extracted features into a vector\n",
        "\n",
        "\n",
        "+\n",
        "\n",
        "We will have a closer look at the DAISY descriptor, which aims to extract dense features, rather than focus on salient points. DAISY descriptor collects information about gradients in various orientations in the image. We first calculate the gradient maps in different orientations using filtering techniques. Then we blur the gradient maps with Gaussian filters of different sizes and sample the gradient maps at pre-defined locations.\n",
        "\n",
        "Here is the schematic image of the DAISY descriptor. The red dot is the location of the DAISY descriptor in the image. The red and blue dots show the locations, where we sample the gradient maps. The circles represent the Gaussian kernels used for blurring in each location. The radius of each circle is equal to the standard deviation of the Gaussian kernel.  The orientations of gradients are shown here by the lines plotted over the sampling locations.\n",
        "\n",
        "Finally, the features extracted for each DAISY descriptor are arranged in a vector ready for further processing. In the tutorial we will apply DAISY descriptor to classify breast cancer histological samples and compare it to GLCM.\n",
        "\n",
        "A quick recap:\n",
        "\n",
        "- Calculate gradients maps in different orientations (Measure the changes in colors in different directions)\n",
        "\n",
        "- Blur gradient maps with Gaussian filters of different sizes (ften these changes using Gaussian filters of different sizes)\n",
        "\n",
        "- Sample blurred gradient maps at selected locations (Take samples from the softened maps at certain spots)\n",
        "\n",
        "- Arrange the extracted features into a vector (Organize the gathered features into a line of data)\n"
      ],
      "metadata": {
        "id": "OOP0itRxDA36"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature extraction in the era of deep learning\n",
        "\n",
        "**Feature Engineering:**\n",
        "This is like giving our computer a little manual on how to look at data. We're telling it, \"Hey, check out these particular bits and pieces, they're super important!\" In our chat today, I'll show you some cool techniques we've got up our sleeve for this.\n",
        "\n",
        "**Feature Learning:**\n",
        "Here's where things get futuristic! With deep neural networks, we basically let our computer play detective. Instead of us pointing out the clues, it finds them on its own. It's especially awesome for looking at things like pictures or sounds where there's a ton of info to sift through.\n",
        "\n",
        "**Why Still Love Feature Engineering?**\n",
        "Even with all the fancy auto-detective work, there's value in the old-school approach:\n",
        "\n",
        "- We get to be the boss! We decide what features the computer should focus on.\n",
        "\n",
        "- It's way easier to explain. If someone asks, \"Why did the computer decide this?\", we've got clear answers.\n",
        "\n",
        "- It's super handy when we don't have a mountain of data, or when we really want to understand what's driving our computer's decisions.\n",
        "\n",
        "In a nutshell, while letting the computer learn on its own is super powerful, there's still a special place in our hearts (and in many projects) for good old feature engineering!"
      ],
      "metadata": {
        "id": "rfSLvvD8DFyk"
      }
    }
  ]
}