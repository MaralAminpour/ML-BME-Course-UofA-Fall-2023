{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaralAminpour/ML-BME-Course-UofA-Fall-2023/blob/main/Week-5-Clustering%20and%20Features/feature_extraction_introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part IV: Feature Extraction"
      ],
      "metadata": {
        "id": "ZDIGxOzLDmMy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## what is feature extraction?\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-5-Clustering%20and%20Features/imgs/ex1.gif' width=200px >\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-5-Clustering%20and%20Features/imgs/ex2.png' width=400px >\n",
        "\n",
        "Feature extraction is the art of simplifying our data. Imagine you have a vast canvas with a sprawling and complex painting. Feature extraction is like being able to reproduce that painting on a smaller canvas, while still capturing its essence and beauty. When we explore large datasets, especially those from **signals or images**, **there's so much going on!** There are tons of details, but not all of them are crucial. Feature extraction helps us **filter out the noise and focus on the salient features**, those significant brush strokes that truly define the artwork.\n",
        "\n",
        "**Why is this important?** Well, when we **reduce the complexity of our data**, we're setting ourselves up for success in the world of machine learning. A simplified, yet meaningful dataset is **less prone to overfitting**, where our model might get too caught up in the tiny details and miss the big picture. Furthermore, models trained on such data are not only **more accurate but also learn faster.**\n",
        "\n",
        "So, the next time you come across a large dataset, think of it as that vast canvas. Through feature extraction, you're not only making the canvas more manageable but ensuring that every brush stroke on it truly counts. By emphasizing the right details and sidelining the redundant ones, we create a perfect setting for our machine learning models to thrive.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "W30aR226B555"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature extraction examples: Brain structure volumes\n",
        "\n",
        "Let’s now have a look at a few examples of feature extraction. You will find, that feature extraction methods are often **non-trivial and application specific**. This makes sense, because extracting **salient features** requires **extensive domain knowledge**. I will now explain the main ideas how we created the features for machine learning examples in this module.\n",
        "\n",
        "One example is trying to find the **size of different parts of the brain** using MRI images from babies born too early.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-5-Clustering%20and%20Features/imgs/extract2.png' width=600px >\n",
        "\n",
        "To extract volumes of brain structures from brain MRI of preterm babies, we first need to perform segmentation of these structures. To do this, we first have to **mark or segment these parts** on the image. After marking them, we measure **how many small picture elements (voxels)** are in that marked area. By **multiplying** this count with the **size of each voxel**, we find out the size of these brain parts.\n",
        "\n",
        "The segmentation is performed by combining two technique: clustering using Gaussian Mixture Models, to separate different brain tissue types, including WM, GM and CSF.  \n",
        "\n",
        "To mark these parts, we use two methods. The first method **groups similar brain tissues **using what's called **Gaussian Mixture Models**. This helps separate different parts like the **white matter (WM), grey matter (GM), and the fluid (CSF)**. Additionally we propagate labels from a number of segmented atlases using deformable registration, to help us define anatomical regions. The second method uses previously marked images as a guide to help mark the regions in our current image."
      ],
      "metadata": {
        "id": "qqPskS2NJnIF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature extraction examples: Cardiac function\n",
        "\n",
        "Extraction of quantitative markers of cardiac function depends on segmentation. For this purpose, in this study (Puyol-Antón et al.) utilized a convolutional neural network to segment various parts of the heart in 3D+T cardiac MRI using numerous annotated samples. We will discuss convolutional neural networks later in the course.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-5-Clustering%20and%20Features/imgs/extract3.png' width=600px >\n",
        "\n",
        "\n",
        "From these segmentations, the volume of the left ventricle throughout the cardiac cycle is computed. The time-points with the maximum and minimum volumes, end diastole and end systole, are identified.\n",
        "\n",
        "The ejection fraction is then calculated using the equation:\n",
        "\n",
        "$$ EF = \\frac{(LVEDV-LVESV)}{LVEDV} \\times 100 $$\n",
        "\n",
        "Summary:\n",
        "\n",
        "- Cardiac function assessment through segmentation of 3D+T MRI images using specialized deep learning techniques.\n",
        "\n",
        "- Parameters such as EF and GLS are derived from these segmentations.\n",
        "\n",
        "- Details of left ventricle volumes include:\n",
        "  - End diastolic (LVEDV)\n",
        "  - End systolic (LVESV)\n",
        "  \n",
        "For further reading, refer to Puyol-Antón et al.'s \"Regional multi-view learning for cardiac motion analysis: Application to identification of dilated cardiomyopathy patients”. Published in IEEE TMBE, 2018."
      ],
      "metadata": {
        "id": "_-XOWzDRJcfL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Formula for calculating ejection fraction (EF)\n",
        "\n",
        "Picture this: You're watching a super detailed movie of your heart in action. This isn't just any regular film; it's captured using a special camera known as a **3D+T MRI**. This camera allows us to see every tiny detail of your heart from **all angles and across time**.\n",
        "\n",
        "Now, these pictures can be a bit overwhelming. Every single detail of your heart is captured in these images. But, what if we just want to understand **how your heart pumps blood**? That's where \"feature extraction\" waltzes in like a detective, looking for clues about your heart's performance. It's like skimming a book to understand the main plot without getting bogged down by all the side stories.\n",
        "\n",
        "Two moments capture our attention: when the balloon is fully **blown** (**end diastole**) and when it's most **squeezed out** (**end systole**). With these two measurements, we can calculate something called the **\"ejection fraction\" (EF)**. It's like finding out how much water the balloon pushes out in one go!\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-5-Clustering%20and%20Features/imgs/ejection.png' width=400px >\n",
        "\n",
        "[source](https://www.nursingcenter.com/ncblog/august-2021/how-to-calculate-ejection-fraction)\n",
        "\n",
        "The formula for calculating the ejection fraction (EF) is given by:\n",
        "\n",
        "$$\n",
        "EF = \\left( \\frac{LVEDV - LVESV}{LVEDV} \\right) \\times 100\n",
        "$$\n",
        "\n",
        "Here, we're just looking at the **difference between the balloon's maximum and minimum water levels as a percentage**. Don't worry about the letters too much; they're just fancy names for the balloon's maximum and minimum water levels.\n",
        "\n",
        "Here's a breakdown of the terms:\n",
        "\n",
        "- **EF (Ejection Fraction)**: This represents the percentage of blood that's pumped out of the left ventricle (a main pumping chamber of the heart) during each heartbeat.\n",
        "\n",
        "- **LVEDV (Left Ventricular End-Diastolic Volume)**: This is the amount of blood in the left ventricle just before it contracts to pump the blood out. Essentially, it's the volume of the left ventricle when it's fullest.\n",
        "\n",
        "- **LVESV (Left Ventricular End-Systolic Volume)**: This is the amount of blood remaining in the left ventricle after it has contracted. It's the volume of the left ventricle when it's least full.\n",
        "\n",
        "The formula calculates the difference between the volume of blood in the left ventricle before and after it contracts. This difference is then divided by the full volume (before contraction) to get a fraction. Multiplying by 100 converts this fraction into a percentage, giving us the ejection fraction. This percentage helps in understanding how effectively the heart is pumping blood out during each beat."
      ],
      "metadata": {
        "id": "gucr6Fr1CORv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature extraction examples: QRS interval from ECG\n",
        "\n",
        "Our third way to measure heart function is called the QRS interval. Instead of using MRI, we get this from an ECG.\n",
        "\n",
        "To derive the QRS interval from an ECG signal, we start with processing the **raw ECG**.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-5-Clustering%20and%20Features/imgs/extract44.png' width=700px >\n",
        "\n",
        "\n",
        "The first step involves **filtering the signal** to discard low-frequency drifts and any external noise. This gives us a cleaner waveform to work with.\n",
        "\n",
        "Next, we determine the peaks which appear as l**ocal highs and lows (maxima and minima)** within the filtered signal.\n",
        "\n",
        "To make sure we get the right points, we set limits on how close or how intense these points can be. By setting **certain thresholds**, like a **minimum height and distance between the peaks**, we can accurately pinpoint the Q, R, and S peaks.\n",
        "\n",
        "After these peaks are detected, the next step is to compute the **average duration of the QRS interval** from the data gathered over the course of the ECG recording.\n",
        "\n"
      ],
      "metadata": {
        "id": "kSYHZ6CJCYie"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Types of features: Feature Categories**\n",
        "\n",
        "Let’s now have a look at various types of features that we can use to train our machine learning models. Some features are directly observed or measured, and no feature extraction is needed. These are for example age, gender, height, or blood glucose levels.\n",
        "\n",
        "1. **Direct Observations:**\n",
        "   - Examples include age, gender, height, blood glucose levels, and others. These are straightforward measurements that don't need any additional processing.\n",
        "\n",
        "2. **Morphological Features:**\n",
        "   - These focus on the shape and structure of objects. Common ones are length, area, and volume. Think about measurements like the size of brain structures or the duration of QRS waves.\n",
        "\n",
        "3. **Texture Descriptions:**\n",
        "   - These dive into the statistical details of images or parts of images. Key terms here are mean, variance, kurtosis, and entropy. For instance, studying the texture of breast tissue samples can help detect cancer.\n",
        "\n",
        "4. **Transform-based Attributes:**\n",
        "   - This involves changing signals or images into different formats or domains. One example is moving to the frequency domain, which can help pinpoint abnormalities in things like ECG readings.\n",
        "\n",
        "5. **Local Feature Descriptions:**\n",
        "   - These emphasize specific points in images, like edges, blobs, or corners. They are typically highlighted using filtering techniques.\n",
        "\n",
        "In summary, there's a wide variety of features that assist in training machine learning models. Some are direct and easy to see, while others require a deeper look or transformation to understand fully."
      ],
      "metadata": {
        "id": "U4hyM6QjCeW_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature extraction example: Breast Cancer Classification Using Histological Images\n",
        "\n",
        "We will be exploring the PatchCamelyon dataset, which includes histological images of breast tissues. These images showcase both healthy and cancerous cells. Our goal is to determine if texture descriptors, specifically GLCM statistical properties, or localized feature descriptors like DAISY can be indicators of cancer.\n",
        "\n",
        "**Breast Cancer Classification Using Histological Images**\n",
        "\n",
        "- **Dataset:** PatchCamelyon\n",
        "\n",
        "- **Feature Extraction Techniques:**\n",
        "\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-5-Clustering%20and%20Features/imgs/extract6.png' width=500px >\n",
        "\n",
        "\n",
        "  - Texture Descriptors (GLCM)\n",
        "\n",
        "  - Localised Feature Descriptors (DAISY)\n",
        "\n",
        "In the upcoming tutorial, you'll have the opportunity to do this yourself.\n",
        "\n",
        "**Jupyter Notebook:** 8.4 Feature Extraction\""
      ],
      "metadata": {
        "id": "n8_OJApTCvWv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Texture descriptors: Gray level co-occurrence matrix (GLCM)\n",
        "\n",
        "A gray level co-occurrence matrix is like a table that shows how often certain pairs of colors or shades are next to each other in an image. In a 2D picture, we look at pairs in left-right (x) and up-down (y) directions.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-5-Clustering%20and%20Features/imgs/extract7.png' width=500px >\n",
        "\n",
        "Imagine you have a chart for the left-right direction. The chart shows we have one pair where the first color is 1 and the next color is 2. So, in our table, where we have row 1 and column 2, we put the number 1.\n",
        "\n",
        "Now, for the up-down direction, we have another chart. Here, we see the pair (1,2) twice. So, in the same table, for row 1 and column 2, we put the number 2."
      ],
      "metadata": {
        "id": "ZO-V4JM9RBXo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Texture Description: Texture Descriptors\n",
        "\n",
        "Once were recorded the frequencies in the matrix, we normalise it by the total number of discretised grey level intensity pairs. This way, we will convert the matrix to 2D probabilistic distribution. We can then calculate statistical properties of this distribution as our texture descriptors. In particular, we calculate contrast, dissimilarity, homogeneity, energy and correlation.\n",
        "\n",
        "The Gray level co-occurrence matrix, or GLCM for short, is a way to understand the texture of an image. Think of it like a table that shows how often pairs of pixels with specific brightness values are found together in an image.\n",
        "\n",
        "To make sense of the numbers in this table, we change them into percentages or probabilities. This makes it a 2D probability distribution, represented as \\( p(i,j) \\).\n",
        "\n",
        "Now, from this table, we can figure out some cool things about the texture of our image:\n",
        "\n",
        "1. **Contrast**: Measures the amount of local variations present. It's calculated using the formula:\n",
        "$$ \\sum_{i,j=0}^{l-1} (i-j)^2 p(i,j) $$\n",
        "\n",
        "2. **Dissimilarity**: Gives us an idea about how different pairs of pixels are. The formula is:\n",
        "$$ \\sum_{i,j=0}^{l-1} |i-j| p(i,j) $$\n",
        "\n",
        "3. **Homogeneity**: Tells us how close the elements of the matrix are to the matrix diagonal. In simpler words, how uniform our texture is. It's found using:\n",
        "$$ \\sum_{i,j=0}^{l-1} \\frac{1}{1+(i-j)^2} p(i,j) $$\n",
        "\n",
        "4. **Energy**: It's like a measure of uniformity or orderliness of the pixels. The higher the energy, the more order or regularity there is in the image. Formula:\n",
        "$$ \\sqrt{\\sum_{i,j=0}^{l-1} p(i,j)^2} $$\n",
        "\n",
        "5. **Correlation**: Shows how a pixel is related to its neighbor. The math behind it is a bit complex, but it's done using:\n",
        "$$ \\sum_{i,j=0}^{l-1} \\frac{(i-\\mu_i)(j-\\mu_j)}{\\sigma_I \\sigma_j} p(i,j) $$\n",
        "\n",
        "In simple words, once we've got our matrix set up and converted to percentages, these formulas help us describe the texture of our image in different ways!"
      ],
      "metadata": {
        "id": "BgW-0-9pC0TE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The DAISY descriptor\n",
        "\n",
        "Its main goal of DAISY descriptor is to gather detailed features from an image, rather than just pinpointing standout points. It's all about understanding how colors or shades change in different directions. To do this, we first create maps of these changes using specific techniques. Afterward, we slightly blur these maps using different-sized filters and then pick information from set spots on them.\n",
        "\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-5-Clustering%20and%20Features/imgs/extract9.png' width=400px >\n",
        "\n",
        "\n",
        "Imagine an illustrative picture of how the DAISY descriptor works. There's a central red spot marking the DAISY's position on the image. There are other red and blue spots indicating where we're getting our information. Circles on this picture signify the blurring filters we use; bigger circles mean more blurring. Lines on the image show the direction of the color changes.\n",
        "\n",
        "Once we've got all this info, we organize it into a neat list or vector for later use. In our upcoming tutorial, we'll be using the DAISY descriptor to help identify types of breast cancer in samples and see how it stacks up against GLCM.\n",
        "\n",
        "+\n",
        "\n",
        "Think of it as a special tool that doesn't just look for standout spots in an image but captures details all over it. It's super keen on understanding how colors change and transition in pictures by studying gradients from various angles.\n",
        "\n",
        "So, how does it work? First, we figure out how colors shift in different directions using some neat filtering tricks. After that, we add a gentle blur with varying degrees of fuzziness and pick certain spots to gather our information.\n",
        "\n",
        "Now, imagine a picture of the DAISY descriptor layout. There's a central red dot where the DAISY is located in the image. Then, surrounding it, you'll see red and blue dots. These are our special spots where we gather all the gradient details. And see those circles around? They symbolize the amount of blur we're using—the bigger the circle, the fuzzier the blur. Plus, those lines coming out from the dots? They're showing us the gradient directions.\n",
        "\n",
        "When we're done, we line up all the cool features we've captured from DAISY into a neat list, ready for some action! We'll even see how DAISY helps in classifying medical samples for breast cancer and stack it up against another method called GLCM.\n",
        "\n",
        "A quick recap:\n",
        "DAISY:\n",
        "- Dots = where we're checking details\n",
        "- Circles = how much we're blurring things\n"
      ],
      "metadata": {
        "id": "OOP0itRxDA36"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature extraction in the era of deep learning\n",
        "\n",
        "**Feature Engineering:**\n",
        "This is like giving our computer a little manual on how to look at data. We're telling it, \"Hey, check out these particular bits and pieces, they're super important!\" In our chat today, I'll show you some cool techniques we've got up our sleeve for this.\n",
        "\n",
        "**Feature Learning:**\n",
        "Here's where things get futuristic! With deep neural networks, we basically let our computer play detective. Instead of us pointing out the clues, it finds them on its own. It's especially awesome for looking at things like pictures or sounds where there's a ton of info to sift through.\n",
        "\n",
        "**Why Still Love Feature Engineering?**\n",
        "Even with all the fancy auto-detective work, there's value in the old-school approach:\n",
        "\n",
        "- We get to be the boss! We decide what features the computer should focus on.\n",
        "\n",
        "- It's way easier to explain. If someone asks, \"Why did the computer decide this?\", we've got clear answers.\n",
        "\n",
        "- It's super handy when we don't have a mountain of data, or when we really want to understand what's driving our computer's decisions.\n",
        "\n",
        "In a nutshell, while letting the computer learn on its own is super powerful, there's still a special place in our hearts (and in many projects) for good old feature engineering!"
      ],
      "metadata": {
        "id": "rfSLvvD8DFyk"
      }
    }
  ]
}