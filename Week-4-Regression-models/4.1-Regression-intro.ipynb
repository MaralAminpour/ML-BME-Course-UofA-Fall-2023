{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaralAminpour/ML-BME-Course-UofA-Fall-2023/blob/main/Week-4-Regression-models/4.1-Regression-intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57170492-140c-4d56-8e94-4a49232293ef",
      "metadata": {
        "id": "57170492-140c-4d56-8e94-4a49232293ef"
      },
      "source": [
        "# Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6320406c-7cb4-4c24-8773-fe1aa94795f1",
      "metadata": {
        "id": "6320406c-7cb4-4c24-8773-fe1aa94795f1"
      },
      "source": [
        "## Application: neonatal brain growth\n",
        "\n",
        "- Around the time of birth the brain grows very quickly\n",
        "- Preterm birth alters brain development\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/DevelopingPretermBrain.png\" width = \"300\" style=\"float: left;\">\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/Baby.png\" width = \"200\" style=\"float: right;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4519d6ff-afc0-4cfe-9592-ccbbe13f318e",
      "metadata": {
        "id": "4519d6ff-afc0-4cfe-9592-ccbbe13f318e"
      },
      "source": [
        "We will demonstrate the regression concepts using the application of brain growth in preterm neonates. Around the time of birth the brain grows very quickly. Preterm birth can disrupt this process, and therefore preterm brain development is a subject of extensive research.\n",
        "\n",
        "To investigate the changes caused by preterm birth, we can acquire MRI scans of newborn babies. We can perform automatic segmentations of various brain structures and measure their volumes. The features in the datasets we'll be working with are brain volumes, either for (1) the whole brain, (2) six brain tissues, or (3) 86 brain structures.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/BrainMRI-BrainSegmentation.png\" width = \"700\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fe8f6db-e261-45eb-8d19-1483be8b96c6",
      "metadata": {
        "id": "2fe8f6db-e261-45eb-8d19-1483be8b96c6"
      },
      "source": [
        "The machine learning regression is the predict the age (target value) of a baby from volumes of brain structures (features).\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/AgeVolumePlot.png\" width = \"400\">\n",
        "\n",
        "In this plot each circle corresponds to a baby (a sample), on the x-axis we have the brain volume (a feature scaled with StandardScaler). The age (target value) is plotted on the y-axis. In this case we are showing a univariate linear regression model in red, with two parameters (slope and intercept). We have achieved R2 score 0.85 which is quite good, so the linear model fits this dataset well."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How can we increase the performance of this model?\n",
        "\n",
        "So, you're wondering if we can boost the performance of our model, right?\n",
        "Specifically, you're looking to get that R2 score closer to the perfect 1. Well, let's dig into the type of error we're dealing with in our dataset. What's your guess?\n",
        "\n",
        "- bias\n",
        "\n",
        "- variance\n",
        "\n",
        "- noise\n",
        "\n",
        "**Good news:**\n",
        "\n",
        "1. we're not dealing with much bias here. Our linear model seems to capture the general trend quite nicely, so there's no glaring systematic issue.\n",
        "\n",
        "2. And as for variance, our model is a simple one, with only two parameters, so it's unlikely that overfitting is the culprit.\n",
        "\n",
        "3. That leaves us with noise, which, unfortunately, we can't really get rid of by switching up the model.\n",
        "\n",
        "Now, what about cranking up the complexity by using a dataset with 6 features or even a whopping 86 features?\n",
        "\n",
        "Sure, we could venture into **multivariate linear regression** territory.\n",
        "\n",
        "But remember, adding more features doesn't always mean better predictions, especially when the main issue is noise. Keep that in mind, and happy modeling!"
      ],
      "metadata": {
        "id": "RpveaNO9e7mV"
      },
      "id": "RpveaNO9e7mV"
    },
    {
      "cell_type": "markdown",
      "id": "eb30d104-3cb7-4f8a-828c-ff9796f95495",
      "metadata": {
        "id": "eb30d104-3cb7-4f8a-828c-ff9796f95495"
      },
      "source": [
        "## Multivariate linear regression\n",
        "\n",
        "Can we improve the predictions by using the 6-feature or 86-feature datasets? We can use multivariate linear regression for these datasets.\n",
        "\n",
        "Multiple features: $$x_{i1}, ... , x_{iD}$$\n",
        "\n",
        "Target values: $$y_i$$\n",
        "\n",
        "Samples: $$i = 1, ..., N$$\n",
        "\n",
        "Prediction model: $$\\hat{y} = w_0  + w_1x_1 + ... + w_D x_D$$\n",
        "\n",
        "Loss function: Sum of square errors\n",
        "\n",
        "$$F(\\textbf{w}) = \\frac{1}{2} \\sum_{i}(y_i - \\sum_{k} w_k x_{ik} - w_0)^2$$\n",
        "\n",
        "The model is fitted by minimizing the loss function (sum of squared errors)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the realm of linear regression, we refer to it as multivariate when there's more than just one feature to consider. For instance, let's say we are examining the volumes of D different structures in the brain. These volumes are represented as $ x_{i1}, \\ldots, x_{iD} $. For each of these D-dimensional feature vectors, there is an associated target value, $ y_i $, which in this specific application, corresponds to the age at the time of the scan.\n",
        "\n",
        "In this scenario, the index $ i $ identifies individual samples in our training set. In the context of our example, each sample corresponds to an individual baby. We have a total of $ N $ samples in our dataset. The predictive model we're working with is based on a linear equation. In this equation, $ w_0 $ serves as the intercept and the weights $ w_1 $ through $ w_N $ are the slopes associated with various features.\n",
        "\n",
        "Our goal is to fine-tune these weights, which are essentially the parameters of our model, to make the most accurate predictions possible. The predicted target value in this context is represented as $ \\hat{y} $. To find the best-fitting model, we minimize the sum of squared errors between the predicted and the actual target values.\n",
        "\n"
      ],
      "metadata": {
        "id": "TeIp02EkiZEq"
      },
      "id": "TeIp02EkiZEq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Matrix formulation\n",
        "\n",
        "It is also useful to express the\n",
        "linear regression problem using matrix formulation.\n",
        "\n",
        "In this version, we use a feature matrix, denoted by $X$, which contains all samples and their respective features.\n",
        "\n",
        "To simplify the calculations, we include an extra column of ones in this matrix. This extra column corresponds to the model's intercept, allowing the number of features and parameters ($w$) to be identical, at $D+1$.\n",
        "\n",
        "- Feature matrix:\n",
        "  $$\n",
        "  X = \\begin{pmatrix}\n",
        "    1 & x_{11} & \\cdots & x_{1D} \\\\\n",
        "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "    1 & x_{N1} & \\cdots & x_{ND}\n",
        "  \\end{pmatrix}\n",
        "  $$\n",
        "\n",
        "The target values are represented by the vector $y$, while the weight parameters are in the vector $w$.\n",
        "\n",
        "- Target vector:\n",
        "  $$\n",
        "  y = \\begin{pmatrix}\n",
        "    y_1 \\\\\n",
        "    \\vdots \\\\\n",
        "    y_N\n",
        "  \\end{pmatrix}\n",
        "  $$\n",
        "\n",
        "- Weight vector:\n",
        "  $$\n",
        "  w = \\begin{pmatrix}\n",
        "    w_0 \\\\\n",
        "    \\vdots \\\\\n",
        "    w_D\n",
        "  \\end{pmatrix}\n",
        "  $$\n",
        "\n",
        "The predictive model calculates the estimated target values, $\\hat{y}$, through simple matrix multiplication: $X \\times w$.\n",
        "\n",
        "- Prediction model:\n",
        "  $$\n",
        "  \\hat{y} = X w\n",
        "  $$\n",
        "\n",
        "The loss function is then calculated as half the square of the difference between the predicted and actual target values, expressed as\n",
        "\n",
        "\n",
        "- Loss function:\n",
        "  $$\n",
        "  F(w) = \\frac{1}{2} (y - Xw)^T (y - Xw)\n",
        "  $$\n",
        "\n",
        "In summary, $X$ is your feature matrix, $y$ is the target vector you're trying to predict, and $w$ are the weights you'll adjust to make your predictions as accurate as possible.\n",
        "\n"
      ],
      "metadata": {
        "id": "KfExahWMrDKX"
      },
      "id": "KfExahWMrDKX"
    },
    {
      "cell_type": "markdown",
      "id": "476f761a-2ca5-4ce6-8f04-429246a6bc4c",
      "metadata": {
        "id": "476f761a-2ca5-4ce6-8f04-429246a6bc4c"
      },
      "source": [
        "## How to fit the multivariate linear regression model to the training data\n",
        "\n",
        "To fit the multivariate linear regression model to the training data, our goal is to find the weight vector $\\hat{w}$ that minimizes the loss function. We accomplish this by taking the derivative of the loss function with respect to $w$ and setting it to zero. This leads us to the **Normal Equation**:\n",
        "\n",
        "$$\\hat{\\textbf{w}} = (\\textbf{X}^T \\textbf{X})^{-1} \\textbf{X}^T \\textbf{y}$$\n",
        "\n",
        "In this equation, $\\hat{\\textbf{w}}$ is the weight vector that minimizes the loss, $\\textbf{X}$ is the feature matrix (which includes a first column of ones for the intercept), and $\\textbf{y}$ is the vector of target values.\n",
        "\n",
        " Note that $X^T X$ is a square matrix with dimensions $(D+1) \\times (D+1)$, making it invertible—unless it's singular, but we'll get to that later.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78d1f798-6582-4779-bb5e-f5a5aa7740bf",
      "metadata": {
        "id": "78d1f798-6582-4779-bb5e-f5a5aa7740bf"
      },
      "source": [
        "## Gradient Descent: An Alternative to the Normal Equation\n",
        "\n",
        "When dealing with a massive number of features—say, upwards of 100,000—the Normal Equation starts to lose its appeal due to the computational cost of matrix inversion. That's where gradient descent comes in handy. Unlike the Normal Equation, **gradient descent** is an iterative method. You start with a random initial guess for the weight vector and then keep tweaking it (then iteratively updated ) until you find a value that minimizes your loss function or until it converges to a local minimum..\n",
        "\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/GradientDescent.png\" width = \"300\">\n",
        "\n",
        "\n",
        "The Importance of the Learning Rate\n",
        "One crucial factor to consider is the learning rate.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-4-Regression-models/imgs/learning_rate_comic.jpg\" width = \"300\">\n",
        "\n",
        "\n",
        "If the learning rate is too small, the algorithm may take a very large number of steps to converge. If the learning rate is too large the algorithm may oscillate instead of converging.\n",
        "\n",
        " So, you've got to find that Goldilocks zone for the learning rate, otherwise the gradient descent will not converge. Here you can see.\n",
        "\n",
        "rate is set well, otherwise the gradient descent will not converge. Here you can see\n",
        "examples of different learning rates.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/LearrningRate.png\" width = \"700\">\n",
        "\n",
        "**Scenario 1**\n",
        "\n",
        "**The Ideal Learning Rate:** When set just right, the gradient descent algorithm will efficiently converge to the optimal solution, represented by the red star in this case.\n",
        "\n",
        "**Scenario 2**\n",
        "**Too Small a Learning Rate:** A too-small learning rate is like walking towards a destination in baby steps. You'll get there eventually, but it will take a very long time. Worse still, you might run out of time (or computational resources) before you reach the optimum solution.\n",
        "\n",
        "**Scenario 3**\n",
        "**Too Large a Learning Rate:** On the flip side, a too-large learning rate will make the algorithm oscillate around the minimum like a pendulum that's been pushed too hard. Instead of settling at the lowest point, it'll swing from one side to the other, never truly converging.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another issue to consider with gradient descent is how much of the training data to use at each iteration.\n",
        "\n",
        "\n",
        "**Batch gradient descent:**\n",
        "In the classical gradient descent, also called batch gradient descent, we use all the samples to update the weight vector at each iteration. If we have a large number of samples, this process can be very slow.\n",
        "\n",
        "**Stochastic gradient descent:** Uses samples, this process can be very slow. Stochastic gradient descent only uses one random sample at each iteration, and it is therefore very fast, but it can oscillate and\n",
        "is therefore error prone (unstable).\n",
        "\n",
        "**Mini-batch gradient descent:** A compromise between batch and stochastic, this uses a subset of the training data at each iteration.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/GradientDescentBatches.png\" width = \"700\">"
      ],
      "metadata": {
        "id": "7cSW_bhw2sug"
      },
      "id": "7cSW_bhw2sug"
    },
    {
      "cell_type": "markdown",
      "id": "7a35708e-7636-4e4b-af93-648971e6d87e",
      "metadata": {
        "id": "7a35708e-7636-4e4b-af93-648971e6d87e"
      },
      "source": [
        "## Evaluating performance of regression models\n",
        "\n",
        "### $R^2$ score\n",
        "\n",
        "Certainly! Here's the text with mathematical expressions using a single $ on each side for in-text math:\n",
        "\n",
        "Last week, we discussed how to assess the performance of a regression model using the $ R^2 $ score. This score tells us how well the model explains the variation in the data. A perfect model would have an $ R^2 $ score of 1.\n",
        "\n",
        "To calculate $ R^2 $, we start by finding the unexplained variance, which is essentially the squared difference between the actual and predicted target values. We then divide this by the total variance in the data. Although we usually divide these by the number of samples to get variances, these factors cancel out when calculating $ R^2 $.\n",
        "\n",
        "In code, calculating the $ R^2 $ score is straightforward using Scikit-learn. To find the $ R^2 $ score for all data, you can use the `model.score` method. If you want to use cross-validation, the `cross_val_score` function can help.\n",
        "\n",
        "This is the proportion of variance in the data explained by the model. A perfect fit would have $R^2 = 1$.\n",
        "\n",
        "$$R^2 = 1 - \\frac{\\sum_{i} (y_i - \\hat{y}_i)^2}{\\sum_{i} (y_i - \\bar{y})}$$\n",
        "\n",
        "where the $\\hat{y}_i$s are the predicted target values and $\\hat{y}$ is the average target value where\n",
        "\n",
        "- **Predicted Target Values:**\n",
        "$$\n",
        "\\hat{y}_i = w_0 + \\sum_{k} w_k x_{ik}\n",
        "$$\n",
        "\n",
        "- **Average Target Value:**\n",
        "$$\n",
        "\\bar{y} = \\frac{1}{N} \\sum_{i=1}^{N} y_i\n",
        "$$\n",
        "\n",
        "In these formulas:\n",
        "- $\\hat{y}_i$ is the predicted target value for the $i$-th sample.\n",
        "- $w_0$ is the intercept, and $w_k$ are the weights for the features.\n",
        "- $x_{ik}$ are the feature values for the $i$-th sample.\n",
        "- $\\bar{y}$ is the average target value.\n",
        "- $N$ is the total number of samples.\n",
        "- $y_i$ is the actual target value for the $i$-th sample."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b31ae29-b668-4921-9b59-b553f94979d2",
      "metadata": {
        "id": "4b31ae29-b668-4921-9b59-b553f94979d2"
      },
      "source": [
        "Let's use the $R^2$ score to compare linear regression models for each of the datasets.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/R2_scores.png\" width = \"700\">\n",
        "\n",
        "The $R^2$ scores improve with the larger number of features. However, when we calculate cross-validation scores $R^2$ scores, the best performance is with the 6-feature dataset. This indicates that the 86-feature dataset is being overfitted to the noise in the data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73a4a7e6-df61-4263-a668-8e2df1c135a5",
      "metadata": {
        "id": "73a4a7e6-df61-4263-a668-8e2df1c135a5"
      },
      "source": [
        "### Root mean squared error (RMSE)\n",
        "\n",
        "Another way to measure performance of regression models is with root mean squared error (RMSE).\n",
        "\n",
        "$$ \\text{RMSE} = \\sqrt{\\frac{1}{N} \\sum_{i} (y_i - \\hat{y}_i)^2}$$\n",
        "\n",
        "where the $y_i$s are the target values and the $\\hat{y}_i$s are the predicted values.\n",
        "\n",
        "Note that unlike $R^2$, the RMSE is in the same units as the target values (i.e. weeks in our example).\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/RMSE_scores.png\" width = \"700\">\n",
        "\n",
        "Better RMSE scores are lower values (closer to 0). The RMSE scores agree with $R^2$ scores. The best model is on the 6-feature dataset. The 86-feature dataset has been overfitted."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9983bbdc-245d-4179-b9b0-3c64326c61bb",
      "metadata": {
        "tags": [],
        "id": "9983bbdc-245d-4179-b9b0-3c64326c61bb"
      },
      "source": [
        "## Penalised Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91330a3f-6f54-4433-b6e9-5c79103dba81",
      "metadata": {
        "id": "91330a3f-6f54-4433-b6e9-5c79103dba81"
      },
      "source": [
        "### Ridge regression\n",
        "\n",
        "Overfitting can be avoided by reducing the number of parameters in the model (in the case of linear regression models this can be done by reducing the number of features). An alternative is **regularization**, where a penalty term is added to the loss function.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/RidgePenalty.png\" width = \"500\">\n",
        "\n",
        "The **ridge penalty** penalizes weights with a large magnitude. It is also called the **L2 norm**. $\\lambda$ is a hyperparameter that determines the strength of the regularization.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/L2_norm.png\" width = \"200\">\n",
        "\n",
        "The ridge penalty rises quadratically as the magnitude of the weight increases."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a36ace0e-d9d3-4b34-9a25-bfbe38968eee",
      "metadata": {
        "id": "a36ace0e-d9d3-4b34-9a25-bfbe38968eee"
      },
      "source": [
        "The behavior of ridge regression depends on hyperparameter $\\lambda$.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/RidgeRegressionBehavior.png\" width = \"600\">\n",
        "\n",
        "You can use a grid search to find the optimal lambda values."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fe37998-1afc-4f5f-8ede-7715e87b3039",
      "metadata": {
        "id": "1fe37998-1afc-4f5f-8ede-7715e87b3039"
      },
      "source": [
        "Does ridge regression help avoid overfitting on our sample datasets?\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/RidgeRegresion_scores.png\" width = \"600\">\n",
        "\n",
        "Yes, with an optimal lambda, the overfitting on the 86-feature dataset is avoided."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3f6a165-5faa-414e-914a-b7d9c9239ba5",
      "metadata": {
        "id": "d3f6a165-5faa-414e-914a-b7d9c9239ba5"
      },
      "source": [
        "### Lasso regression\n",
        "\n",
        "An alternative type of regulariztion is **Lasso regression**. This uses the **Lasso penalty** or **L1 norm**.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/LassoPenalty.png\" width = \"500\">\n",
        "\n",
        "The **ridge penalty** penalizes weights with a large magnitude. It is also called the **L2 norm**. $\\lambda$ is a hyperparameter that determines the strength of the regularization.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/L1_norm.png\" width = \"200\">\n",
        "\n",
        "Compared with Ridge, Lasso tends to reduce weights to 0 and thus produces sparse solutions (many weights with value = 0)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6dc15015-b888-4ca2-bdaa-f028b38847ae",
      "metadata": {
        "id": "6dc15015-b888-4ca2-bdaa-f028b38847ae"
      },
      "source": [
        "### Comparison of Ridge and Lasso\n",
        "\n",
        "- Ridge and Lasso penalties both decrease the magnitude of weights\n",
        "- Ridge penalises weights with large magnitude more\n",
        "- Lasso penalises weights with small magnitude more\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/ComparisonRidgeLasso.png\" width = \"700\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8b0c82c-58c0-47f3-abae-229457b9abca",
      "metadata": {
        "id": "f8b0c82c-58c0-47f3-abae-229457b9abca"
      },
      "source": [
        "## Non-linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fcc2dac-0071-4b08-ab9b-1b10150f6119",
      "metadata": {
        "id": "9fcc2dac-0071-4b08-ab9b-1b10150f6119"
      },
      "source": [
        "Let’s now have a look how we can formulate a non linear regression model. In machine learning it is common to consider non-linear _feature transformation_ followed by multivariate linear regression model. We have already seen this when we introduced polynomial regression. In general terms, we denote the non linear feature transformation by $\\phi$ (_phi_), and the prediction model will then become:\n",
        "\n",
        "$$\\hat{y} = \\phi(x)^T \\textbf{w}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3be4cad4-69c0-4590-ade8-44e86eaeac79",
      "metadata": {
        "id": "3be4cad4-69c0-4590-ade8-44e86eaeac79"
      },
      "source": [
        "For example, if we use a polynomial regression model of the second degree, then:\n",
        "\n",
        "Feature transformation for 1D feature vector x:\n",
        "\n",
        "$$\\phi(x) = (1, x, x^2)^T$$\n",
        "\n",
        "Prediction model:\n",
        "\n",
        "$$\\hat{y} = \\phi(x)^T \\textbf{w} = w_0 + x w_1 + x^2 w_2$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c0354a3-e246-4b5e-ac15-393f68e97f62",
      "metadata": {
        "id": "4c0354a3-e246-4b5e-ac15-393f68e97f62"
      },
      "source": [
        "Note: Feature transformation usually increases dimension of a feature vector. In the case of univariate polynomial regression 1D feature vector x is transformed to an $M + 1$ dimensional vector $(1, x, x^2, ..., x^M)^T$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a0c9eeb-ec84-4969-bdfe-8318c9c05a61",
      "metadata": {
        "id": "4a0c9eeb-ec84-4969-bdfe-8318c9c05a61"
      },
      "source": [
        "Loss function for non-linear regression:\n",
        "\n",
        "$$F(\\textbf{w}) = \\frac{1}{2} \\sum_{i}(y_i - \\phi(\\textbf{x}_i)^T \\textbf{w})^2$$\n",
        "\n",
        "Non-linear ridge regression (to avoid overfitting)\n",
        "\n",
        "$$F(\\textbf{w}) = \\frac{1}{2} \\sum_{i}(y_i - \\phi(\\textbf{x}_i)^T \\textbf{w})^2 + \\frac{\\lambda}{2}\\textbf{w}^T\\textbf{w}$$\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/PolynomialRidgeRegression2.png\" width = \"700\">\n",
        "\n",
        "The loss function will then become a sum of squared errors between the expected target values $y_i$ and the target values predicted using the modified model. Because we tend to significantly increase the number of features by introducing feature transformation, we will include ridge penalty in our non linear regression model.\n",
        "\n",
        "In this plot you can see how ridge penalty can be useful for polynomial regression. In the plot on the left we have fitted polynomial of 10 th degree to predict age from cortical volume. We can see that model slightly overfitted the data and CV R2 is 0.88. If we add ridge penalty with lambda 0.25, the overfitting is reduced and the CV $R^2$ increases to 0.9."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40f76f5d-5786-4f3f-86ac-0a205d563586",
      "metadata": {
        "id": "40f76f5d-5786-4f3f-86ac-0a205d563586"
      },
      "source": [
        "How does polynomial ridge regression change with increasing number of features?\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/PolynomialRidgeRegressionPerformance.png\" width = \"700\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09aa6d55-a695-48b6-90ed-3e731bab5b2c",
      "metadata": {
        "id": "09aa6d55-a695-48b6-90ed-3e731bab5b2c"
      },
      "source": [
        "### Kernel trick\n",
        "\n",
        "The kernel trick helps us to design more versatile non-linear regression models.\n",
        "\n",
        "We define a **dual representation** $a$ of our model parameter vector $\\textbf{w}$\n",
        "\n",
        "$$\\textbf{w} = \\phi^T a$$\n",
        "\n",
        "The prediction model then becomes\n",
        "\n",
        "\n",
        "$$\\hat{y} = \\phi^T(x) \\textbf{w} = \\phi^T(x) \\phi^T a = \\sum^N_{i=1} \\phi^T(x) + \\phi(x_i) a_i$$\n",
        "\n",
        "We now define a **kernel** $\\kappa$\n",
        "\n",
        "$$\\kappa(x, x_i) = \\phi^T(x)\\phi(x_i)$$\n",
        "\n",
        "The kernel represents a similarity between two feature vectors.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/GaussianKernel.png\" width = \"300\"  align=\"right\">\n",
        "\n",
        "For example, we can use a **gaussian kernel**\n",
        "\n",
        "$$\\kappa(x, x_i) = e^{-\\frac{\\lvert x - x_i \\rvert_2^2}{2\\sigma}}$$\n",
        "\n",
        "$$\\lvert x - x_i \\rvert_2^2 = \\sum_k(x_k- x_{ik})^2$$\n",
        "\n",
        "Note:\n",
        "- The original parameter vector $\\textbf{w}$ vector has $D$ elements (one per each feature)\n",
        "- The dual parameter vector $a$ has $N$ elements (one per each sample)\n",
        "\n",
        "The resulting **dual prediction model** is a linear combination of kernels placed around the feature vectors $x_i$\n",
        "\n",
        "$$\\hat{y} = \\sum^N_{i=1} \\kappa(x, x_i) a_i$$\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/DualPredictionModel.png\" width = \"700\">\n",
        "\n",
        "In this plot we can see three samples with feature vectors $x_1$, $x_2$ and $x_3$. The gaussian kernels are placed around them, multiplied by\n",
        "coefficients ai and then summed up to produce the prediction model. We can also see, that large kernels produce smoother models."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aba9e1d8-5152-4854-bcd2-dfd06141fc32",
      "metadata": {
        "id": "aba9e1d8-5152-4854-bcd2-dfd06141fc32"
      },
      "source": [
        "We will see more details on non-linear regression in Notebook 4.4."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}