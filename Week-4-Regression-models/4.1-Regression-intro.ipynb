{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaralAminpour/ML-BME-Course-UofA-Fall-2023/blob/main/Week-4-Regression-models/4.1-Regression-intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57170492-140c-4d56-8e94-4a49232293ef",
      "metadata": {
        "id": "57170492-140c-4d56-8e94-4a49232293ef"
      },
      "source": [
        "# Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6320406c-7cb4-4c24-8773-fe1aa94795f1",
      "metadata": {
        "id": "6320406c-7cb4-4c24-8773-fe1aa94795f1"
      },
      "source": [
        "## Application: neonatal brain growth\n",
        "\n",
        "- Around the time of birth the brain grows very quickly\n",
        "- Preterm birth alters brain development\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/DevelopingPretermBrain.png\" width = \"300\" style=\"float: left;\">\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/Baby.png\" width = \"200\" style=\"float: right;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4519d6ff-afc0-4cfe-9592-ccbbe13f318e",
      "metadata": {
        "id": "4519d6ff-afc0-4cfe-9592-ccbbe13f318e"
      },
      "source": [
        "We will demonstrate the regression concepts using the application of brain growth in preterm neonates. Around the time of birth the brain grows very quickly. Preterm birth can disrupt this process, and therefore preterm brain development is a subject of extensive research.\n",
        "\n",
        "To investigate the changes caused by preterm birth, we can acquire MRI scans of newborn babies. We can perform automatic segmentations of various brain structures and measure their volumes. The features in the datasets we'll be working with are brain volumes, either for (1) the whole brain, (2) six brain tissues, or (3) 86 brain structures.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/BrainMRI-BrainSegmentation.png\" width = \"700\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fe8f6db-e261-45eb-8d19-1483be8b96c6",
      "metadata": {
        "id": "2fe8f6db-e261-45eb-8d19-1483be8b96c6"
      },
      "source": [
        "The machine learning regression is the predict the age (target value) of a baby from volumes of brain structures (features).\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/AgeVolumePlot.png\" width = \"400\">\n",
        "\n",
        "In this plot each circle corresponds to a baby (a sample), on the x-axis we have the brain volume (a feature scaled with StandardScaler). The age (target value) is plotted on the y-axis. In this case we are showing a univariate linear regression model in red, with two parameters (slope and intercept). We have achieved R2 score 0.85 which is quite good, so the linear model fits this dataset well."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How can we increase the performance of this model?\n",
        "\n",
        "So, you're wondering if we can boost the performance of our model, right?\n",
        "Specifically, you're looking to get that R2 score closer to the perfect 1. Well, let's dig into the type of error we're dealing with in our dataset. What's your guess?\n",
        "\n",
        "- bias\n",
        "\n",
        "- variance\n",
        "\n",
        "- noise\n",
        "\n",
        "**Good news:**\n",
        "\n",
        "1. we're not dealing with much bias here. Our linear model seems to capture the general trend quite nicely, so there's no glaring systematic issue.\n",
        "\n",
        "2. And as for variance, our model is a simple one, with only two parameters, so it's unlikely that overfitting is the culprit.\n",
        "\n",
        "3. That leaves us with noise, which, unfortunately, we can't really get rid of by switching up the model.\n",
        "\n",
        "Now, what about cranking up the complexity by using a dataset with 6 features or even a whopping 86 features?\n",
        "\n",
        "Sure, we could venture into **multivariate linear regression** territory.\n",
        "\n",
        "But remember, adding more features doesn't always mean better predictions, especially when the main issue is noise. Keep that in mind, and happy modeling!"
      ],
      "metadata": {
        "id": "RpveaNO9e7mV"
      },
      "id": "RpveaNO9e7mV"
    },
    {
      "cell_type": "markdown",
      "id": "eb30d104-3cb7-4f8a-828c-ff9796f95495",
      "metadata": {
        "id": "eb30d104-3cb7-4f8a-828c-ff9796f95495"
      },
      "source": [
        "## Multivariate linear regression\n",
        "\n",
        "Can we improve the predictions by using the 6-feature or 86-feature datasets? We can use multivariate linear regression for these datasets.\n",
        "\n",
        "Multiple features: $$x_{i1}, ... , x_{iD}$$\n",
        "\n",
        "Target values: $$y_i$$\n",
        "\n",
        "Samples: $$i = 1, ..., N$$\n",
        "\n",
        "Prediction model: $$\\hat{y} = w_0  + w_1x_1 + ... + w_D x_D$$\n",
        "\n",
        "Loss function: Sum of square errors\n",
        "\n",
        "$$F(\\textbf{w}) = \\frac{1}{2} \\sum_{i}(y_i - \\sum_{k} w_k x_{ik} - w_0)^2$$\n",
        "\n",
        "The model is fitted by minimizing the loss function (sum of squared errors)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the realm of linear regression, we refer to it as multivariate when there's more than just one feature to consider. For instance, let's say we are examining the volumes of D different structures in the brain. These volumes are represented as $ x_{i1}, \\ldots, x_{iD} $. For each of these D-dimensional feature vectors, there is an associated target value, $ y_i $, which in this specific application, corresponds to the age at the time of the scan.\n",
        "\n",
        "In this scenario, the index $ i $ identifies individual samples in our training set. In the context of our example, each sample corresponds to an individual baby. We have a total of $ N $ samples in our dataset. The predictive model we're working with is based on a linear equation. In this equation, $ w_0 $ serves as the intercept and the weights $ w_1 $ through $ w_N $ are the slopes associated with various features.\n",
        "\n",
        "Our goal is to fine-tune these weights, which are essentially the parameters of our model, to make the most accurate predictions possible. The predicted target value in this context is represented as $ \\hat{y} $. To find the best-fitting model, we minimize the sum of squared errors between the predicted and the actual target values.\n",
        "\n"
      ],
      "metadata": {
        "id": "TeIp02EkiZEq"
      },
      "id": "TeIp02EkiZEq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Matrix formulation\n",
        "\n",
        "It is also useful to express the\n",
        "linear regression problem using matrix formulation.\n",
        "\n",
        "In this version, we use a feature matrix, denoted by $X$, which contains all samples and their respective features.\n",
        "\n",
        "To simplify the calculations, we include an extra column of ones in this matrix. This extra column corresponds to the model's intercept, allowing the number of features and parameters ($w$) to be identical, at $D+1$.\n",
        "\n",
        "- Feature matrix:\n",
        "  $$\n",
        "  X = \\begin{pmatrix}\n",
        "    1 & x_{11} & \\cdots & x_{1D} \\\\\n",
        "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "    1 & x_{N1} & \\cdots & x_{ND}\n",
        "  \\end{pmatrix}\n",
        "  $$\n",
        "\n",
        "The target values are represented by the vector $y$, while the weight parameters are in the vector $w$.\n",
        "\n",
        "- Target vector:\n",
        "  $$\n",
        "  y = \\begin{pmatrix}\n",
        "    y_1 \\\\\n",
        "    \\vdots \\\\\n",
        "    y_N\n",
        "  \\end{pmatrix}\n",
        "  $$\n",
        "\n",
        "- Weight vector:\n",
        "  $$\n",
        "  w = \\begin{pmatrix}\n",
        "    w_0 \\\\\n",
        "    \\vdots \\\\\n",
        "    w_D\n",
        "  \\end{pmatrix}\n",
        "  $$\n",
        "\n",
        "The predictive model calculates the estimated target values, $\\hat{y}$, through simple matrix multiplication: $X \\times w$.\n",
        "\n",
        "- Prediction model:\n",
        "  $$\n",
        "  \\hat{y} = X w\n",
        "  $$\n",
        "\n",
        "The loss function is then calculated as half the square of the difference between the predicted and actual target values, expressed as\n",
        "\n",
        "\n",
        "- Loss function:\n",
        "  $$\n",
        "  F(w) = \\frac{1}{2} (y - Xw)^T (y - Xw)\n",
        "  $$\n",
        "\n",
        "In summary, $X$ is your feature matrix, $y$ is the target vector you're trying to predict, and $w$ are the weights you'll adjust to make your predictions as accurate as possible.\n",
        "\n"
      ],
      "metadata": {
        "id": "KfExahWMrDKX"
      },
      "id": "KfExahWMrDKX"
    },
    {
      "cell_type": "markdown",
      "id": "476f761a-2ca5-4ce6-8f04-429246a6bc4c",
      "metadata": {
        "id": "476f761a-2ca5-4ce6-8f04-429246a6bc4c"
      },
      "source": [
        "## How to fit the multivariate linear regression model to the training data\n",
        "\n",
        "To fit the multivariate linear regression model to the training data, our goal is to find the weight vector $\\hat{w}$ that minimizes the loss function. We accomplish this by taking the derivative of the loss function with respect to $w$ and setting it to zero. This leads us to the **Normal Equation**:\n",
        "\n",
        "$$\\hat{\\textbf{w}} = (\\textbf{X}^T \\textbf{X})^{-1} \\textbf{X}^T \\textbf{y}$$\n",
        "\n",
        "In this equation, $\\hat{\\textbf{w}}$ is the weight vector that minimizes the loss, $\\textbf{X}$ is the feature matrix (which includes a first column of ones for the intercept), and $\\textbf{y}$ is the vector of target values.\n",
        "\n",
        " Note that $X^T X$ is a square matrix with dimensions $(D+1) \\times (D+1)$, making it invertible—unless it's singular, but we'll get to that later.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78d1f798-6582-4779-bb5e-f5a5aa7740bf",
      "metadata": {
        "id": "78d1f798-6582-4779-bb5e-f5a5aa7740bf"
      },
      "source": [
        "## Gradient Descent: An Alternative to the Normal Equation\n",
        "\n",
        "When dealing with a massive number of features—say, upwards of 100,000—the Normal Equation starts to lose its appeal due to the computational cost of matrix inversion. That's where gradient descent comes in handy. Unlike the Normal Equation, **gradient descent** is an iterative method. You start with a random initial guess for the weight vector and then keep tweaking it (then iteratively updated ) until you find a value that minimizes your loss function or until it converges to a local minimum..\n",
        "\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/GradientDescent.png\" width = \"300\">\n",
        "\n",
        "\n",
        "The Importance of the Learning Rate\n",
        "One crucial factor to consider is the learning rate.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-4-Regression-models/imgs/learning_rate_comic.jpg\" width = \"300\">\n",
        "\n",
        "\n",
        "If the learning rate is too small, the algorithm may take a very large number of steps to converge. If the learning rate is too large the algorithm may oscillate instead of converging.\n",
        "\n",
        " So, you've got to find that Goldilocks zone for the learning rate, otherwise the gradient descent will not converge. Here you can see.\n",
        "\n",
        "rate is set well, otherwise the gradient descent will not converge. Here you can see\n",
        "examples of different learning rates.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/LearrningRate.png\" width = \"700\">\n",
        "\n",
        "**Scenario 1**\n",
        "\n",
        "**The Ideal Learning Rate:** When set just right, the gradient descent algorithm will efficiently converge to the optimal solution, represented by the red star in this case.\n",
        "\n",
        "**Scenario 2**\n",
        "**Too Small a Learning Rate:** A too-small learning rate is like walking towards a destination in baby steps. You'll get there eventually, but it will take a very long time. Worse still, you might run out of time (or computational resources) before you reach the optimum solution.\n",
        "\n",
        "**Scenario 3**\n",
        "**Too Large a Learning Rate:** On the flip side, a too-large learning rate will make the algorithm oscillate around the minimum like a pendulum that's been pushed too hard. Instead of settling at the lowest point, it'll swing from one side to the other, never truly converging.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another issue to consider with gradient descent is how much of the training data to use at each iteration.\n",
        "\n",
        "\n",
        "**Batch gradient descent:**\n",
        "In the classical gradient descent, also called batch gradient descent, we use all the samples to update the weight vector at each iteration. If we have a large number of samples, this process can be very slow.\n",
        "\n",
        "**Stochastic gradient descent:** Uses samples, this process can be very slow. Stochastic gradient descent only uses one random sample at each iteration, and it is therefore very fast, but it can oscillate and\n",
        "is therefore error prone (unstable).\n",
        "\n",
        "**Mini-batch gradient descent:** A compromise between batch and stochastic, this uses a subset of the training data at each iteration.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/GradientDescentBatches.png\" width = \"700\">"
      ],
      "metadata": {
        "id": "7cSW_bhw2sug"
      },
      "id": "7cSW_bhw2sug"
    },
    {
      "cell_type": "markdown",
      "id": "7a35708e-7636-4e4b-af93-648971e6d87e",
      "metadata": {
        "id": "7a35708e-7636-4e4b-af93-648971e6d87e"
      },
      "source": [
        "## Evaluating performance of regression models\n",
        "\n",
        "### $R^2$ score\n",
        "\n",
        "Certainly! Here's the text with mathematical expressions using a single $ on each side for in-text math:\n",
        "\n",
        "Last week, we discussed how to assess the performance of a regression model using the $ R^2 $ score. This score tells us how well the model explains the variation in the data. A perfect model would have an $ R^2 $ score of 1.\n",
        "\n",
        "To calculate $ R^2 $, we start by finding the unexplained variance, which is essentially the squared difference between the actual and predicted target values. We then divide this by the total variance in the data. Although we usually divide these by the number of samples to get variances, these factors cancel out when calculating $ R^2 $.\n",
        "\n",
        "In code, calculating the $ R^2 $ score is straightforward using Scikit-learn. To find the $ R^2 $ score for all data, you can use the `model.score` method. If you want to use cross-validation, the `cross_val_score` function can help.\n",
        "\n",
        "This is the proportion of variance in the data explained by the model. A perfect fit would have $R^2 = 1$.\n",
        "\n",
        "$$R^2 = 1 - \\frac{\\sum_{i} (y_i - \\hat{y}_i)^2}{\\sum_{i} (y_i - \\bar{y})}$$\n",
        "\n",
        "where the $\\hat{y}_i$s are the predicted target values and $\\hat{y}$ is the average target value where\n",
        "\n",
        "- **Predicted Target Values:**\n",
        "$$\n",
        "\\hat{y}_i = w_0 + \\sum_{k} w_k x_{ik}\n",
        "$$\n",
        "\n",
        "- **Average Target Value:**\n",
        "$$\n",
        "\\bar{y} = \\frac{1}{N} \\sum_{i=1}^{N} y_i\n",
        "$$\n",
        "\n",
        "In these formulas:\n",
        "- $\\hat{y}_i$ is the predicted target value for the $i$-th sample.\n",
        "- $w_0$ is the intercept, and $w_k$ are the weights for the features.\n",
        "- $x_{ik}$ are the feature values for the $i$-th sample.\n",
        "- $\\bar{y}$ is the average target value.\n",
        "- $N$ is the total number of samples.\n",
        "- $y_i$ is the actual target value for the $i$-th sample."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b31ae29-b668-4921-9b59-b553f94979d2",
      "metadata": {
        "id": "4b31ae29-b668-4921-9b59-b553f94979d2"
      },
      "source": [
        "Let's use the $R^2$ score to compare linear regression models for each of the datasets.\n",
        "\n",
        "Now, let's explore if increasing the number of features can improve the $R^2$ score. When we look at the $R^2$ score for the entire dataset, it does increase as we add more features. However, similar to what we observed with polynomial degrees, it's not clear if this improvement is due to a better fit or just overfitting.\n",
        "\n",
        "To clarify this, we can calculate the cross-validated $R^2$ score. Our findings show that the performance improves as we increase the number of features from one to six, but then declines when we jump to 86 features. This leads us to conclude that the optimal model has 6 features, with a cross-validated $R^2$ score of 0.89. On the other hand, the model with 86 features appears to be overfitting, as its cross-validated $R^2$ score drops to 0.68.\n",
        "\n",
        "Keep in mind that adding more features also increases the complexity of the model, as it has to fit more parameters.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/R2_scores.png\" width = \"700\">\n",
        "\n",
        "**Summary:** The $R^2$ scores improve with the larger number of features. However, when we calculate cross-validation scores $R^2$ scores, the best performance is with the 6-feature dataset. This indicates that the 86-feature dataset is being overfitted to the noise in the data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73a4a7e6-df61-4263-a668-8e2df1c135a5",
      "metadata": {
        "id": "73a4a7e6-df61-4263-a668-8e2df1c135a5"
      },
      "source": [
        "### Root mean squared error (RMSE)\n",
        "\n",
        "Another method for evaluating the performance of regression models is to use the Root Mean Squared Error (RMSE). RMSE provides an average error in the same units as the target values, with a perfect model yielding an RMSE of zero.\n",
        "\n",
        "In a simple terms, RMSE tells you the average mistake your model makes in the same units as what you're trying to predict. A perfect model would have an RMSE of zero, meaning it makes no mistakes.\n",
        "\n",
        "$$ \\text{RMSE} = \\sqrt{\\frac{1}{N} \\sum_{i} (y_i - \\hat{y}_i)^2}$$\n",
        "\n",
        "where the formula for predicted target values can be rewritten as:\n",
        "\n",
        "$$\n",
        "\\hat{y}_i = w_0 + \\sum_{k} w_k x_{ik}\n",
        "$$\n",
        "\n",
        "In this formula:\n",
        "- $\\hat{y}_i$ represents the predicted target value for the $i$-th sample.\n",
        "- $w_0$ is the intercept term.\n",
        "- $w_k$ are the feature weights.\n",
        "- $x_{ik}$ are the feature values for the $i$-th sample.\n",
        "- The sum runs over all features \\( k \\).\n",
        "\n",
        "where the $y_i$s are the target values and the $\\hat{y}_i$s are the predicted values.\n",
        "\n",
        "Note that unlike $R^2$, the RMSE is in the same units as the target values (i.e. weeks in our example).\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/RMSE_scores.png\" width = \"700\">\n",
        "\n",
        "Here is the performance for the different datasets measured by RMSE. Better RMSE scores are lower values (closer to 0). The RMSE scores agree with $R^2$ scores. We see that RMSE on the whole set decreases with increasing number of features.  When we use cross validation, the lowest error is achieved for 6 features, 1.27 weeks, and the\n",
        "models with 86 features is overfitted to the noise in the data, because it has large\n",
        "cross validated RMSE.\n",
        "\n",
        "**Summary:** The best model is on the 6-feature dataset. The 86-feature dataset has been overfitted.\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To compute RMSE using Scikit-learn, you first fit the model and make predictions. After that, you calculate the mean squared error between the actual and predicted target values and take the square root to get the RMSE. You can also evaluate the model's performance by training on one dataset and testing it on another.\n",
        "\n",
        "For cross-validated RMSE, you can use Scikit-learn's `cross_val_score` function. However, you'll need to set the `scoring` parameter to \"negative mean squared error.\" This function returns an array of scores for all the cross-validation folds. These scores are negative, so you'll need to negate them and then take the square root to get the RMSE for each fold. Finally, you average these values to get the overall cross-validated RMSE."
      ],
      "metadata": {
        "id": "gfyhTdqT-KaP"
      },
      "id": "gfyhTdqT-KaP"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating performance of regression models\n",
        "\n",
        "So what is the relationship between R2 score and RMSE?\n",
        "\n",
        "Both are related to sum of\n",
        "squared errors, which is also our loss function. While R2 score is normalised and\n",
        "therefore comparable between models and datasets, RMSE is interpretable, because\n",
        "it is expressed in units of the target values.\n",
        "\n",
        "- **Sum of Squared Errors (SSE) Loss Function:**\n",
        "\n",
        "$$\n",
        "F(w) = \\frac{1}{2} \\sum_{i} \\left( y_i - \\hat{y}_i \\right)^2\n",
        "$$\n",
        "\n",
        "- **R-squared ($R^2$) Score:**\n",
        "\n",
        "$$\n",
        "R^2 = 1 - \\frac{F(w)}{\\sigma^2 \\left( y_i - \\bar{y} \\right)^2}\n",
        "$$\n",
        "\n",
        "- **Root Mean Squared Error (RMSE):**\n",
        "\n",
        "$$\n",
        "RMSE = \\sqrt{\\frac{1}{N} F(w)}\n",
        "$$\n",
        "\n",
        "In these formulas:\n",
        "\n",
        "- $F(w)$ represents the sum of squared errors between the expected ($y_i$) and predicted ($\\hat{y}_i$) target values.\n",
        "\n",
        "- $R^2$ is a normalized score that can be compared between different models or datasets.\n",
        "- $RMSE$ is interpretable because it's in the same units as the target values.\n",
        "- $N$ is the total number of samples.\n",
        "- $\\sigma^2$ is the variance of the target values, and $\\bar{y}$ is their average."
      ],
      "metadata": {
        "id": "C9qWbkg0BKXs"
      },
      "id": "C9qWbkg0BKXs"
    },
    {
      "cell_type": "markdown",
      "id": "9983bbdc-245d-4179-b9b0-3c64326c61bb",
      "metadata": {
        "tags": [],
        "id": "9983bbdc-245d-4179-b9b0-3c64326c61bb"
      },
      "source": [
        "## Penalised Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Overfitting\n",
        "\n",
        "Overfitting is like the classic \"too much of a good thing\" problem in machine learning, including in multivariate linear regression. You'd think that adding extra features is like adding more spices to your cooking—it'll only make things tastier, right? Well, not necessarily. Too many features can cause your model to learn the \"noise,\" or random fluctuations in the data, rather than the real pattern you're interested in. When that happens, your model may ace the training data but will likely bomb on new, unseen data.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-4-Regression-models/imgs/overfitting_comic.png\" width = \"400\">\n",
        "\n",
        "performance of multivariate linear regression. This is due to overfitting. So what is the relationship between number of samples, number of features and performance\n",
        "of the model?\n",
        "\n",
        "- The typical way to get the model parameters $w$ in multivariate linear regression is by using the normal equation:\n",
        "\n",
        "$$\n",
        "w = (X^T X)^{-1} X^T y\n",
        "$$\n",
        "\n",
        "- In this equation, $X$ is the feature matrix, which has dimensions $N \\times (D+1)$. Here, $N$ is the number of samples and $D$ is the number of features. The vector $y$ contains your target or output values. The $(D+1)$ bit comes in because we usually add a \"bias\" term to the features.\n",
        "\n",
        "- Now, here's where it gets tricky. You have to invert the matrix $X^T X$, and this is a $(D+1) \\times (D+1)$ matrix. Inverting matrices isn't just a click of a button; it's computationally heavy and can be problematic if $X^T X$ is nearly singular (which is a fancy term for non-invertible or ill-conditioned).\n",
        "\n",
        "- Also, don't forget that the rank of the matrix $X^T X$ can't be higher than $N$, the number of your samples. So if you have more features than samples ($D+1 > N$), then you're in trouble: $X^T X$ becomes singular and you can't invert it. Even if you have fewer features ($D+1 \\leq N$), a low rank could set the stage for overfitting.\n",
        "\n",
        "- This is when Ridge regression becomes the hero of the day. It adds a penalty term to the loss function, effectively converting the matrix to be inverted into $X^T X + \\lambda I$. This not only makes it invertible but also helps in keeping overfitting at bay."
      ],
      "metadata": {
        "id": "AvLejpW89NYh"
      },
      "id": "AvLejpW89NYh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The relationship between the number of samples \\( N \\) and the number of features \\( D+1 \\)\n",
        "\n",
        "The number of samples $N$ and the number of features $D+1$ play a vital role in how well a multivariate linear regression model performs, particularly when you're worried about overfitting. Here's a quick rundown:\n",
        "\n",
        "- **Less Samples than Features ($N < D+1$)**\n",
        "  - In this tricky situation, the matrix $X^T X$ isn't invertible. This happens because you've got more unknowns than equations, leaving you with an underdetermined system. Your options are limited: you can either reduce the number of features or introduce regularization methods to make the matrix invertible.\n",
        "\n",
        "- **Slightly More Samples than Features ($N \\approx D+1$)**\n",
        "  - Okay, so here the matrix $X^T X$ is invertible, but it's not time to celebrate just yet. Your model might get too cozy with the noise in the data, picking up random fluctuations instead of the real trend. The result? It might do great on the training data but stumble when it sees new, unseen data. To handle this, you could use regularization techniques like Ridge or Lasso.\n",
        "\n",
        "- **Significantly More Samples than Features ($N \\gg D+1$)**\n",
        "  - Now you're talking! In this case, the matrix $X^T X$ is happily invertible, and the risk of overfitting drops. With a heap of data at your disposal, your model can do a much better job capturing the underlying trends, setting you up for success on new data.\n",
        "\n",
        "- **Increasing the Number of Samples**\n",
        "  - If you're still wringing your hands about overfitting, there's a simple fix—just add more samples to the training set. This beefs up your model's understanding of the underlying data distribution and helps it generalize better to new, out-of-sample data.\n",
        "\n",
        "So, the balance between your number of samples and features is crucial for your model's performance, particularly when overfitting is on the radar."
      ],
      "metadata": {
        "id": "af7bnuwV-Zvf"
      },
      "id": "af7bnuwV-Zvf"
    },
    {
      "cell_type": "markdown",
      "id": "91330a3f-6f54-4433-b6e9-5c79103dba81",
      "metadata": {
        "id": "91330a3f-6f54-4433-b6e9-5c79103dba81"
      },
      "source": [
        "### Ridge regression\n",
        "\n",
        "Overfitting can be avoided by reducing the number of parameters in the model (in the case of linear regression models this can be done by reducing the number of features). An alternative is **regularization**, where a penalty term is added to the loss function.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/RidgePenalty.png\" width = \"500\">\n",
        "\n",
        "The **ridge penalty** penalizes weights with a large magnitude. It is also called the **L2 norm**. $\\lambda$ is a hyperparameter that determines the strength of the regularization.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/L2_norm.png\" width = \"200\">\n",
        "\n",
        "The ridge penalty rises quadratically as the magnitude of the weight increases.\n",
        "\n",
        "Absolutely, Ridge regression is a form of regularization that tackles the overfitting problem by penalizing the magnitude of the model weights. Let's delve into some of the key aspects:\n",
        "\n",
        "- **Penalizes Weights with Large Magnitude**\n",
        "  Ridge regression uses the squared L2 norm of the weight vector $w$ as a penalty term. The idea is to discourage the model from assigning excessively large weights to the features. The larger the weight, the higher the penalty.\n",
        "  \n",
        "- **Hyperparameter $ \\lambda $**\n",
        "\n",
        "  The strength of this regularization is controlled by the hyperparameter $\\lambda$. A high value of $\\lambda$ will result in a stronger penalty, pushing the weights closer to zero. Conversely, a lower value of $\\lambda$ will make the regularization less severe.\n",
        "\n",
        "- **Matrix Formulation**\n",
        "\n",
        "  The loss function $ F(w) $ for Ridge regression can be expressed in matrix form as:\n",
        "\n",
        "$$\n",
        "F(w) = \\frac{1}{2} (y - Xw)^T (y - Xw) + \\frac{\\lambda}{2} w^T w\n",
        "$$\n",
        "\n",
        "This function combines the standard loss term, which measures how well the model fits the data, with the regularization term, which penalizes large weights. The $\\frac{1}{2}$ in front of each term is often added for mathematical convenience, making it easier to differentiate the function.\n",
        "\n",
        "In a nutshell, Ridge regression provides a way to balance fitting the data well while keeping the model weights in check, effectively helping to manage overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have seen that overfitting in multivariate linear regression can be addressed by\n",
        "reducing the number of features. An alternative is to keep the same features and\n",
        "regularise the model instead. To do that we can add a penalty to the loss function\n",
        "that will discourage weights with very large magnitude. If the penalty is calculated\n",
        "using squared L2 norm of the weight vector (which is equal to sum of squares of the\n",
        "individual weights), it is called Ridge penalty. The hyperparameter lambda determines\n",
        "the strength of the penalty term. The ridge penalty can be expressed in matrix form\n",
        "as w transposed times w. The penalty for a single weight is plotted in this graph. You\n",
        "can see that it is zero for a zero weight, and rises quadratically as the magnitude of\n",
        "the w increases, whether it is negative or positive."
      ],
      "metadata": {
        "id": "C_M5XpviEX0x"
      },
      "id": "C_M5XpviEX0x"
    },
    {
      "cell_type": "markdown",
      "id": "a36ace0e-d9d3-4b34-9a25-bfbe38968eee",
      "metadata": {
        "id": "a36ace0e-d9d3-4b34-9a25-bfbe38968eee"
      },
      "source": [
        "The behavior of ridge regression depends on hyperparameter $\\lambda$.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/RidgeRegressionBehavior.png\" width = \"600\">\n",
        "\n",
        "You can use a grid search to find the optimal lambda values."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fe37998-1afc-4f5f-8ede-7715e87b3039",
      "metadata": {
        "id": "1fe37998-1afc-4f5f-8ede-7715e87b3039"
      },
      "source": [
        "Does ridge regression help avoid overfitting on our sample datasets?\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/RidgeRegresion_scores.png\" width = \"600\">\n",
        "\n",
        "Yes, with an optimal lambda, the overfitting on the 86-feature dataset is avoided."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3f6a165-5faa-414e-914a-b7d9c9239ba5",
      "metadata": {
        "id": "d3f6a165-5faa-414e-914a-b7d9c9239ba5"
      },
      "source": [
        "### Lasso regression\n",
        "\n",
        "An alternative type of regulariztion is **Lasso regression**. This uses the **Lasso penalty** or **L1 norm**.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/LassoPenalty.png\" width = \"500\">\n",
        "\n",
        "The **ridge penalty** penalizes weights with a large magnitude. It is also called the **L2 norm**. $\\lambda$ is a hyperparameter that determines the strength of the regularization.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/L1_norm.png\" width = \"200\">\n",
        "\n",
        "Compared with Ridge, Lasso tends to reduce weights to 0 and thus produces sparse solutions (many weights with value = 0)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6dc15015-b888-4ca2-bdaa-f028b38847ae",
      "metadata": {
        "id": "6dc15015-b888-4ca2-bdaa-f028b38847ae"
      },
      "source": [
        "### Comparison of Ridge and Lasso\n",
        "\n",
        "- Ridge and Lasso penalties both decrease the magnitude of weights\n",
        "- Ridge penalises weights with large magnitude more\n",
        "- Lasso penalises weights with small magnitude more\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/ComparisonRidgeLasso.png\" width = \"700\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8b0c82c-58c0-47f3-abae-229457b9abca",
      "metadata": {
        "id": "f8b0c82c-58c0-47f3-abae-229457b9abca"
      },
      "source": [
        "## Non-linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fcc2dac-0071-4b08-ab9b-1b10150f6119",
      "metadata": {
        "id": "9fcc2dac-0071-4b08-ab9b-1b10150f6119"
      },
      "source": [
        "Let’s now have a look how we can formulate a non linear regression model. In machine learning it is common to consider non-linear _feature transformation_ followed by multivariate linear regression model. We have already seen this when we introduced polynomial regression. In general terms, we denote the non linear feature transformation by $\\phi$ (_phi_), and the prediction model will then become:\n",
        "\n",
        "$$\\hat{y} = \\phi(x)^T \\textbf{w}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3be4cad4-69c0-4590-ade8-44e86eaeac79",
      "metadata": {
        "id": "3be4cad4-69c0-4590-ade8-44e86eaeac79"
      },
      "source": [
        "For example, if we use a polynomial regression model of the second degree, then:\n",
        "\n",
        "Feature transformation for 1D feature vector x:\n",
        "\n",
        "$$\\phi(x) = (1, x, x^2)^T$$\n",
        "\n",
        "Prediction model:\n",
        "\n",
        "$$\\hat{y} = \\phi(x)^T \\textbf{w} = w_0 + x w_1 + x^2 w_2$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c0354a3-e246-4b5e-ac15-393f68e97f62",
      "metadata": {
        "id": "4c0354a3-e246-4b5e-ac15-393f68e97f62"
      },
      "source": [
        "Note: Feature transformation usually increases dimension of a feature vector. In the case of univariate polynomial regression 1D feature vector x is transformed to an $M + 1$ dimensional vector $(1, x, x^2, ..., x^M)^T$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a0c9eeb-ec84-4969-bdfe-8318c9c05a61",
      "metadata": {
        "id": "4a0c9eeb-ec84-4969-bdfe-8318c9c05a61"
      },
      "source": [
        "Loss function for non-linear regression:\n",
        "\n",
        "$$F(\\textbf{w}) = \\frac{1}{2} \\sum_{i}(y_i - \\phi(\\textbf{x}_i)^T \\textbf{w})^2$$\n",
        "\n",
        "Non-linear ridge regression (to avoid overfitting)\n",
        "\n",
        "$$F(\\textbf{w}) = \\frac{1}{2} \\sum_{i}(y_i - \\phi(\\textbf{x}_i)^T \\textbf{w})^2 + \\frac{\\lambda}{2}\\textbf{w}^T\\textbf{w}$$\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/PolynomialRidgeRegression2.png\" width = \"700\">\n",
        "\n",
        "The loss function will then become a sum of squared errors between the expected target values $y_i$ and the target values predicted using the modified model. Because we tend to significantly increase the number of features by introducing feature transformation, we will include ridge penalty in our non linear regression model.\n",
        "\n",
        "In this plot you can see how ridge penalty can be useful for polynomial regression. In the plot on the left we have fitted polynomial of 10 th degree to predict age from cortical volume. We can see that model slightly overfitted the data and CV R2 is 0.88. If we add ridge penalty with lambda 0.25, the overfitting is reduced and the CV $R^2$ increases to 0.9."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40f76f5d-5786-4f3f-86ac-0a205d563586",
      "metadata": {
        "id": "40f76f5d-5786-4f3f-86ac-0a205d563586"
      },
      "source": [
        "How does polynomial ridge regression change with increasing number of features?\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/PolynomialRidgeRegressionPerformance.png\" width = \"700\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09aa6d55-a695-48b6-90ed-3e731bab5b2c",
      "metadata": {
        "id": "09aa6d55-a695-48b6-90ed-3e731bab5b2c"
      },
      "source": [
        "### Kernel trick\n",
        "\n",
        "The kernel trick helps us to design more versatile non-linear regression models.\n",
        "\n",
        "We define a **dual representation** $a$ of our model parameter vector $\\textbf{w}$\n",
        "\n",
        "$$\\textbf{w} = \\phi^T a$$\n",
        "\n",
        "The prediction model then becomes\n",
        "\n",
        "\n",
        "$$\\hat{y} = \\phi^T(x) \\textbf{w} = \\phi^T(x) \\phi^T a = \\sum^N_{i=1} \\phi^T(x) + \\phi(x_i) a_i$$\n",
        "\n",
        "We now define a **kernel** $\\kappa$\n",
        "\n",
        "$$\\kappa(x, x_i) = \\phi^T(x)\\phi(x_i)$$\n",
        "\n",
        "The kernel represents a similarity between two feature vectors.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/GaussianKernel.png\" width = \"300\"  align=\"right\">\n",
        "\n",
        "For example, we can use a **gaussian kernel**\n",
        "\n",
        "$$\\kappa(x, x_i) = e^{-\\frac{\\lvert x - x_i \\rvert_2^2}{2\\sigma}}$$\n",
        "\n",
        "$$\\lvert x - x_i \\rvert_2^2 = \\sum_k(x_k- x_{ik})^2$$\n",
        "\n",
        "Note:\n",
        "- The original parameter vector $\\textbf{w}$ vector has $D$ elements (one per each feature)\n",
        "- The dual parameter vector $a$ has $N$ elements (one per each sample)\n",
        "\n",
        "The resulting **dual prediction model** is a linear combination of kernels placed around the feature vectors $x_i$\n",
        "\n",
        "$$\\hat{y} = \\sum^N_{i=1} \\kappa(x, x_i) a_i$$\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/DualPredictionModel.png\" width = \"700\">\n",
        "\n",
        "In this plot we can see three samples with feature vectors $x_1$, $x_2$ and $x_3$. The gaussian kernels are placed around them, multiplied by\n",
        "coefficients ai and then summed up to produce the prediction model. We can also see, that large kernels produce smoother models."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aba9e1d8-5152-4854-bcd2-dfd06141fc32",
      "metadata": {
        "id": "aba9e1d8-5152-4854-bcd2-dfd06141fc32"
      },
      "source": [
        "We will see more details on non-linear regression in Notebook 4.4."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}