{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaralAminpour/ML-BME-Course-UofA-Fall-2023/blob/main/Week-4-Regression-models/4.4-Nonlinear-regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7D2ydMMivpg6"
      },
      "source": [
        "# Non-linear regression\n",
        "\n",
        "### Polynomial Ridge regression\n",
        "\n",
        "Non-linear regression can be achieved by combining a non-linear feature transformation $\\boldsymbol{\\phi}(\\mathbf{x})$ with linear regression models. The prediction model will become $y=\\boldsymbol{\\phi}(\\mathbf{x})^T\\mathbf{w}$. An example is a polynomial feature transformation $\\boldsymbol{\\phi}(x)=(1,x,...,x^M)$.\n",
        "\n",
        "The **Non-linear Ridge regression** is obtained by minimising the loss\n",
        "\n",
        "$$ F(\\mathbf{w})=\\frac{1}{2}\\sum_{i=1}^N(y_i-\\boldsymbol{\\phi}(\\mathbf{x}_i)^T\\mathbf{w})+\\frac{\\lambda}{2} \\mathbf{w}^T\\mathbf{w}$$\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/KRRsigma.gif\" width = \"450\" style=\"float: right;\">\n",
        "\n",
        "### Kernel Ridge Regression\n",
        "\n",
        "The regression model can be alternatively defined using a **dual representation** $a$\n",
        "\n",
        "$$\\hat{y}=\\sum_{i=1}^N\\kappa(\\mathbf{x},\\mathbf{x}_i)a_i$$\n",
        "\n",
        "where $\\kappa(\\mathbf{x},\\mathbf{x}_i)$ is a kernel measuring simmilarity between samples $\\mathbf{x}$ and $\\mathbf{x}_i$, such as Gaussian Kernel. Predictions using Kernel Ridge regression are evaluated as\n",
        "\n",
        "$$\\hat{y}=\\mathbf{k(x)}^T(\\mathbf{K}+\\lambda\\mathbf{I})^{-1}\\mathbf{y}$$\n",
        "\n",
        "where $\\mathbf{k(x)}=(\\kappa(\\mathbf{x},\\mathbf{x}_1),...,\\kappa(\\mathbf{x},\\mathbf{x}_N))^T$ is a vector of similarities of the training samples with the new sample $\\mathbf{x}$ and\n",
        "\n",
        "$$\\mathbf{K}=\\begin{pmatrix}\\kappa(\\mathbf{x}_1,\\mathbf{x}_1)&\\dots&\\kappa(\\mathbf{x}_1,\\mathbf{x}_N)\\\\\\vdots& \\ddots & \\vdots\\\\ \\kappa(\\mathbf{x}_N,\\mathbf{x}_1)&\\dots&\\kappa(\\mathbf{x}_N,\\mathbf{x}_N) \\end{pmatrix}$$\n",
        "\n",
        "is the matrix of pair-wise similarities between training samples. A Kernel Ridge regression model fitted to three datapoints with Gaussian kernel with increasing value of $\\sigma$ is shown on the right.\n",
        "\n",
        "In this notebook we will demonstrate fitting of **Polynomial Ridge** and **Kernel Ridge Regression** to predict the GA from a single feature (volume of cortex) and from volumes of 6 brain structures."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example: Kernel ridge regression\n",
        "\n",
        "In Kernel Ridge Regression, the idea is to extend the capabilities of Ridge Regression by using a kernel to implicitly map the input data to a higher-dimensional feature space. Here's how you could go about fitting a Kernel Ridge Regression model to your provided data.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/KRRsigma.gif\" width = \"450\" style=\"float: right;\">\n",
        "\n",
        "In this movie, we demonstrate a Kernel Ridge Regression model that's been trained on just three samples, while varying the standard deviation ($\\sigma$) of the Gaussian kernel. Here's a breakdown of what you'll notice:\n",
        "\n",
        "- **Small $\\sigma$:** With a tiny standard deviation in the Gaussian kernel, the similarity between different points becomes highly localized. As a result, the model's predictions between samples drop to zero. Essentially, the model becomes too \"narrow\" in its focus, clinging too closely to the training samples. This is classic overfitting: the model is too tuned to the training data and isn't generalizing well to unseen data.\n",
        "\n",
        "- **Large $\\sigma$:** On the flip side, as we crank up the standard deviation, the Gaussian kernel starts treating disparate points as if they're quite similar, effectively smoothing out the differences between them. This leads to a model that's almost too \"easygoing,\" predicting similar outputs for a wide range of inputs. Here, the risk is underfitting—our model becomes too simple to capture the underlying complexity of the data.\n",
        "\n",
        "So, as you'll see in the movie, kernel size matters—a lot. Too small, and your model obsesses over the training data. Too large, and your model becomes a laid-back generalist, failing to pick up the nuances in the data. Finding the \"Goldilocks zone\" for $\\sigma$ (and the regularization parameter $\\lambda$) is crucial for a model that's just right.\n",
        "\n",
        "## Details of the example\n",
        "\n",
        "OK. Let's go deeper to go over the details of the example.\n",
        "\n",
        "\n",
        "### Given Data\n",
        "\n",
        "- Feature matrix $X = [-1, 0, 1]^T$\n",
        "\n",
        "- Target vector $y = [2, 3, 2]^T$\n",
        "\n",
        "### Hyperparameters\n",
        "\n",
        "- Standard deviation of the Gaussian kernel, denoted as $\\sigma$\n",
        "\n",
        "- Regularization parameter for Ridge penalty, denoted as $\\lambda$\n",
        "\n",
        "### Procedure\n",
        "\n",
        "1. Use a Gaussian kernel to compute the similarity between points. The Gaussian kernel is defined as:\n",
        "\n",
        "   $$\n",
        "   K(x_i, x_j) = e^{-\\frac{(x_i - x_j)^2}{2\\sigma^2}}\n",
        "   $$\n",
        "   \n",
        "2. Apply Ridge Regression in the transformed feature space. The optimization problem becomes:\n",
        "\n",
        "   $$\n",
        "   \\min_w \\frac{1}{2} ||Kw - y||^2 + \\frac{\\lambda}{2} w^T w\n",
        "   $$\n",
        "\n",
        "### Observations\n",
        "\n",
        "- **When $\\sigma$ is small and $\\lambda = 0.02$:** The model tends to make some predictions close to zero. This is likely because the kernel values are too sensitive to differences between points, leading to a less general model.\n",
        "  \n",
        "- **When $\\sigma$ is large:** The model ends up underfitting the data. In this case, the kernel values become more uniform, making it hard for the model to capture any complex patterns in the data.\n",
        "\n",
        "So, you'll want to select $\\sigma$ and $\\lambda$ carefully. Typically, you would use something like cross-validation to select these hyperparameters. This way, you could find a good trade-off between fitting the training data well while also generalizing to new, unseen data.\n",
        "\n"
      ],
      "metadata": {
        "id": "NIZl1VSIxiw1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example: Change Ridge regularization parameter ($\\lambda$)\n",
        "\n",
        "In the case of Kernel Ridge Regression, both the standard deviation of the Gaussian kernel ($\\sigma$) and the Ridge regularization parameter ($\\lambda$) play pivotal roles in model performance. When you're watching the movie that demonstrates these concepts, here's what to look out for as we adjust $\\lambda$ while keeping $\\sigma$ constant at 1:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-4-Regression-models/imgs/landa.png\" width = \"450\" style=\"float: right;\">\n",
        "\n",
        "\n",
        "- **Large $\\lambda$**: With a high regularization parameter, you'll notice the model's predictions gradually approaching zero. This isn't just a coincidence. When $\\lambda$ is large, the Ridge penalty becomes dominant, and the model essentially aims to make the weights as small as possible to minimize the loss function.\n",
        "\n",
        "- **Kernel Ridge Penalizes Intercept**:\n",
        "\n",
        "Unlike some other implementations where the intercept term is left alone, in Kernel Ridge Regression, the regularization term penalizes the intercept as well. This is significant because it adds another layer of constraint, pulling the intercept—and subsequently the predictions—toward zero when $\\lambda$ is large.\n",
        "\n",
        "So, the challenge here is to strike the right balance. You want to choose a $\\lambda$ that's neither too large (which would drive your predictions toward zero) nor too small (which would risk overfitting). It's another layer of complexity to consider as you try to optimize your model's performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "r9Va709f1yq-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example: Linear Kernel\n",
        "\n",
        "Indeed, the Linear Kernel plays a special role in the context of Kernel Ridge Regression. The feature transformation for a linear kernel is given by:\n",
        "\n",
        "$$\n",
        "\\phi(x) = [1, x^T]\n",
        "$$\n",
        "\n",
        "When you calculate the linear kernel $ k(x, x') $ using this transformation, it turns out to be:\n",
        "\n",
        "$$\n",
        "k(x, x') = \\phi(x)^T \\phi(x') = (1, x)^T (1, x') = 1 + x x'\n",
        "$$\n",
        "\n",
        "The fascinating part is that when you apply Kernel Ridge Regression with this particular linear kernel, you essentially revert back to good old Linear Ridge Regression! This offers an interesting bridge between these two methods, showcasing that Kernel Ridge Regression with a linear kernel is just a specialized form of Linear Ridge Regression.\n",
        "\n",
        "So, if you're dealing with linearly separable data or you want to simplify your model to reduce computational cost, utilizing the linear kernel can be a solid strategy. It allows you to take advantage of Ridge regularization while sticking to the basics of linear regression.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-4-Regression-models/imgs/kernel1.png\" width = \"200\" style=\"float: left;\">\n",
        "\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-4-Regression-models/imgs/kernel2.png\" width = \"200\" style=\"float: center;\">\n",
        "\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-4-Regression-models/imgs/kernel3.png\" width = \"200\" style=\"float: right;\">\n",
        "\n",
        "\n",
        "Exactly, you've summarized it well. If the feature transformation $ \\phi(x) $ is $ [1, x] $, then you can easily compute the kernel \\( \\kappa(x, x') \\) by taking the dot product of $ \\phi(x) $ and $ \\phi(x') $. In mathematical terms:\n",
        "\n",
        "$$\n",
        "\\kappa(x, x') = \\phi(x)^T \\phi(x') = [1, x^T] [1, x']^T = 1 + x^T x' = 1 + x \\cdot x'\n",
        "$$\n",
        "\n",
        "By using this linear kernel in the framework of Kernel Ridge Regression, you effectively end up performing Linear Ridge Regression. It's an elegant way to understand how kernel methods can generalize linear methods!"
      ],
      "metadata": {
        "id": "FeBrzQP62K0V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example: Polynomial Kernel\n",
        "\n",
        "When using a polynomial feature transformation of the second degree, $ \\phi(x) $ becomes $ [1, x, x^2] $. This allows us to calculate the quadratic kernel $ \\kappa(x, x') $ between two vectors $ x $ and $ x' $ as follows:\n",
        "\n",
        "$$\n",
        "\\kappa(x, x') = \\phi(x)^T \\phi(x') = [1, x, x^2] [1, x', x'^2]^T = 1 + x x' + (x^2) (x'^2)\n",
        "$$\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-4-Regression-models/imgs/3_Regression_lecture_notes.jpg\" width = \"400\" style=\"float: left;\">\n",
        "\n",
        "\n",
        "\n",
        "In this case, the Kernel Ridge Regression using this polynomial kernel effectively translates into Polynomial Ridge Regression. This showcases the flexibility and extensibility of using kernel methods to model various types of relationships in the data.\n",
        "\n",
        "\n",
        "Similarly, we can calculate polynomial kernel. You can extend the same line of thinking to compute the polynomial kernel. For a second-degree polynomial feature transformation, the transformed feature $x$ is represented as the vector $[1, x, x^2]$. Consequently, the polynomial kernel of the second degree can be expressed as:\n",
        "\n",
        "$$\n",
        "1 + x \\times x' + x^2 \\times (x')^2\n",
        "$$\n",
        "\n",
        "In this case, using this polynomial kernel in Kernel Ridge Regression is essentially the same as executing Polynomial Ridge Regression. This just further demonstrates the versatility of kernel methods in modeling different types of data relationships."
      ],
      "metadata": {
        "id": "JxkxdD-P6jOw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Kernel Ridge Regression\n",
        "\n",
        "The prediction model, as we know, can be expressed as a linear combination of kernels. In matrix notation, this can be represented as $k(x)^T \\times a$, where $k(x)$ is a vector that stores the similarities between a new feature vector and all feature vectors in the training set.\n",
        "\n",
        "To assess the model's performance, it's necessary to calculate these similarities for each new sample in relation to all samples in the training set. While this makes the model highly adaptable and flexible, it also comes with a downside: if the number of samples is large, evaluating the model could become computationally expensive and slow. In such scenarios, it might be more efficient to revert to the original parameterization using $w$.\n",
        "\n",
        "**Note:**\n",
        "\n",
        "- **Evaluation Method:** To compute the model equation, we find the similarities between a new sample and every sample in the training set. This makes the model non-parametric.\n",
        "\n",
        "- **Advantage:** The model is highly flexible and can adapt well to the data. This adaptability makes it well-suited for complex and nuanced data patterns.\n",
        "\n",
        "- **Disadvantage:** The model's computational efficiency takes a hit when the number of samples, $N$, is much larger than the number of features, $D$. In such cases, the model becomes slower to evaluate compared to traditional models that are parameterized by $w$."
      ],
      "metadata": {
        "id": "_irV5QRqwzaQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Kernel Ridge Regression in Scikit learn\n",
        "\n",
        "When it comes to using Scikit-learn for Kernel Ridge Regression, here's what you need to know:\n",
        "\n",
        "The Kernel Ridge Regression is implemented in the KernelRidge object.\n",
        "\n",
        "The kernel parameter specifies the type of kernel you want to use (e.g., linear, polynomial, Gaussian, etc.)\n",
        "\n",
        "The **gamma parameter** sets the (inverse) width of the kernel, defined as $\\gamma = \\frac{1}{2\\sigma}$.\n",
        "\n",
        "The **alpha parameter** controls the strength of the Ridge penalty.\n",
        "\n",
        "In summary, Scikit-learn provides a convenient and efficient way to work with Kernel Ridge Regression, giving you the flexibility to customize it according to your needs.\n",
        "\n",
        "Kernel Ridge Regression in\n",
        "Scikit learn is implemented in object `KernelRidge` . We set the type of kernel through parameter kernel. `rbf` stands for radial basis function and is implemented by a Gaussian kernel.\n",
        "\n",
        "Parameter gamma set the inverse width of the kernel, so small gamma means large kernel and other way round. Parameter alpha sets the strength of ridge penalty. The model can be tuned using GridSearchCV as usual."
      ],
      "metadata": {
        "id": "LrP5_xvY8UIy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6BcAgBmvpg7"
      },
      "source": [
        "### Load data\n",
        "\n",
        "The code below imports the essential libraries and loads the dataset with 6 brain volumes and creates feature matrix `X` and target vector `y`. Run the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "shqn23_Pvpg7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import requests\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.kernel_ridge import KernelRidge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tAvY4m5Vvpg8"
      },
      "outputs": [],
      "source": [
        "# This code will download the required data files from GitHub\n",
        "def download_data(source, dest):\n",
        "    base_url = 'https://raw.githubusercontent.com/'\n",
        "    owner = 'SirTurtle'\n",
        "    repo = 'ML-BME-UofA-data'\n",
        "    branch = 'main'\n",
        "    url = '{}/{}/{}/{}/{}'.format(base_url, owner, repo, branch, source)\n",
        "    r = requests.get(url)\n",
        "    f = open(dest, 'wb')\n",
        "    f.write(r.content)\n",
        "    f.close()\n",
        "\n",
        "# Create the temp directory, if it doesn't already exist\n",
        "import os\n",
        "if not os.path.exists('temp'):\n",
        "   os.makedirs('temp')\n",
        "\n",
        "download_data('Week-4-Regression-models/data/GA-brain-volumes-1-feature.csv', 'temp/GA-brain-volumes-1-feature.csv')\n",
        "download_data('Week-4-Regression-models/data/GA-brain-volumes-6-features.csv', 'temp/GA-brain-volumes-6-features.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6iFPzFLzvpg8"
      },
      "outputs": [],
      "source": [
        "def CreateFeaturesTargets(filename):\n",
        "\n",
        "    df = pd.read_csv(filename,header=None)\n",
        "\n",
        "    # convert from 'DataFrame' to numpy array\n",
        "    data = df.values\n",
        "\n",
        "    # Features are in columns one to end\n",
        "    X = data[:,1:]\n",
        "\n",
        "    # Scale features\n",
        "    X = StandardScaler().fit_transform(X)\n",
        "\n",
        "    # Labels are in the column zero\n",
        "    y = data[:,0]\n",
        "\n",
        "    # return Features and Labels\n",
        "    return X, y\n",
        "\n",
        "X,y = CreateFeaturesTargets('temp/GA-brain-volumes-6-features.csv')\n",
        "\n",
        "print('Number of samples is', X.shape[0])\n",
        "print('Number of features is', X.shape[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoBGXncdvpg9"
      },
      "source": [
        "## Univariate non-linear regression\n",
        "\n",
        "We will explore univariate non-linear regression to understand behaviour of Polynomial Ridge and Gaussian Kernel Ridge regression models. We predict age at scan from the volume of cortex, the first feature in six brain tissue dataset.\n",
        "\n",
        "### Create univariate dataset\n",
        "\n",
        "First we extract the cortical volumes. Run the code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AY6iNrwDvpg9"
      },
      "outputs": [],
      "source": [
        "# Extract volume of cortex\n",
        "X_cortex = X[:,0].reshape(-1,1)\n",
        "\n",
        "# Print dimensions\n",
        "print('Number of samples is', X_cortex.shape[0])\n",
        "print('Number of features is', X_cortex.shape[1])\n",
        "\n",
        "# Plot the dataset\n",
        "plt.scatter(X_cortex, y)\n",
        "plt.xlabel('Cortical Volume')\n",
        "plt.ylabel('Age at scan (weeks)')\n",
        "plt.title('Univariate dataset')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofH9J9ELvpg-"
      },
      "source": [
        "## Univariate regression model\n",
        "\n",
        "In the next cell you are given functions to calculate cross-validated RMSE and plot a univariate regression model. Look at them and run the cell, you will need this functions later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "RAwABtxZvpg-"
      },
      "outputs": [],
      "source": [
        "def RMSE_CV(model,X,y):\n",
        "    scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')\n",
        "    print('Average cross-validated RMSE: {} weeks '.format(round(np.sqrt(-scores.mean()),2)))\n",
        "\n",
        "def PlotRegressionCurve(model, X, y):\n",
        "    # Plot the data\n",
        "    plt.scatter(X, y)\n",
        "    plt.xlabel('Volume')\n",
        "    plt.ylabel('GA')\n",
        "\n",
        "    # Plot the model\n",
        "    x = np.linspace(X.min(),X.max(),101)\n",
        "    x = x.reshape(-1, 1)\n",
        "    y = model.predict(x)\n",
        "    plt.plot(x, y, 'r-')\n",
        "    plt.ylim([25,46])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSRHMXcDvpg-"
      },
      "source": [
        "### Polynomial Ridge Regression\n",
        "\n",
        "This code fits, plots and evaluates the polynomial ridge regression model. Note that:\n",
        "\n",
        "* The model is a `Pipeline` of `PolynomialFeatures`, `StandardScaler` and `Ridge`.\n",
        "* Standard scaler normalises the features after the polynomial transformation and this improves the performance of Ridge regression.\n",
        "* We exclude the feature 1 from `PolynomialFeatures`, because `Ridge` will create an intercept that is not penalised. This also improves the performance of the model.\n",
        "\n",
        "__Questions:__ Play with the parameters `degree` and `alpha` to see the effect on the curve and performance. Answer the following questions:\n",
        "\n",
        "* Can you find a setting with the lowest error?\n",
        "* Set polynomial `degree` to 10 and `alpha` to zero. Is the model overfitted?\n",
        "* Find the parameter `alpha` to reduce overfitting for `degree` 10.\n",
        "\n",
        "__Answers:__\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtEeXr8Bvpg_"
      },
      "outputs": [],
      "source": [
        "# Create model\n",
        "model = Pipeline((\n",
        "(\"poly_features\", PolynomialFeatures(degree=2, include_bias=False)),\n",
        "(\"scaler\", StandardScaler()),\n",
        "(\"ridge\", Ridge(alpha=0)),))\n",
        "\n",
        "# Fit model\n",
        "model.fit(X_cortex,y)\n",
        "\n",
        "# Evaluate model\n",
        "RMSE_CV(model,X_cortex,y)\n",
        "\n",
        "# Plot model\n",
        "PlotRegressionCurve(model, X_cortex, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWiA6sFzvpg_"
      },
      "source": [
        "The code below performs the grid search to find the best parameters for the Polynomial Ridge Regression model that we defined above. Run it to find the best fit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Xp8ni4zQvpg_",
        "outputId": "034cd834-8480-404c-8188-231e63c19b99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average cross-validated RMSE: 1.16 weeks \n",
            "Best degree:  10\n",
            "Best alpha:  0.305\n"
          ]
        }
      ],
      "source": [
        "# Define parameter grid\n",
        "parameters = {\"poly_features__degree\": range(1,15),\n",
        "             \"ridge__alpha\":np.logspace(-3,3,100)}\n",
        "\n",
        "# Perform grid search\n",
        "grid_search = GridSearchCV(model, parameters, cv=5)\n",
        "grid_search.fit(X_cortex, y)\n",
        "\n",
        "# Calculate best CV RMSE\n",
        "RMSE_CV(grid_search.best_estimator_, X_cortex, y)\n",
        "\n",
        "# Print best parameters\n",
        "print('Best degree: ', grid_search.best_estimator_.named_steps[\"poly_features\"].degree)\n",
        "print('Best alpha: ', round(grid_search.best_estimator_.named_steps[\"ridge\"].alpha,3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0YByzJqvpg_"
      },
      "source": [
        "### Kernel Ridge Regression\n",
        "\n",
        "This code fits, plots and evaluates the kernel ridge regression model [`KernelRidge`](https://scikit-learn.org/stable/modules/generated/sklearn.kernel_ridge.KernelRidge.html). Note that:\n",
        "\n",
        "* The kernel has been set to Gaussian using parameter `kernel='rbf'`\n",
        "* Parameter `gamma` represents $\\frac{1}{2\\sigma}$, where $\\sigma$ is the standard deviation of the Gaussian kernel. Note that small values of `gamma` correspond to a large kernel and other way round.\n",
        "* Parameter `alpha` determines strengths of the regularisation.\n",
        "\n",
        "__Questions:__ Play with the parameters `degree` and `alpha` to see the effect on the curve and performance. Answer the following questions:\n",
        "\n",
        "* Keep `alpha` fixed to `1e-5` and change values of `gamma` to see the effect of the kernel size on the curve. You can for example try settings `1e-5`, `1e-3`, `1e-1`, `1e1` and `1e3`. Which setting performs the best?\n",
        "* Set `gamma=1` while keeping `alpha=1e-5`. Is the model overfitted?\n",
        "* Find the parameter `alpha` to reduce overfitting for `gamma=1e1`.\n",
        "\n",
        "__Answers:__\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hwkNGcyvphA"
      },
      "outputs": [],
      "source": [
        "# Create model\n",
        "model = KernelRidge(kernel='rbf', gamma=1e0, alpha=1e-5)\n",
        "\n",
        "# Fit model\n",
        "model.fit(X_cortex,y)\n",
        "\n",
        "# Evaluate model\n",
        "RMSE_CV(model, X_cortex,y)\n",
        "\n",
        "# Plot model\n",
        "PlotRegressionCurve(model, X_cortex, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90bFEA_pvphA"
      },
      "source": [
        "The code below performs the grid search to find the best parameters for the Kernel Ridge Regression model that we defined above. Run it to find the best fit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Db6LLLY_vphA",
        "outputId": "a6a8e42c-c95d-4e59-bb43-703ea291e62a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average cross-validated RMSE: 1.14 weeks \n",
            "Gamma: 1.0 Alpha: 0.01\n"
          ]
        }
      ],
      "source": [
        "# Define parameter grid\n",
        "parameters = {\"alpha\": np.logspace(-5, 5, num=11), # You can increase the value of num, but it will slow things down\n",
        "              \"gamma\": np.logspace(-5, 5, num=11)}\n",
        "\n",
        "# Perform grid search\n",
        "grid_search = GridSearchCV(model, parameters, cv=5)\n",
        "grid_search.fit(X_cortex, y)\n",
        "\n",
        "# Calculate best CV RMSE\n",
        "RMSE_CV(grid_search.best_estimator_, X_cortex, y)\n",
        "\n",
        "# Print best parameters\n",
        "print('Gamma: {} Alpha: {}'.format(round(grid_search.best_estimator_.gamma,3), round(grid_search.best_estimator_.alpha,3)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oS3BJ-uvphA"
      },
      "source": [
        "## Exercise\n",
        "\n",
        "In this exercise we will fit multivariate non-linear regression model to the dataset with volumes of 6 brain tissues, calculate cross-validater RMSE and check the bias error in the model. We will compare three models:\n",
        "\n",
        "* Linear Ridge Regression\n",
        "* Polynomial Ridge Regression\n",
        "* Kernel Ridge Regression with Gaussian Kernel\n",
        "\n",
        "\n",
        "Answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When diving into a multivariate non-linear regression example, let's say we're looking at predicting gestational age (GA) from the volumes of six different brain tissues. Here are some important considerations:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-4-Regression-models/imgs/3_Regression_lecture_notes_2.jpg\" width = \"700\" style=\"float: left;\">\n",
        "\n",
        "\n",
        "\n",
        "- A useful way to evaluate the model's performance is by plotting the expected versus predicted target values. This helps us to identify any bias error in the model.\n",
        "  \n",
        "- In the case of Linear Ridge Regression, you might notice that there's a bias in the predictions for younger babies. Essentially, the model may not be capturing the complexities associated with lower gestational ages.\n",
        "\n",
        "- Switching to non-linear models like Kernel Ridge Regression can help remove this bias and improve the root mean square error of cross-validation (RMSE CV), offering a more accurate and nuanced representation of the data.\n",
        "\n",
        "- After various tests, you might find that the best-performing model for this particular application is Kernel Ridge Regression. This model allows you to capture the non-linear relationships in the data more effectively, thus improving overall prediction quality.\n",
        "\n",
        "\n",
        "Revisiting the example of predicting age at the time of a scan based on the volumes of six different brain tissues provides some insightful observations:\n",
        "\n",
        "- In multivariate regression problems like this, visualizing the model directly can be challenging. However, plotting the expected versus predicted target values is a great alternative. This helps us identify any bias errors.\n",
        "\n",
        "- Linear Ridge Regression appears to have a bias when it comes to younger babies, as it tends to predict higher ages than what are actually observed. This is a critical issue if accurate age prediction is crucial for your study or application.\n",
        "\n",
        "- The good news is that non-linear models like Polynomial Ridge and Kernel Ridge can mitigate this bias. This improvement is quantifiable: The cross-validation root mean square error (CV RMSE) drops from 1.27 weeks with Linear Ridge to 0.83 weeks with Polynomial Ridge, and even further to 0.77 weeks with Kernel Ridge.\n",
        "\n",
        "- Among all models, Kernel Ridge Regression with six features emerges as the top performer, achieving the lowest CV RMSE.\n",
        "\n",
        "So, it's clear that for this specific application, Kernel Ridge Regression offers the most accurate and least biased estimates, demonstrating the power and flexibility of non-linear models in capturing complex relationships in the data.\n",
        "\n",
        "In a nutshell, non-linear models like Kernel Ridge Regression can be more adept at capturing complex relationships, making them a strong candidate for tasks where linear models show noticeable bias or limitations.\n",
        "\n"
      ],
      "metadata": {
        "id": "HMtJhAy_IdIn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcnlYF-KvphA"
      },
      "source": [
        "### Task: Linear Ridge Regression\n",
        "\n",
        "The code below tunes Linear Ridge Regression model to the dataset with 6 features, with feature matrix `X` and target vector `y`. Run the code and note the performance. The tuned model is saved in `model_lin`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_EnH3LaJvphA",
        "outputId": "a04bc4da-b0ae-4381-db52-000652c09fae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Ridge Regression:\n",
            "Best alpha = 0.1\n",
            "Average cross-validated RMSE: 1.27 weeks \n"
          ]
        }
      ],
      "source": [
        "print('Linear Ridge Regression:')\n",
        "\n",
        "# grid for hyperparameter alpha\n",
        "parameters = {\"alpha\": np.logspace(-3,3,7)}\n",
        "\n",
        "# create ridge model\n",
        "model = Ridge()\n",
        "\n",
        "# perform grid search\n",
        "grid_search = GridSearchCV(model, parameters,cv=5)\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# remember optimised model\n",
        "model_lin = grid_search.best_estimator_\n",
        "\n",
        "# Print optimal alpha\n",
        "print('Best alpha =', round(model_lin.alpha,3))\n",
        "\n",
        "# Calculate RMSE_CV\n",
        "rmse_cv = RMSE_CV(model_lin,X,y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcznuMefvphB"
      },
      "source": [
        "Your task is now to plot expected vs predicted target values to see whether there is the bias in the linear model. Complete the function `PlotTargets` to do that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YN6BrQ3yvphB"
      },
      "outputs": [],
      "source": [
        "def PlotTargets(model,X,y):\n",
        "\n",
        "    # Predict targets\n",
        "    y_pred = None # Edit this line\n",
        "\n",
        "    # Plot expected targets on x axis and predicted targets on y axis\n",
        "    #plt.plot(None, None, 'o', label='Target values') # Edit this line\n",
        "    plt.plot([27,45], [27,45], 'r', label = '$y=\\hat{y}$')\n",
        "    plt.xlabel('Expected target values')\n",
        "    plt.ylabel('Predicted target values')\n",
        "    plt.legend()\n",
        "\n",
        "#PlotTargets(model_lin, X, y)\n",
        "plt.title('Linear Ridge Regression')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-Cq5eWivphB"
      },
      "source": [
        "**Question:** Does the plot show bias error?\n",
        "\n",
        "**Answer:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caf1DC7AvphB"
      },
      "source": [
        "### Task: Polynomial Ridge Regression\n",
        "\n",
        "Next, you will tune the polynomial ridge regression, measure its performance and plot the target values to see whether there is still bias.\n",
        "\n",
        "Complete the code before to tune the model. Note that it is saved in `model_poly`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "bON4UArzvphB"
      },
      "outputs": [],
      "source": [
        "# create model\n",
        "model = Pipeline((\n",
        "(\"poly_features\", PolynomialFeatures(include_bias=False)),\n",
        "(\"scaler\", StandardScaler()),\n",
        "(\"ridge\", Ridge())))\n",
        "\n",
        "# define parameter grid\n",
        "parameters = {\"poly_features__degree\": range(1,5),\n",
        "             \"ridge__alpha\":np.logspace(-3,3,7)}\n",
        "\n",
        "# perform grid search\n",
        "#grid_search = None\n",
        "\n",
        "# remember optimised model\n",
        "#model_poly = grid_search.best_estimator_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "In954n1-vphB"
      },
      "source": [
        "Complete the code below to print the optimal parameters and evaluate performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWUssmjJvphB",
        "outputId": "56d366c4-7671-452d-bd0a-0bcce972b50d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Polynomial Ridge Regression:\n",
            "Best degree: None\n",
            "Best alpha: None\n"
          ]
        }
      ],
      "source": [
        "print('Polynomial Ridge Regression:')\n",
        "\n",
        "# print optimal parameters\n",
        "print('Best degree:', None)\n",
        "print('Best alpha:', None)\n",
        "# Remember from Notebook 2.4 you can use the named_steps method of your Pipeline to access the variables of each step\n",
        "\n",
        "# Calculate CV RMSE\n",
        "#RMSE_CV(model_poly, X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjxI9RWkvphC"
      },
      "source": [
        "**Question:** Is the performance better than for Linear Ridge?\n",
        "\n",
        "**Answer:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RplukIk4vphC"
      },
      "source": [
        "Plot the expected vs predicted target values using the function `PlotTargets` that you created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HulBOJYOvphC"
      },
      "outputs": [],
      "source": [
        "# Plot\n",
        "# Add your code here\n",
        "plt.title('Polynomial Ridge Regression')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EUWoHJAvphC"
      },
      "source": [
        "**Question:** Is there less bias than for the linear model?\n",
        "\n",
        "**Answer:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2XNMKv1vphC"
      },
      "source": [
        "### Task: Kernel Ridge Regression\n",
        "\n",
        "Finally, you will tune the Gaussian Kernel Ridge Regression, measure its performance and plot the target values to see whether the bias error is further reduced.\n",
        "\n",
        "Complete the code before to tune the model. Note that it is saved in `model_kernel`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "MZP4_St0vphC"
      },
      "outputs": [],
      "source": [
        "# Create model\n",
        "model = None\n",
        "\n",
        "# Define parameter grid\n",
        "parameters = {\"alpha\": np.logspace(-5, 5, num=3), # You can try increasing num, but it will make the program slower!\n",
        "              \"gamma\": np.logspace(-5, 5, num=3)}\n",
        "\n",
        "# Perform grid search\n",
        "grid_search = None\n",
        "\n",
        "# Remember optimised model\n",
        "model_kernel = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8ZoKCQ4vphC"
      },
      "source": [
        "Complete the code bellow to print the optimal parameters and evaluate performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Ph6558CEvphC",
        "outputId": "9b5183fc-0aee-4bb3-c977-4b1132deb348",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kernel Ridge Regression:\n"
          ]
        }
      ],
      "source": [
        "print('Kernel Ridge Regression:')\n",
        "\n",
        "# print optimal parameters\n",
        "#print('Best gamma: ', round(None,3))\n",
        "#print('Best alpha: ', round(None,5))\n",
        "\n",
        "# Calculate CV RMSE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4tKsnjevphC"
      },
      "source": [
        "**Question:** Is the performance better than for Polynomial Ridge?\n",
        "\n",
        "**Answer:** The CV RMSE decreased from 0.84 weeks to 0.77 weeks, so the performance is better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stmr3yVMvphC"
      },
      "source": [
        "Plot the expected vs predicted target values using the function `PlotTargets` that you created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwm9c6fVvphC"
      },
      "outputs": [],
      "source": [
        "# Plot\n",
        "plt.title('Kernel Ridge Regression')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ij7Yf3jQvphC"
      },
      "source": [
        "**Question:** Is there less bias than for the polynomial model?\n",
        "\n",
        "**Answer:**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}