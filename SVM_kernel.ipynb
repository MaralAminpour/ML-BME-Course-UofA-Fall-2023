{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNbRSLdwOr6A+CzlnDr+q+n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaralAminpour/ML-BME-Course-UofA-Fall-2023/blob/main/SVM_kernel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. I support vector machines and so should you**\n",
        "\n",
        "\"**Alright, what’s a support vector machine good for?**\n",
        "\n",
        "They are a type of machine learning that allows a computer to take a set of data and **classify it into two groups** (or more by comparing any one group to all the rest). They can help us figure out all sorts of things, like looking at someone's financial details to predict if they might skip out on paying back a loan. Or even helping a computer tell the difference between a picture of a dog and a cat. **They have a benefit of maximizing the margin (or space) between the two groups** to allow for new observation that the computer has not seen before to be better classified. They also can **take advantage of something called the kernel trick** which I will explain shortly.\n",
        "\n",
        "Basically the computer uses lots of math that smart people on youtube can explain to you to **draw a line between groups of already labeled data** and then uses the line to predict what class or group **new observations** fall under. If you think about it, it is rather impressive. We can naturally look at the graph below and realize where best to draw a line separating the blue from red X’s, but **there are actually a lot of different possibilities** and **finding smart ways to get a computer to recognize the best one** is big business.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-3-Classification-models/imgs/gif1.gif' width=300px>\n",
        "\n",
        "\n",
        "Mathematically the computer draws a line between the data, moves other lines away from it in both directions, and rotates **these until it hits observations on either side.**\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-3-Classification-models/imgs/gif2.gif' width=300px>\n",
        "\n",
        "\n",
        "It gets its name from **these vectors that are ultimately supporting the model**. These vectors are also the only important information the computer needs to hold onto as all other points have no affect on the model or predictions.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-3-Classification-models/imgs/gif3.gif' width=300px>\n",
        "\n",
        "All this seems pretty simple right? It is! Though technically **we have only built a Maximum Margin Classifier**. If we are classifying groups of things that have either **some overlap or extreme outliers this method will fail**. Imagine you are trying to tell the difference between cats and dogs and someone throws a **chihuahua** into the mix! Or, god forbid, a shih tzu. No computer could tell those apart, right? Wrong. We just tell our ignore them! If only people were so easily to program…\n",
        "\n",
        "\n",
        "In the below GIF, you can first see a mixed group where a computer could not mathematically draw a line between them. **By ignoring two points** it is able to find the best support vectors and draw our line.\n",
        "\n",
        "In the second half of the GIF, **one extreme outlier** (the infamous shih tzu) would otherwise **PUSH our line far too close to the red class** and cause our model to misclassify future predictions. **Teaching a computer** that some observations simply do not fit what we normally expect is very important in model building and prevents overfitting to whatever training data we give it.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-3-Classification-models/imgs/gif4.gif' width=300px>\n",
        "\n",
        "\n",
        "OK. Now all these visualizations have shown 2D representations of data. If we are classifying dogs and cats, it would be as if we are only looking at two pieces of information about them. Maybe the **x-axis represents their weight** and the y-axis represents, ya know, innate evil or something. All of this works in 3 dimensions and beyond. Instead of lines we are drawing planes (or hyperplanes in 4D and beyond), but everything still works out just the same. Look, here is a picture of the same magic in 3D:\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-3-Classification-models/imgs/gif5.webp' width=300px>\n",
        "\n",
        "So far so good, yeah? **Now we get into the real magic: the kernel trick.** So, I am sure you can imagine lots of different datasets or graphs were you **simply cannot draw a line/plane/whatever** through them. **A cluster of red surrounded by a sea of blue** would be impossible to draw a line through. Math comes to the rescue again. In the below example we have **1D data** and no matter where we draw a line, we won't be able to divide our groups up. **The kernel trick in this example is just adding another dimension with the square of the first dimension.** If we are just talking about the weight of cats and dogs, it is like we **decide also to tell the computer the square of their weight too**. To us, that makes no sense, but watch.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-3-Classification-models/imgs/gif6.gif' width=300px>\n",
        "\n",
        "Holy crap, am I right? Now watch the same thing from 2D to 3D!\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-3-Classification-models/imgs/gif7.gif' width=300px>\n",
        "\n",
        "\n",
        "That was just a super simple kernel trick. People have been coming up with some fancy ones for support vector machines for a while now and that allows them to tackle some pretty complex stuff. AND if you are still having trouble after adding one dimension, **there is nothing saying you can't add more!** Crazy.\n",
        "If you want to learn more about kernels and some of the more effective ones, check out this article written by Eric Kim. It pretty approachable as those things go.\n",
        "And if you are interested in the math behind the other aspects of support vector machines, I recommend these two videos on youtube:\n",
        "\n",
        "Udi Aharoni:\n",
        "\n",
        "https://www.youtube.com/watch?v=3liCbRZPrZA\n",
        "\n",
        "Temel Bilisim:\n",
        "\n",
        "https://www.youtube.com/watch?v=5zRmhOUjjGY\n",
        "\n",
        "I hope that helped to demystify support vector machines and teach you the basics of what is going on under the hood when you start modeling with them.\"\n",
        "\n",
        "[Source](https://towardsdatascience.com/i-support-vector-machines-and-so-should-you-7af122b6748)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fkR0k95rJwBi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A very good resource about the formulation of SVM\n",
        "\n",
        " [Machine Learning Lecture 14 \"(Linear) Support Vector Machines\" -Cornell CS4780 SP17](https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote09.html)\n",
        "\n",
        "\n",
        "The Support Vector Machine (SVM) is a linear classifier that can be viewed as an extension of the Perceptron developed by Rosenblatt in 1958.  While the Perceptron is great for finding a \"line\" (more formally, a hyperplane) that separates your data into two categories—if such a line exists—the SVM goes a step further. It finds a hyperplane that not only separates the data but does so in a way that maximizes the distance, or \"margin,\" between the closest points of each class.\n",
        "\n",
        "For the math enthusiasts among us, we define our linear classifier with this equation: $ h(x) = \\text{sign}(w^T x + b) $. We're focusing on binary classification here, so the labels for our data points are either ( +1 \\) or \\( -1 ).\n",
        "\n",
        "- ( +1 \\) if $ w^T x + b > 0 $\n",
        "- ( 0 \\) if $ w^T x + b = 0 $ (though this is often ignored or treated as a special case)\n",
        "- ( -1 \\) if $ w^T x + b < 0 $\n",
        "\n",
        "\n",
        "Typically, if a data set is linearly separable, there are infinitely many separating hyperplanes. A natural question to ask is:\n",
        "\n",
        "**Question: What is the best separating hyperplane? **\n",
        "\n",
        "**SVM Answer:** The one that maximizes the distance to the closest data points from both classes. We say it is the hyperplane with maximum margin.\n"
      ],
      "metadata": {
        "id": "undIz2di0IQw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Margin and hyperplanes in the context of SVM\n",
        "\n",
        "We use \\( w \\) and \\( b \\) to define a hyperplane \\( H \\) through the equation set \\( H = \\{ x | w^T x + b = 0 \\} \\). The margin \\( \\gamma \\) is the minimum distance from the hyperplane \\( H \\) to the closest point, considering both classes.\n",
        "\n",
        "### Distance of a Point to the Hyperplane\n",
        "\n",
        "Suppose you have a point \\( x \\). Let \\( d \\) be the shortest vector from \\( H \\) to \\( x \\), and \\( x_P \\) be the projection of \\( x \\) onto \\( H \\). Then we can write:\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-3-Classification-models/imgs/projection.png' width=300px>\n",
        "\n",
        "\n",
        "$$\n",
        "x_P = x - d\n",
        "$$\n",
        "\n",
        "\\( d \\) is parallel to \\( w \\), which means \\( d = \\alpha w \\) for some \\( \\alpha \\in \\mathbb{R} \\).\n",
        "\n",
        "\\( x_P \\) belongs to \\( H \\), meaning \\( w^T x_P + b = 0 \\). Thus, we get:\n",
        "\n",
        "$$\n",
        "w^T x_P + b = w^T (x - \\alpha w) + b = 0\n",
        "$$\n",
        "\n",
        "From this, we can solve for \\( \\alpha \\):\n",
        "\n",
        "$$\n",
        "\\alpha = \\frac{w^T x + b}{w^T w}\n",
        "$$\n",
        "\n",
        "### Length of \\( d \\)\n",
        "\n",
        "The length of \\( d \\), denoted as \\( \\| d \\| \\), is given by:\n",
        "\n",
        "$$\n",
        "\\| d \\| = \\sqrt{d^T d} = \\sqrt{\\alpha^2 w^T w} = \\frac{|w^T x + b|}{\\sqrt{w^T w}} = \\frac{|w^T x + b|}{\\| w \\|}\n",
        "$$\n",
        "\n",
        "### Margin of \\( H \\)\n",
        "\n",
        "The margin \\( \\gamma(w, b) \\) is the minimum distance to the hyperplane for all \\( x \\) in dataset \\( D \\):\n",
        "\n",
        "$$\n",
        "\\gamma(w, b) = \\min_{x \\in D} \\frac{|w^T x + b|}{\\| w \\|}\n",
        "$$\n",
        "\n",
        "### Scale Invariance\n",
        "\n",
        "Interestingly, the margin and the hyperplane are scale-invariant. That is:\n",
        "\n",
        "$$\n",
        "\\gamma(\\beta w, \\beta b) = \\gamma(w, b), \\forall \\beta \\neq 0\n",
        "$$\n",
        "\n",
        "This means if \\( \\gamma \\) is maximized, the hyperplane sits exactly in the middle of the two classes, providing the largest buffer (\\( \\gamma \\)) between the classes. This ensures optimal separation, making SVM a powerful tool for classification."
      ],
      "metadata": {
        "id": "0a-zR3QPYPXP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2EtgaCYqZTkV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Unveiling the Magic: The Untold Power of Kernels in Machine Learning and SVM\n",
        "\n",
        "A kernel function offers a practical approach to certain calculations. Instead of performing computations directly in higher-dimensional spaces, using a kernel often proves to be a faster and more efficient choice.\n",
        "\n",
        "### **Mathematical Definition**:\n",
        "\n",
        "The kernel function is given by:\n",
        "$K(x, y) = \\langle f(x), f(y) \\rangle$\n",
        "Where:\n",
        "- $K$ is the kernel function.\n",
        "- $x$ and $y$ are our n-dimensional input vectors.\n",
        "- $f$ is a function that maps these n-dimensional vectors into an m-dimensional space.\n",
        "- $\\langle x, y \\rangle$ represents the dot product of two vectors.\n",
        "Typically, $m$ (the dimensions of the transformed space) is much larger than $n$ (the dimensions of our original inputs).\n",
        "\n",
        "### **Intuition**:\n",
        "\n",
        "To determine $\\langle f(x), f(y) \\rangle$, you might think that you **first need to calculate $f(x)$ and $f(y)$ and then compute their dot product**. This process can be computationally intensive, especially as it involves operations in an **m-dimensional** space, which might be considerably **large**. However, after all the effort in this vast m-dimensional space, the final outcome is just a **scalar**.\n",
        "\n",
        "This **raises a question:** Is it necessary to go through the extensive process in the m-dimensional space just for a single scalar result?\n",
        "\n",
        "**If we utilize an appropriate kernel function, the answer is \"no\".**\n",
        "\n",
        "\n",
        "### **Simple Example with Kernel Trick**:\n",
        "\n",
        "Given two vectors:\n",
        "\n",
        "$$ x = (x_1, x_2, x_3) $$\n",
        "$$ y = (y_1, y_2, y_3) $$\n",
        "\n",
        "Consider the function:\n",
        "\n",
        "$$ f(x) = (x_1x_1, x_1x_2, x_1x_3, x_2x_1, x_2x_2, x_2x_3, x_3x_1, x_3x_2, x_3x_3) $$\n",
        "\n",
        "With this, our kernel is defined as:\n",
        "\n",
        "$$ K(x, y) = \\langle x, y \\rangle^2 $$\n",
        "\n",
        "To elucidate further, let's use specific values for our vectors:\n",
        "\n",
        "$$ x = (1, 2, 3) $$\n",
        "$$ y = (4, 5, 6) $$\n",
        "\n",
        "Using our function $f$, we can derive:\n",
        "\n",
        "$$ f(x) = (1*1, 1*2, 1*3, 2*1, 2*2, 2*3, 3*1, 3*2, 3*3) $$\n",
        "\n",
        "This yields:\n",
        "\n",
        "$$ f(x) = (1, 2, 3, 2, 4, 6, 3, 6, 9) $$\n",
        "\n",
        "Similarly:\n",
        "\n",
        "$$ f(y) = (4*4, 4*5, 4*6, 5*4, 5*5, 5*6, 6*4, 6*5, 6*6) $$\n",
        "\n",
        "Which gives:\n",
        "\n",
        "$$ f(y) = (16, 20, 24, 20, 25, 30, 24, 30, 36) $$\n",
        "\n",
        "Taking the dot product of $f(x)$ and $f(y)$, we have:\n",
        "\n",
        "$$ \\langle f(x), f(y) \\rangle = 16 + 40 + 72 + 40 + 100 + 180 + 72 + 180 + 324 = 1024 $$\n",
        "\n",
        "This computation might seem convoluted, mainly because **the function $f$ transforms our original 3-dimensional vectors into a 9-dimensional space.**\n",
        "\n",
        "Now, observe the efficiency of the kernel trick:\n",
        "Using the kernel, we calculate:\n",
        "\n",
        "$$ K(x, y) = (1*4 + 2*5 + 3*6)^2 = 32^2 = 1024 $$\n",
        "\n",
        "Remarkably, **both methods yield the same result** of 1024. However, employing the kernel trick has simplified our calculations considerably.\n",
        "\n",
        "\n",
        "### **The Additional Sophistication of Kernels**:\n",
        "\n",
        "Kernels are particularly powerful because **they allow us to perform calculations in infinite dimensions**. Sometimes, moving to a higher-dimensional space isn't just computationally taxing—it may also be **impractical or indefinable**. For example, $f(x)$ could map from an $n$-dimensional space to an infinite-dimensional one, which is usually too complex to work with. Here, **kernels offer a remarkable computational shortcut.**\n",
        "\n",
        "### **Relation to Support Vector Machines (SVM)**:\n",
        "\n",
        "**How do kernels integrate into the SVM framework?**\n",
        "\n",
        "The basic SVM decision rule is\n",
        "\n",
        "$$y = w \\phi(x) + b,$$\n",
        "\n",
        "where $w$ represents the weights, **$\\phi(x)$ is the feature vector**, and $b$ is the bias. If $y > 0$, we categorize the data point as belonging to class 1; otherwise, it falls into class 0.\n",
        "\n",
        "The goal in SVM is to find weights and a bias that maximize the margin between the classes. While it is commonly stated that kernels make data linearly separable in SVM, a more nuanced explanation is that **the feature vector $\\phi(x)$ makes the data linearly separable**. The kernel function simply streamlines the calculation, especially when $\\phi$ maps to a high-dimensional space, like\n",
        "\n",
        "$x_1, x_2, x_3, \\ldots, x_{D}^n, x_1^2, x_2^2, \\ldots, x_{D}^2$.\n",
        "\n",
        "### **Kernel as a Measure of Similarity**:\n",
        "\n",
        "When considering SVM and feature vectors, **the kernel definition $\\langle f(x), f(y) \\rangle$ transforms into $\\langle \\phi(x), \\phi(y) \\rangle$.**\n",
        "\n",
        "This inner product effectively measures **how much $\\phi(x)$ projects onto $\\phi(y)$**, or in simpler terms, the extent to which **$x$ and $y$ overlap in their feature space**. This degree of overlap serves as an indicator of their similarity.\n",
        "\n",
        "[Source](https://colab.research.google.com/drive/1tBNGOT-ubxqGswskHLQ36FsSbejeImE7#scrollTo=8x-S-qBwBotf&line=89&uniqifier=1)\n"
      ],
      "metadata": {
        "id": "8x-S-qBwBotf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. Another Stab at the Kernel Trick (Why?)**\n",
        "\n",
        "\"There are a lot of great tutorials on Support Vector Machines, and how to apply them in binary classification. But I wanted to dive deeper specifically into the kernel trick and why it’s used.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-3-Classification-models/imgs/Picture1.png' width=700px>\n",
        "\n",
        "\n",
        "**The Support Vector Machine relies on enlarging the feature space to a dimension that allows the two classes to be separable by a linear hyperplane.**\n",
        "\n",
        "1. The first way this can be achieved, a simpler approach, is to transform your input data. This means to take your input features and square, cube or otherwise transform them to produce new additional features.\n",
        "\n",
        "2. The other method is to use kernels.\n",
        "\n",
        "A kernel is a function that represents the **similarity between two observations in a desired dimension** (mathematically defined by Mercer’s Theorem, have a google). The similarity between two observations with the same coordinates is 1, and tends to 0 as the euclidean distance between them increases.\n",
        "\n",
        "Without getting too mathematical, the calculation required to solve the SVM optimisation problem to find the hyperplane’s position, is the inner product between observations. The output of an **inner product** is a **scalar** meaning **the output we are interested in exists in one dimension**, not whatever crazy dimensional space we visit to separate the data. **The inner product calculation is equivalent to a linear kernel function and represents similarity in a linear space.**\n",
        "\n",
        "The reason this is important, is that it means **the value of the observations in the higher dimension is irrelevant to us**, and we only care about the **inner product result**. This inner product can be calculated by the **two step stage of method one**,\n",
        "\n",
        "1. transforming the input data and then calculating the inner product.\n",
        "\n",
        "2. Or applying a kernel to the input data. The benefit of method two is computational efficiency.\n",
        "\n",
        "The computational efficiency arises from the fact that the kernel calculation is faster than the calculation that would be needed to **transform the input data to the higher dimension** that would be required to linearly separate the data.\n",
        "\n",
        "You can intuitively think about why this would be faster as instead of first needing to perform a data transform and then perform an inner product calculation, **you just apply a kernel function in one step and have the inner product result, voila!**\n",
        "\n",
        "There is little benefit to using the kernel trick when the problem is quite simple and only transforming the input data quadratically or cubicly suffices to allow for the data to be linearly separable.\n",
        "\n",
        "But imagine you had a **radial boundary between the two** classes such as in the below figure. To figure out how to transform the input data to be able to linearly separate the below classes is as mind boggling as it is computationally expensive. This is where kernels (radial/gaussian in this case) really come in to their own, and the similarity computation between observations drastically improves on transforming the input data.\n",
        "\n",
        "This was just a quick story (and my first) about why the kernel trick is widely used in difficult binary classification problems, I hope to follow up with how to use the kernel trick soon regarding parameter tuning etc.\"\n",
        "[\n",
        "Source](https://towardsdatascience.com/another-stab-at-the-kernel-trick-why-f73c70ce98dd)"
      ],
      "metadata": {
        "id": "aRIQYPNmIOGU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The \"kernel trick\" is a technique used in machine learning, especially in the context of support vector machines (SVMs) and other kernelized models, to operate in a high-dimensional feature space without explicitly computing the coordinates of the data in that space. Essentially, it allows algorithms to become more flexible and capable of separating data that isn't linearly separable in its original space.\n",
        "\n",
        "et's break this down with a simple explanation:\n",
        "\n",
        "**Kernel Functions:** These are mathematical functions that take two inputs and output a single number. The function is designed to quantify some form of similarity between the inputs.\n",
        "High-Dimensional Space: Sometimes, data that is not linearly separable in its original space can become separable when it is mapped to a higher-dimensional space.\n",
        "\n",
        "**Computational Efficiency:** Working directly in a high-dimensional space can be computationally intensive because it might require dealing with a very large number of features. The kernel trick helps to avoid this computational burden by working in the original space but using the kernel function to implicitly work in the high-dimensional space.\n",
        "\n",
        "**Support Vector Machines (SVMs):** SVMs are a kind of machine learning model often used for classification tasks. SVMs can use the kernel trick to find the optimal hyperplane in the high-dimensional space, which separates different classes in the data in such a way that the margin between them is maximized.\n",
        "Common Kernels: Some commonly used kernel functions include the linear kernel, polynomial kernel, and radial basis function (RBF) kernel, each having its own way of measuring similarity between data points.\n",
        "\n",
        "For example, imagine you have a dataset of two features that, when plotted on a graph, cannot be separated by a straight line. By using the kernel trick, you might find that if you add another dimension (making the graph 3D), you can now separate the data points perfectly with a plane. This \"trick\" essentially allows the SVM to find complex boundaries between classes without having to perform computationally expensive transformations on the data.\n",
        "\n",
        "Let's focus on the **mathematical aspect** of some common kernel functions:\n",
        "\n",
        "### 1. Linear Kernel\n",
        "The linear kernel is the simplest type of kernel which is essentially the standard dot product in the input space. Given two vectors \\( x \\) and \\( y \\), it is defined as:\n",
        "$$\n",
        "K(x, y) = x^T y\n",
        "$$\n",
        "\n",
        "### 2. Polynomial Kernel\n",
        "The polynomial kernel allows one to classify data that is separable by a polynomial decision boundary. It is computed as:\n",
        "$$\n",
        "K(x, y) = (x^T y + c)^d\n",
        "$$\n",
        "where:\n",
        "- $c$ is a user-defined constant (typically $\\geq 0$)\n",
        "\n",
        "- $d$ is the degree of the polynomial\n",
        "\n",
        "\n",
        "### 3. Radial Basis Function (RBF) or Gaussian Kernel\n",
        "The RBF kernel is a popular choice and can map samples into an infinite-dimensional space, making it a powerful tool for separating non-linear data. It is defined as:\n",
        "$$\n",
        "K(x, y) = \\exp\\left( -\\frac{\\|x - y\\|^2}{2\\sigma^2} \\right)\n",
        "$$\n",
        "or equivalently using parameter $ \\gamma = \\frac{1}{2\\sigma^2} $:\n",
        "$$\n",
        "K(x, y) = \\exp\\left( -\\gamma \\|x - y\\|^2 \\right)\n",
        "$$\n",
        "where:\n",
        "- $ \\sigma^2 $ is the variance (a user-defined parameter)\n",
        "- $ \\|x - y\\|^2 $ is the squared Euclidean distance between $ x $ and $ y $\n",
        "\n",
        "### 4. Sigmoid Kernel\n",
        "The sigmoid kernel is defined as:\n",
        "$$\n",
        "K(x, y) = \\tanh(\\alpha x^T y + c)\n",
        "$$\n",
        "where:\n",
        "- $ \\alpha $ is a scaling parameter\n",
        "- $ c $ is a constant\n",
        "\n",
        "Each kernel has its own characteristics and can be chosen based on the problem at hand. It's important to note that choosing a good kernel and tuning the parameters correctly is essential for building a successful SVM model."
      ],
      "metadata": {
        "id": "a3i9uq0xJSMu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "let's go deeper into the world of Support Vector Machines (SVM) and the concept of a \"hard margin,\" making each point a bit more relatable.\n",
        "\n",
        "**Why Keep Samples Outside the Margins?**:\n",
        "\n",
        "Think of the \"hard margin\" like an invisible force field that's trying to keep samples from two different classes as far apart as possible. The goal here is to find a boundary (or for the math geeks, a 'hyperplane') that perfectly separates, say, cats from dogs, while also giving them as much room as possible to roam around on their respective sides. **That \"room to roam\" is what we call the margin.**\n",
        "\n",
        "**What's the Deal with Label 1?**:\n",
        "\n",
        "If a sample has a label of 1 (let's say it's a cat), **we want our magic boundary to be confident about it.** That means the decision function—the formula that tells us which side of the boundary the sample should be on—should give us a value that's at least 1. In our plot, we'd color this area dark green to show **it's safely in 'cat territory.**'\n",
        "\n",
        "**What About Label -1?:**\n",
        "\n",
        "On the other hand, if the sample is labeled **-1 (which might represent a dog)**, we want **the decision function to be equally sure but in the opposite direction.** It should spit out a value less than or equal to -1. In the plot, this area might be shaded yellow to indicate 'dog territory.'\n",
        "\n",
        "**Multiplying Labels and Decisions:**\n",
        "\n",
        "Now, if we multiply the label by the decision function's output, we should get a value greater than or equal to 1. **This is like a safety check to make sure each sample is not just on the correct side of the boundary but also comfortably away from it.**\n",
        "\n",
        "**Max the Margin:**\n",
        "\n",
        "In simpler terms, we're not just separating cats and dogs; we're also trying to give them the **largest playgrounds possible**. We want to maximize this margin to make our boundary as reliable as possible.\n",
        "\n",
        "**The Curious Case of the Slope:**\n",
        "\n",
        "The **slope is like the tilt of our boundary line** (or hyperplane in more dimensions). Interestingly, the 'steepness' of this slope is tied to a thing we call the weight vector, w. **A steeper slope means a smaller margin**. So we aim to **'flatten' this slope**, which actually means minimizing the length of\n",
        "w. This idea is kinda like the Ridge penalty you might have heard about in statistics, aimed at keeping our model from getting too wacky or extreme.\n",
        "\n",
        "So there you have it—this hard margin idea is all about finding that perfect boundary that not only separates our classes but also does so in the most confident way possible, giving each class as much space as it needs. And all these mathematical gymnastics, like minimizing w, are in pursuit of that ideal playground where cats and dogs can roam freely—on their respective sides, of course!\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rGzNhZPTRMFG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly, let's update the mathematical expressions according to your request.\n",
        "\n",
        "Great, let's unpack the concept of \"soft margin\" in the context of Support Vector Machines (SVM). It's like the more flexible cousin of the hard margin, ready to handle the messiness of real-world data. Here's how:\n",
        "\n",
        "1. **Balancing Act for Margins**:\n",
        "\n",
        "The soft margin objective aims to do two things at once: it wants to maximize the margin (or that 'room to roam' for your classes)** while also being forgiving of a few mistakes or violations. These violations are measured by something called slack variables.**\n",
        "\n",
        "2. **Slack Variables – The Forgiving Friends**:\n",
        "\n",
        "Slack variables are like little allowances given to points that don't play by the hard margin rules. These variables are always **non-negative** and are kept **as small as possible**. They represent **how much we're willing to let a sample misbehave or cross over to the wrong side of the boundary.**\n",
        "\n",
        "3. **Softening the Hard Edges**: Thanks to slack variables, samples that would otherwise violate the hard margin are given a bit of leniency (forgiveness). In essence, they let us **relax the rigid boundaries of the hard margin**, making the model more adaptable to real-world, imperfect data.\n",
        "\n",
        "4. **The Role of Hyperparameter \\(C\\)**: This is where the character $C$ comes into play. It's like the supervisor that has to balance two goals:\n",
        "\n",
        "1. making the margin as wide as possible while\n",
        "\n",
        "2. keeping those slack variables—and therefore the margin violations—as low as possible.\n",
        "\n",
        "*   A larger $C$ will penalize violations more, making the boundary stricter,\n",
        "\n",
        "*   whereas a smaller $C$ will be more lenient, allowing for a wider margin but more violations.\n",
        "\n",
        "The mathy way to put this objective is to minimize:\n",
        "\n",
        "\n",
        "$$\\frac{1}{2} \\mathbf{w}^T \\mathbf{w} + C \\sum_{i} \\zeta_i$$\n",
        "\n",
        "\n",
        "Subject to:\n",
        "\n",
        "$$y_i (\\mathbf{w}^T \\mathbf{x}_i + b) \\geq 1 - \\zeta_i \\quad \\text{and} \\quad \\zeta_i \\geq 0 \\quad \\text{for} \\quad i = 1, \\ldots, N$$\n",
        "\n",
        "Here, $\\zeta_i$ are the slack variables for each sample $i$, and $C$ is the hyperparameter that controls the trade-off.\n",
        "\n",
        "So in simpler terms, the soft margin approach is like saying, \"**Let's create a boundary that works well for most points but still allows for a few exceptions, because nobody's perfect!**\" This makes your model more flexible and robust, especially when the data has some overlap or isn't perfectly separable."
      ],
      "metadata": {
        "id": "L_tO1wH0Tk0r"
      }
    }
  ]
}